{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Motivation of why features interactions are important?\n",
    "2nd order\n",
    "3rd order\n",
    "\"\"\"\n",
    "import torch.nn as nn\n",
    "\n",
    "def create_embedding_matrix(feature_columns, init_std=0.0001, linear=False, sparse=False, device='cpu'):\n",
    "    # Return nn.ModuleDict: for sparse features, {embedding_name: nn.Embedding}\n",
    "    # for varlen sparse features, {embedding_name: nn.EmbeddingBag}\n",
    "    sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "\n",
    "    varlen_sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "\n",
    "    embedding_dict = nn.ModuleDict(\n",
    "        {feat.embedding_name: nn.Embedding(feat.vocabulary_size, feat.embedding_dim if not linear else 1, sparse=sparse)\n",
    "         for feat in\n",
    "         sparse_feature_columns + varlen_sparse_feature_columns}\n",
    "    )\n",
    "\n",
    "    # embedding weight initalization\n",
    "    for tensor in embedding_dict.values():\n",
    "        nn.init.normal_(tensor.weight, mean=0, std=init_std)\n",
    "\n",
    "    return embedding_dict.to(device)\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, feature_columns, feature_index, init_std=0.0001, device='cpu'):\n",
    "        super(Linear, self).__init__()\n",
    "        self.feature_index = feature_index\n",
    "        self.device = device\n",
    "        self.sparse_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "        self.dense_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, DenseFeat), feature_columns)) if len(feature_columns) else []\n",
    "        print(\"self.dense_feature_columns\", self.dense_feature_columns)\n",
    "        self.varlen_sparse_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "\n",
    "        self.embedding_dict = create_embedding_matrix(feature_columns, init_std, linear=True, sparse=False,\n",
    "                                                      device=device)\n",
    "\n",
    "        #         nn.ModuleDict(\n",
    "        #             {feat.embedding_name: nn.Embedding(feat.dimension, 1, sparse=True) for feat in\n",
    "        #              self.sparse_feature_columns}\n",
    "        #         )\n",
    "        # .to(\"cuda:1\")\n",
    "        for tensor in self.embedding_dict.values():\n",
    "            nn.init.normal_(tensor.weight, mean=0, std=init_std)\n",
    "\n",
    "        if len(self.dense_feature_columns) > 0:\n",
    "            self.weight = nn.Parameter(torch.Tensor(sum(fc.dimension for fc in self.dense_feature_columns), 1).to(\n",
    "                device))\n",
    "            torch.nn.init.normal_(self.weight, mean=0, std=init_std)\n",
    "\n",
    "    def forward(self, X, sparse_feat_refine_weight=None):\n",
    "\n",
    "        sparse_embedding_list = [self.embedding_dict[feat.embedding_name](\n",
    "            X[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]].long()) for\n",
    "            feat in self.sparse_feature_columns]\n",
    "        print(\"sparse_embedding_list\", len(sparse_embedding_list), sparse_embedding_list[0].shape)\n",
    "        dense_value_list = [X[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]] for feat in\n",
    "                            self.dense_feature_columns]\n",
    "\n",
    "        varlen_embedding_list = get_varlen_pooling_list(self.embedding_dict, X, self.feature_index,\n",
    "                                                        self.varlen_sparse_feature_columns, self.device)\n",
    "\n",
    "        sparse_embedding_list += varlen_embedding_list\n",
    "\n",
    "        linear_logit = torch.zeros([X.shape[0], 1]).to(sparse_embedding_list[0].device)\n",
    "        if len(sparse_embedding_list) > 0:\n",
    "            sparse_embedding_cat = torch.cat(sparse_embedding_list, dim=-1)\n",
    "            print(\"sparse_embedding_cat\", sparse_embedding_cat.shape)\n",
    "            if sparse_feat_refine_weight is not None:\n",
    "                # w_{x,i}=m_{x,i} * w_i (in IFM and DIFM)\n",
    "                sparse_embedding_cat = sparse_embedding_cat * sparse_feat_refine_weight.unsqueeze(1)\n",
    "            sparse_feat_logit = torch.sum(sparse_embedding_cat, dim=-1, keepdim=False)\n",
    "            print(\"sparse_feat_logit\", sparse_feat_logit.shape)\n",
    "            linear_logit += sparse_feat_logit\n",
    "        if len(dense_value_list) > 0:\n",
    "            dense_value_logit = torch.cat(\n",
    "                dense_value_list, dim=-1).matmul(self.weight)\n",
    "            linear_logit += dense_value_logit\n",
    "\n",
    "        return linear_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict, namedtuple, defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "DEFAULT_GROUP_NAME = \"default_group\"\n",
    "\n",
    "\n",
    "class SparseFeat(namedtuple('SparseFeat',\n",
    "                            ['name', 'vocabulary_size', 'embedding_dim', 'use_hash', 'dtype', 'embedding_name',\n",
    "                             'group_name'])):\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __new__(cls, name, vocabulary_size, embedding_dim=4, use_hash=False, dtype=\"int32\", embedding_name=None,\n",
    "                group_name=DEFAULT_GROUP_NAME):\n",
    "        if embedding_name is None:\n",
    "            embedding_name = name\n",
    "        if embedding_dim == \"auto\":\n",
    "            embedding_dim = 6 * int(pow(vocabulary_size, 0.25))\n",
    "        if use_hash:\n",
    "            print(\n",
    "                \"Notice! Feature Hashing on the fly currently is not supported in torch version,you can use tensorflow version!\")\n",
    "        return super(SparseFeat, cls).__new__(cls, name, vocabulary_size, embedding_dim, use_hash, dtype,\n",
    "                                              embedding_name, group_name)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.name.__hash__()\n",
    "    \n",
    "class VarLenSparseFeat(namedtuple('VarLenSparseFeat',\n",
    "                                  ['sparsefeat', 'maxlen', 'combiner', 'length_name'])):\n",
    "    __slots__ = ()\n",
    "    \"\"\"\n",
    "    This feature would like to support embedding bag such as average of embedding \n",
    "    \"\"\"\n",
    "    def __new__(cls, sparsefeat, maxlen, combiner=\"mean\", length_name=None):\n",
    "        return super(VarLenSparseFeat, cls).__new__(cls, sparsefeat, maxlen, combiner, length_name)\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.sparsefeat.name\n",
    "\n",
    "    @property\n",
    "    def vocabulary_size(self):\n",
    "        return self.sparsefeat.vocabulary_size\n",
    "\n",
    "    @property\n",
    "    def embedding_dim(self):\n",
    "        return self.sparsefeat.embedding_dim\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.sparsefeat.dtype\n",
    "\n",
    "    @property\n",
    "    def embedding_name(self):\n",
    "        return self.sparsefeat.embedding_name\n",
    "\n",
    "    @property\n",
    "    def group_name(self):\n",
    "        return self.sparsefeat.group_name\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.name.__hash__()\n",
    "\n",
    "\n",
    "class DenseFeat(namedtuple('DenseFeat', ['name', 'dimension', 'dtype'])):\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __new__(cls, name, dimension=1, dtype=\"float32\"):\n",
    "        return super(DenseFeat, cls).__new__(cls, name, dimension, dtype)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.name.__hash__()\n",
    "    \n",
    "def build_input_features(feature_columns):\n",
    "    # Return OrderedDict: {feature_name:(start, start+dimension)}\n",
    "\n",
    "    features = OrderedDict()\n",
    "\n",
    "    start = 0\n",
    "    for feat in feature_columns:\n",
    "        feat_name = feat.name\n",
    "        if feat_name in features:\n",
    "            continue\n",
    "        if isinstance(feat, SparseFeat):\n",
    "            features[feat_name] = (start, start + 1)\n",
    "            start += 1\n",
    "        elif isinstance(feat, DenseFeat):\n",
    "            features[feat_name] = (start, start + feat.dimension)\n",
    "            start += feat.dimension\n",
    "        elif isinstance(feat, VarLenSparseFeat):\n",
    "            features[feat_name] = (start, start + feat.maxlen)\n",
    "            start += feat.maxlen\n",
    "            if feat.length_name is not None and feat.length_name not in features:\n",
    "                features[feat.length_name] = (start, start + 1)\n",
    "                start += 1\n",
    "        else:\n",
    "            raise TypeError(\"Invalid feature column type,got\", type(feat))\n",
    "    return features\n",
    "\n",
    "def get_feature_names(feature_columns):\n",
    "    features = build_input_features(feature_columns)\n",
    "    return list(features.keys())\n",
    "\n",
    "def get_varlen_pooling_list(embedding_dict, features, feature_index, varlen_sparse_feature_columns, device):\n",
    "    varlen_sparse_embedding_list = []\n",
    "\n",
    "    for feat in varlen_sparse_feature_columns:\n",
    "        seq_emb = embedding_dict[feat.embedding_name](\n",
    "            features[:, feature_index[feat.name][0]:feature_index[feat.name][1]].long())\n",
    "        if feat.length_name is None:\n",
    "            seq_mask = features[:, feature_index[feat.name][0]:feature_index[feat.name][1]].long() != 0\n",
    "\n",
    "            emb = SequencePoolingLayer(mode=feat.combiner, supports_masking=True, device=device)(\n",
    "                [seq_emb, seq_mask])\n",
    "        else:\n",
    "            seq_length = features[:,\n",
    "                         feature_index[feat.length_name][0]:feature_index[feat.length_name][1]].long()\n",
    "            emb = SequencePoolingLayer(mode=feat.combiner, supports_masking=False, device=device)(\n",
    "                [seq_emb, seq_length])\n",
    "        varlen_sparse_embedding_list.append(emb)\n",
    "    return varlen_sparse_embedding_list\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"/Users/yunruili/DeepCTR-Torch/examples/movielens_sample.txt\")\n",
    "sparse_features = [\"movie_id\", \"user_id\",\n",
    "                   \"gender\", \"age\", \"occupation\", \"zip\"]\n",
    "target = ['rating']\n",
    "\n",
    "# 1.Label Encoding for sparse features,and do simple Transformation for dense features\n",
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feat] = lbe.fit_transform(data[feat])\n",
    "# 2.count #unique features for each sparse field\n",
    "fixlen_feature_columns = [SparseFeat(feat, data[feat].nunique())\n",
    "                          for feat in sparse_features]\n",
    "\n",
    "\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "\n",
    "train_model_input = {name: train[name] for name in feature_names}\n",
    "test_model_input = {name: test[name] for name in feature_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>169</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>961476022</td>\n",
       "      <td>Star Wars: Episode V - The Empire Strikes Back...</td>\n",
       "      <td>Action|Adventure|Drama|Sci-Fi|War</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>138</td>\n",
       "      <td>96</td>\n",
       "      <td>4</td>\n",
       "      <td>965343416</td>\n",
       "      <td>Fear and Loathing in Las Vegas (1998)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>134</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>966223349</td>\n",
       "      <td>Searching for Bobby Fischer (1993)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>78</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>980013556</td>\n",
       "      <td>Nutty Professor, The (1996)</td>\n",
       "      <td>Comedy|Fantasy|Romance|Sci-Fi</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>176</td>\n",
       "      <td>117</td>\n",
       "      <td>4</td>\n",
       "      <td>959787754</td>\n",
       "      <td>Beetlejuice (1988)</td>\n",
       "      <td>Comedy|Fantasy</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>89</td>\n",
       "      <td>161</td>\n",
       "      <td>5</td>\n",
       "      <td>973804787</td>\n",
       "      <td>Green Mile, The (1999)</td>\n",
       "      <td>Drama|Thriller</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>108</td>\n",
       "      <td>3</td>\n",
       "      <td>977788576</td>\n",
       "      <td>Parent Trap, The (1998)</td>\n",
       "      <td>Children's|Drama</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>76</td>\n",
       "      <td>49</td>\n",
       "      <td>3</td>\n",
       "      <td>974658834</td>\n",
       "      <td>20,000 Leagues Under the Sea (1954)</td>\n",
       "      <td>Adventure|Children's|Fantasy|Sci-Fi</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>80</td>\n",
       "      <td>106</td>\n",
       "      <td>3</td>\n",
       "      <td>979353437</td>\n",
       "      <td>Dangerous Liaisons (1988)</td>\n",
       "      <td>Drama|Romance</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>67</td>\n",
       "      <td>186</td>\n",
       "      <td>5</td>\n",
       "      <td>974753321</td>\n",
       "      <td>Meet the Parents (2000)</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id  movie_id  rating  timestamp  \\\n",
       "175      169        59       2  961476022   \n",
       "119      138        96       4  965343416   \n",
       "8        134        30       5  966223349   \n",
       "15        78        40       3  980013556   \n",
       "35       176       117       4  959787754   \n",
       "..       ...       ...     ...        ...   \n",
       "143       89       161       5  973804787   \n",
       "13         1       108       3  977788576   \n",
       "92        76        49       3  974658834   \n",
       "93        80       106       3  979353437   \n",
       "187       67       186       5  974753321   \n",
       "\n",
       "                                                 title  \\\n",
       "175  Star Wars: Episode V - The Empire Strikes Back...   \n",
       "119              Fear and Loathing in Las Vegas (1998)   \n",
       "8                   Searching for Bobby Fischer (1993)   \n",
       "15                         Nutty Professor, The (1996)   \n",
       "35                                  Beetlejuice (1988)   \n",
       "..                                                 ...   \n",
       "143                             Green Mile, The (1999)   \n",
       "13                             Parent Trap, The (1998)   \n",
       "92                 20,000 Leagues Under the Sea (1954)   \n",
       "93                           Dangerous Liaisons (1988)   \n",
       "187                            Meet the Parents (2000)   \n",
       "\n",
       "                                  genres  gender  age  occupation  zip  \n",
       "175    Action|Adventure|Drama|Sci-Fi|War       1    1           9  110  \n",
       "119                         Comedy|Drama       1    3          15   75  \n",
       "8                                  Drama       1    2          16   84  \n",
       "15         Comedy|Fantasy|Romance|Sci-Fi       1    1           0   94  \n",
       "35                        Comedy|Fantasy       1    4           1  145  \n",
       "..                                   ...     ...  ...         ...  ...  \n",
       "143                       Drama|Thriller       1    2          13   38  \n",
       "13                      Children's|Drama       1    6           1   86  \n",
       "92   Adventure|Children's|Fantasy|Sci-Fi       1    5          15   15  \n",
       "93                         Drama|Romance       1    2           0   77  \n",
       "187                               Comedy       1    2           7  151  \n",
       "\n",
       "[160 rows x 10 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('movie_id', (0, 1)),\n",
       "             ('user_id', (1, 2)),\n",
       "             ('gender', (2, 3)),\n",
       "             ('age', (3, 4)),\n",
       "             ('occupation', (4, 5)),\n",
       "             ('zip', (5, 6))])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_index = build_input_features(\n",
    "            linear_feature_columns + dnn_feature_columns)\n",
    "feature_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_id\n",
      "user_id\n",
      "gender\n",
      "age\n",
      "occupation\n",
      "zip\n"
     ]
    }
   ],
   "source": [
    "for feat in feature_index:\n",
    "    print(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175    1\n",
       "119    3\n",
       "8      2\n",
       "15     1\n",
       "35     4\n",
       "      ..\n",
       "143    2\n",
       "13     6\n",
       "92     5\n",
       "93     2\n",
       "187    2\n",
       "Name: age, Length: 160, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model_input[\"age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.dense_feature_columns []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n所以對linear model\\n每一個sparese feature根據你的id會找到對應的weight\\n\\n可以想成是一個y = w1*x1 +....wn*xn\\nn = 7+2+187+20+193+188, 非常大的一個數子\\n只有在對應的id xj, 是1其他都是0\\n'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "linear_model = Linear(\n",
    "            linear_feature_columns, feature_index, device=device)\n",
    "linear_model\n",
    "\"\"\"\n",
    "所以對linear model, 我們把每一個sparese feature根據你的id會找到對應的weight\n",
    "\n",
    "數學式, 可以想成是一個y = w1*x1 +....wn*xn, 而總有有多少個weight of n = 7+2+187+20+193+188,是非常大的一個數子\n",
    "其中只有在對應的id 的 xj, 是1其他都是0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(7, 1)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat = \"age\"\n",
    "embedding_dict = create_embedding_matrix(linear_feature_columns, linear = True)[feat]\n",
    "embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = feature_index[feat][0]\n",
    "end = feature_index[feat][1]\n",
    "feature_index[feat][0], feature_index[feat][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5],\n",
       "        [2],\n",
       "        [5],\n",
       "        [2],\n",
       "        [3]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = x[:, start:end].long()\n",
    "input_[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 16.,  51.,   1.,   5.,   7.,  53.],\n",
       "        [102.,  48.,   1.,   2.,  11., 147.],\n",
       "        [ 37.,  95.,   1.,   5.,  11., 177.],\n",
       "        [168., 192.,   1.,   2.,   6.,  26.],\n",
       "        [ 31., 124.,   1.,   3.,   7.,  16.]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each training example,\n",
    "#   for each spare feature, we get his corresponding embedding and concate them together (linear --> dim of embedding is 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 1])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dict(input_).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_embedding_list = [self.embedding_dict[feat.embedding_name](\n",
    "            X[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]].long()) for\n",
    "            feat in self.sparse_feature_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit method\n",
    "# -*- coding:utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "Author:\n",
    "    Weichen Shen,weichenswc@163.com\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def concat_fun(inputs, axis=-1):\n",
    "    if len(inputs) == 1:\n",
    "        return inputs[0]\n",
    "    else:\n",
    "        return torch.cat(inputs, dim=axis)\n",
    "\n",
    "\n",
    "def slice_arrays(arrays, start=None, stop=None):\n",
    "    \"\"\"Slice an array or list of arrays.\n",
    "\n",
    "    This takes an array-like, or a list of\n",
    "    array-likes, and outputs:\n",
    "        - arrays[start:stop] if `arrays` is an array-like\n",
    "        - [x[start:stop] for x in arrays] if `arrays` is a list\n",
    "\n",
    "    Can also work on list/array of indices: `slice_arrays(x, indices)`\n",
    "\n",
    "    Arguments:\n",
    "        arrays: Single array or list of arrays.\n",
    "        start: can be an integer index (start index)\n",
    "            or a list/array of indices\n",
    "        stop: integer (stop index); should be None if\n",
    "            `start` was a list.\n",
    "\n",
    "    Returns:\n",
    "        A slice of the array(s).\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the value of start is a list and stop is not None.\n",
    "    \"\"\"\n",
    "\n",
    "    if arrays is None:\n",
    "        return [None]\n",
    "\n",
    "    if isinstance(arrays, np.ndarray):\n",
    "        arrays = [arrays]\n",
    "\n",
    "    if isinstance(start, list) and stop is not None:\n",
    "        raise ValueError('The stop argument has to be None if the value of start '\n",
    "                         'is a list.')\n",
    "    elif isinstance(arrays, list):\n",
    "        if hasattr(start, '__len__'):\n",
    "            # hdf5 datasets only support list objects as indices\n",
    "            if hasattr(start, 'shape'):\n",
    "                start = start.tolist()\n",
    "            return [None if x is None else x[start] for x in arrays]\n",
    "        else:\n",
    "            if len(arrays) == 1:\n",
    "                return arrays[0][start:stop]\n",
    "            return [None if x is None else x[start:stop] for x in arrays]\n",
    "    else:\n",
    "        if hasattr(start, '__len__'):\n",
    "            if hasattr(start, 'shape'):\n",
    "                start = start.tolist()\n",
    "            return arrays[start]\n",
    "        elif hasattr(start, '__getitem__'):\n",
    "            return arrays[start:stop]\n",
    "        else:\n",
    "            return [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "split_at 128\n",
      "x 6 <class 'pandas.core.series.Series'> (128,)\n",
      "val_x 6 <class 'pandas.core.series.Series'> (32,)\n",
      "y <class 'numpy.ndarray'> (128, 1)\n",
      "val_y <class 'numpy.ndarray'> (32, 1)\n",
      "x <class 'list'> 6 <class 'numpy.ndarray'> (128, 1)\n",
      "Train on 128 samples, validate on 32 samples, 1 steps per epoch\n",
      "torch.Size([128, 6])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 6])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 6])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 6])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 6])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 6])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 6])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 6])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 6])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 6])\n",
      "torch.Size([128, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as Data\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "x = train_model_input\n",
    "y = train[target].values\n",
    "batch_size = None\n",
    "shuffle = True\n",
    "if isinstance(x, dict):\n",
    "    x = [x[feature] for feature in feature_index]\n",
    "    print(type(x[0]))\n",
    "\n",
    "validation_split = 0.2\n",
    "do_validation = True\n",
    "initial_epoch = 0\n",
    "epochs = 10\n",
    "verbose = 0\n",
    "if hasattr(x[0], 'shape'):\n",
    "    split_at = int(x[0].shape[0] * (1. - validation_split))\n",
    "else:\n",
    "    split_at = int(len(x[0]) * (1. - validation_split))\n",
    "print(\"split_at\", split_at)\n",
    "x, val_x = (slice_arrays(x, 0, split_at),\n",
    "            slice_arrays(x, split_at))\n",
    "print(\"x\", len(x), type(x[0]), x[0].shape)\n",
    "print(\"val_x\", len(val_x), type(val_x[0]), val_x[0].shape)\n",
    "\n",
    "y, val_y = (slice_arrays(y, 0, split_at),\n",
    "            slice_arrays(y, split_at))\n",
    "print(\"y\", type(y), y.shape)\n",
    "print(\"val_y\", type(val_y), val_y.shape)\n",
    "for i in range(len(x)):\n",
    "    if len(x[i].shape) == 1:\n",
    "        x[i] = np.expand_dims(x[i], axis=1)\n",
    "print(\"x\", type(x), len(x), type(x[0]), x[0].shape)\n",
    "train_tensor_data = Data.TensorDataset(\n",
    "    torch.from_numpy(\n",
    "        np.concatenate(x, axis=-1)),\n",
    "    torch.from_numpy(y))\n",
    "if batch_size is None:\n",
    "    batch_size = 256\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_tensor_data, shuffle=shuffle, batch_size=batch_size)\n",
    "sample_num = len(train_tensor_data)\n",
    "steps_per_epoch = (sample_num - 1) // batch_size + 1\n",
    "# Train\n",
    "print(\"Train on {0} samples, validate on {1} samples, {2} steps per epoch\".format(\n",
    "    len(train_tensor_data), len(val_y), steps_per_epoch))\n",
    "for epoch in range(initial_epoch, epochs):\n",
    "    #callbacks.on_epoch_begin(epoch)\n",
    "    epoch_logs = {}\n",
    "    start_time = time.time()\n",
    "    loss_epoch = 0\n",
    "    total_loss_epoch = 0\n",
    "    train_result = {}\n",
    "    try:\n",
    "        with tqdm(enumerate(train_loader), disable=verbose != 1) as t:\n",
    "            for _, (x_train, y_train) in t:\n",
    "                print(x_train.shape)\n",
    "                print(y_train.shape)\n",
    "                x = x_train.to(device).float()\n",
    "                y = y_train.to(device).float()\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        t.close()\n",
    "        raise\n",
    "    t.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 1)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_num, steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse_embedding_list 6 torch.Size([128, 1, 1])\n",
      "sparse_embedding_cat torch.Size([128, 1, 6])\n",
      "sparse_feat_logit torch.Size([128, 1])\n"
     ]
    }
   ],
   "source": [
    "linear_logit = linear_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_logit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 9])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "x_cat = torch.cat([x, x, x], -1)\n",
    "x_cat.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6068,  2.2713, -0.4924,  0.6068,  2.2713, -0.4924,  0.6068,  2.2713,\n",
       "         -0.4924],\n",
       "        [ 0.8889, -0.7034,  0.0802,  0.8889, -0.7034,  0.0802,  0.8889, -0.7034,\n",
       "          0.0802]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.1572, 0.7969])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x_cat, dim=-1, keepdim=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dense features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 500)\n",
    "data = pd.read_csv('/Users/yunruili/DeepCTR-Torch/examples/criteo_sample.txt')\n",
    "\n",
    "sparse_features = ['C' + str(i) for i in range(1, 27)]\n",
    "dense_features = ['I' + str(i) for i in range(1, 14)]\n",
    "\n",
    "data[sparse_features] = data[sparse_features].fillna('-1', )\n",
    "data[dense_features] = data[dense_features].fillna(0, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>I10</th>\n",
       "      <th>I11</th>\n",
       "      <th>I12</th>\n",
       "      <th>I13</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>C10</th>\n",
       "      <th>C11</th>\n",
       "      <th>C12</th>\n",
       "      <th>C13</th>\n",
       "      <th>C14</th>\n",
       "      <th>C15</th>\n",
       "      <th>C16</th>\n",
       "      <th>C17</th>\n",
       "      <th>C18</th>\n",
       "      <th>C19</th>\n",
       "      <th>C20</th>\n",
       "      <th>C21</th>\n",
       "      <th>C22</th>\n",
       "      <th>C23</th>\n",
       "      <th>C24</th>\n",
       "      <th>C25</th>\n",
       "      <th>C26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>260.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17668.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>05db9164</td>\n",
       "      <td>08d6d899</td>\n",
       "      <td>9143c832</td>\n",
       "      <td>f56b7dd5</td>\n",
       "      <td>25c83c98</td>\n",
       "      <td>7e0ccccf</td>\n",
       "      <td>df5c2d18</td>\n",
       "      <td>0b153874</td>\n",
       "      <td>a73ee510</td>\n",
       "      <td>8f48ce11</td>\n",
       "      <td>a7b606c4</td>\n",
       "      <td>ae1bb660</td>\n",
       "      <td>eae197fd</td>\n",
       "      <td>b28479f6</td>\n",
       "      <td>bfef54b3</td>\n",
       "      <td>bad5ee18</td>\n",
       "      <td>e5ba7672</td>\n",
       "      <td>87c6f83c</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0429f84b</td>\n",
       "      <td>-1</td>\n",
       "      <td>3a171ecb</td>\n",
       "      <td>c0d61a5c</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>30251.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>68fd1e64</td>\n",
       "      <td>04e09220</td>\n",
       "      <td>95e13fd4</td>\n",
       "      <td>a1e6a194</td>\n",
       "      <td>25c83c98</td>\n",
       "      <td>fe6b92e5</td>\n",
       "      <td>f819e175</td>\n",
       "      <td>062b5529</td>\n",
       "      <td>a73ee510</td>\n",
       "      <td>ab9456b4</td>\n",
       "      <td>6153cf57</td>\n",
       "      <td>8882c6cd</td>\n",
       "      <td>769a1844</td>\n",
       "      <td>b28479f6</td>\n",
       "      <td>69f825dd</td>\n",
       "      <td>23056e4f</td>\n",
       "      <td>d4bb7bd8</td>\n",
       "      <td>6fc84bfb</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>5155d8a3</td>\n",
       "      <td>-1</td>\n",
       "      <td>be7c41b4</td>\n",
       "      <td>ded4aac9</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>05db9164</td>\n",
       "      <td>38a947a1</td>\n",
       "      <td>3f55fb72</td>\n",
       "      <td>5de245c7</td>\n",
       "      <td>30903e74</td>\n",
       "      <td>7e0ccccf</td>\n",
       "      <td>b72ec13d</td>\n",
       "      <td>1f89b562</td>\n",
       "      <td>a73ee510</td>\n",
       "      <td>acce978c</td>\n",
       "      <td>3547565f</td>\n",
       "      <td>a5b0521a</td>\n",
       "      <td>12880350</td>\n",
       "      <td>b28479f6</td>\n",
       "      <td>c12fc269</td>\n",
       "      <td>95a8919c</td>\n",
       "      <td>e5ba7672</td>\n",
       "      <td>675c9258</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2e01979f</td>\n",
       "      <td>-1</td>\n",
       "      <td>bcdee96c</td>\n",
       "      <td>6d5d1302</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16836.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>05db9164</td>\n",
       "      <td>8084ee93</td>\n",
       "      <td>02cf9876</td>\n",
       "      <td>c18be181</td>\n",
       "      <td>25c83c98</td>\n",
       "      <td>-1</td>\n",
       "      <td>e14874c9</td>\n",
       "      <td>0b153874</td>\n",
       "      <td>7cc72ec2</td>\n",
       "      <td>2462946f</td>\n",
       "      <td>636405ac</td>\n",
       "      <td>8fe001f4</td>\n",
       "      <td>31b42deb</td>\n",
       "      <td>07d13a8f</td>\n",
       "      <td>422c8577</td>\n",
       "      <td>36103458</td>\n",
       "      <td>e5ba7672</td>\n",
       "      <td>52e44668</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>e587c466</td>\n",
       "      <td>-1</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>3b183c5c</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>05db9164</td>\n",
       "      <td>207b2d81</td>\n",
       "      <td>5d076085</td>\n",
       "      <td>862b5ba0</td>\n",
       "      <td>25c83c98</td>\n",
       "      <td>fbad5c96</td>\n",
       "      <td>17c22666</td>\n",
       "      <td>0b153874</td>\n",
       "      <td>a73ee510</td>\n",
       "      <td>534fc986</td>\n",
       "      <td>feb49a68</td>\n",
       "      <td>f24b551c</td>\n",
       "      <td>8978af5c</td>\n",
       "      <td>64c94865</td>\n",
       "      <td>32ec6582</td>\n",
       "      <td>b6d021e8</td>\n",
       "      <td>e5ba7672</td>\n",
       "      <td>25c88e42</td>\n",
       "      <td>21ddcdc9</td>\n",
       "      <td>b1252a9d</td>\n",
       "      <td>0e8585d2</td>\n",
       "      <td>-1</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>0d4a6d1a</td>\n",
       "      <td>001f3601</td>\n",
       "      <td>92c878de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3036.0</td>\n",
       "      <td>575.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>05db9164</td>\n",
       "      <td>0468d672</td>\n",
       "      <td>628b07b0</td>\n",
       "      <td>b63c0277</td>\n",
       "      <td>25c83c98</td>\n",
       "      <td>7e0ccccf</td>\n",
       "      <td>0d339a25</td>\n",
       "      <td>c8ddd494</td>\n",
       "      <td>a73ee510</td>\n",
       "      <td>1722d4c8</td>\n",
       "      <td>7d756b25</td>\n",
       "      <td>0c87b3e9</td>\n",
       "      <td>6f833c7a</td>\n",
       "      <td>1adce6ef</td>\n",
       "      <td>4f3b3616</td>\n",
       "      <td>48af915a</td>\n",
       "      <td>07c540c4</td>\n",
       "      <td>9880032b</td>\n",
       "      <td>21ddcdc9</td>\n",
       "      <td>5840adea</td>\n",
       "      <td>34cc61bb</td>\n",
       "      <td>c9d4222a</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>e5ed7da2</td>\n",
       "      <td>ea9a246c</td>\n",
       "      <td>984e0db0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1607.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>be589b51</td>\n",
       "      <td>aa8fcc21</td>\n",
       "      <td>4255f8fd</td>\n",
       "      <td>7501d94a</td>\n",
       "      <td>25c83c98</td>\n",
       "      <td>fe6b92e5</td>\n",
       "      <td>0492c809</td>\n",
       "      <td>1f89b562</td>\n",
       "      <td>a73ee510</td>\n",
       "      <td>13ba96b0</td>\n",
       "      <td>ba0f9e8a</td>\n",
       "      <td>887a0c20</td>\n",
       "      <td>4e4dd817</td>\n",
       "      <td>07d13a8f</td>\n",
       "      <td>a4f91020</td>\n",
       "      <td>022714ba</td>\n",
       "      <td>1e88c74f</td>\n",
       "      <td>3972b4ed</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>d1aa4512</td>\n",
       "      <td>-1</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>9257f75f</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>05db9164</td>\n",
       "      <td>09e68b86</td>\n",
       "      <td>db151f8b</td>\n",
       "      <td>f1b645fc</td>\n",
       "      <td>25c83c98</td>\n",
       "      <td>-1</td>\n",
       "      <td>b87f4a4a</td>\n",
       "      <td>0b153874</td>\n",
       "      <td>a73ee510</td>\n",
       "      <td>e70742b0</td>\n",
       "      <td>319687c9</td>\n",
       "      <td>af6ad6b6</td>\n",
       "      <td>62036f49</td>\n",
       "      <td>f862f261</td>\n",
       "      <td>1dca7862</td>\n",
       "      <td>05a97a3c</td>\n",
       "      <td>3486227d</td>\n",
       "      <td>5aed7436</td>\n",
       "      <td>54591762</td>\n",
       "      <td>a458ea53</td>\n",
       "      <td>4a2c3526</td>\n",
       "      <td>-1</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>1793a828</td>\n",
       "      <td>e8b83407</td>\n",
       "      <td>1a02cbe1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "      <td>6.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>508.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>05db9164</td>\n",
       "      <td>e5fb1af3</td>\n",
       "      <td>7e1ad1fe</td>\n",
       "      <td>46ec0a38</td>\n",
       "      <td>43b19349</td>\n",
       "      <td>7e0ccccf</td>\n",
       "      <td>24c48926</td>\n",
       "      <td>0b153874</td>\n",
       "      <td>a73ee510</td>\n",
       "      <td>afa26c81</td>\n",
       "      <td>9f0003f4</td>\n",
       "      <td>651d80c6</td>\n",
       "      <td>5afd9e51</td>\n",
       "      <td>07d13a8f</td>\n",
       "      <td>b5de5956</td>\n",
       "      <td>72401022</td>\n",
       "      <td>3486227d</td>\n",
       "      <td>13145934</td>\n",
       "      <td>55dd3565</td>\n",
       "      <td>5840adea</td>\n",
       "      <td>bf647035</td>\n",
       "      <td>-1</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>1481ceb4</td>\n",
       "      <td>e8b83407</td>\n",
       "      <td>988b0775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>be589b51</td>\n",
       "      <td>b46aceb6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>43b19349</td>\n",
       "      <td>-1</td>\n",
       "      <td>17cdc396</td>\n",
       "      <td>0b153874</td>\n",
       "      <td>a73ee510</td>\n",
       "      <td>75d852fc</td>\n",
       "      <td>d79cc967</td>\n",
       "      <td>-1</td>\n",
       "      <td>115d29f4</td>\n",
       "      <td>07d13a8f</td>\n",
       "      <td>217d99f2</td>\n",
       "      <td>-1</td>\n",
       "      <td>d4bb7bd8</td>\n",
       "      <td>908eaeb8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label   I1  I2     I3    I4       I5     I6    I7    I8     I9  I10  \\\n",
       "0        0  0.0   3  260.0   0.0  17668.0    0.0   0.0  33.0    0.0  0.0   \n",
       "1        0  0.0  -1   19.0  35.0  30251.0  247.0   1.0  35.0  160.0  0.0   \n",
       "2        0  0.0   0    2.0  12.0   2013.0  164.0   6.0  35.0  523.0  0.0   \n",
       "3        0  0.0  13    1.0   4.0  16836.0  200.0   5.0   4.0   29.0  0.0   \n",
       "4        0  0.0   0  104.0  27.0   1990.0  142.0   4.0  32.0   37.0  0.0   \n",
       "..     ...  ...  ..    ...   ...      ...    ...   ...   ...    ...  ...   \n",
       "195      0  0.0   0  113.0   3.0   3036.0  575.0   2.0   3.0  214.0  0.0   \n",
       "196      1  0.0   1    1.0   1.0   1607.0   12.0   1.0  12.0   15.0  0.0   \n",
       "197      1  1.0   0    6.0   3.0      0.0    0.0  19.0   3.0    3.0  1.0   \n",
       "198      0  0.0  22    6.0  22.0    203.0  153.0  80.0  18.0  508.0  0.0   \n",
       "199      0  1.0  -1    0.0   0.0    138.0    0.0   1.0   0.0    0.0  1.0   \n",
       "\n",
       "      I11  I12   I13        C1        C2        C3        C4        C5  \\\n",
       "0     0.0  0.0   0.0  05db9164  08d6d899  9143c832  f56b7dd5  25c83c98   \n",
       "1     1.0  0.0  35.0  68fd1e64  04e09220  95e13fd4  a1e6a194  25c83c98   \n",
       "2     3.0  0.0  18.0  05db9164  38a947a1  3f55fb72  5de245c7  30903e74   \n",
       "3     2.0  0.0   4.0  05db9164  8084ee93  02cf9876  c18be181  25c83c98   \n",
       "4     1.0  0.0  27.0  05db9164  207b2d81  5d076085  862b5ba0  25c83c98   \n",
       "..    ...  ...   ...       ...       ...       ...       ...       ...   \n",
       "195   1.0  0.0   3.0  05db9164  0468d672  628b07b0  b63c0277  25c83c98   \n",
       "196   1.0  0.0  12.0  be589b51  aa8fcc21  4255f8fd  7501d94a  25c83c98   \n",
       "197   9.0  0.0   0.0  05db9164  09e68b86  db151f8b  f1b645fc  25c83c98   \n",
       "198  11.0  0.0  22.0  05db9164  e5fb1af3  7e1ad1fe  46ec0a38  43b19349   \n",
       "199   1.0  0.0   0.0  be589b51  b46aceb6        -1        -1  43b19349   \n",
       "\n",
       "           C6        C7        C8        C9       C10       C11       C12  \\\n",
       "0    7e0ccccf  df5c2d18  0b153874  a73ee510  8f48ce11  a7b606c4  ae1bb660   \n",
       "1    fe6b92e5  f819e175  062b5529  a73ee510  ab9456b4  6153cf57  8882c6cd   \n",
       "2    7e0ccccf  b72ec13d  1f89b562  a73ee510  acce978c  3547565f  a5b0521a   \n",
       "3          -1  e14874c9  0b153874  7cc72ec2  2462946f  636405ac  8fe001f4   \n",
       "4    fbad5c96  17c22666  0b153874  a73ee510  534fc986  feb49a68  f24b551c   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "195  7e0ccccf  0d339a25  c8ddd494  a73ee510  1722d4c8  7d756b25  0c87b3e9   \n",
       "196  fe6b92e5  0492c809  1f89b562  a73ee510  13ba96b0  ba0f9e8a  887a0c20   \n",
       "197        -1  b87f4a4a  0b153874  a73ee510  e70742b0  319687c9  af6ad6b6   \n",
       "198  7e0ccccf  24c48926  0b153874  a73ee510  afa26c81  9f0003f4  651d80c6   \n",
       "199        -1  17cdc396  0b153874  a73ee510  75d852fc  d79cc967        -1   \n",
       "\n",
       "          C13       C14       C15       C16       C17       C18       C19  \\\n",
       "0    eae197fd  b28479f6  bfef54b3  bad5ee18  e5ba7672  87c6f83c        -1   \n",
       "1    769a1844  b28479f6  69f825dd  23056e4f  d4bb7bd8  6fc84bfb        -1   \n",
       "2    12880350  b28479f6  c12fc269  95a8919c  e5ba7672  675c9258        -1   \n",
       "3    31b42deb  07d13a8f  422c8577  36103458  e5ba7672  52e44668        -1   \n",
       "4    8978af5c  64c94865  32ec6582  b6d021e8  e5ba7672  25c88e42  21ddcdc9   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "195  6f833c7a  1adce6ef  4f3b3616  48af915a  07c540c4  9880032b  21ddcdc9   \n",
       "196  4e4dd817  07d13a8f  a4f91020  022714ba  1e88c74f  3972b4ed        -1   \n",
       "197  62036f49  f862f261  1dca7862  05a97a3c  3486227d  5aed7436  54591762   \n",
       "198  5afd9e51  07d13a8f  b5de5956  72401022  3486227d  13145934  55dd3565   \n",
       "199  115d29f4  07d13a8f  217d99f2        -1  d4bb7bd8  908eaeb8        -1   \n",
       "\n",
       "          C20       C21       C22       C23       C24       C25       C26  \n",
       "0          -1  0429f84b        -1  3a171ecb  c0d61a5c        -1        -1  \n",
       "1          -1  5155d8a3        -1  be7c41b4  ded4aac9        -1        -1  \n",
       "2          -1  2e01979f        -1  bcdee96c  6d5d1302        -1        -1  \n",
       "3          -1  e587c466        -1  32c7478e  3b183c5c        -1        -1  \n",
       "4    b1252a9d  0e8585d2        -1  32c7478e  0d4a6d1a  001f3601  92c878de  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "195  5840adea  34cc61bb  c9d4222a  32c7478e  e5ed7da2  ea9a246c  984e0db0  \n",
       "196        -1  d1aa4512        -1  32c7478e  9257f75f        -1        -1  \n",
       "197  a458ea53  4a2c3526        -1  32c7478e  1793a828  e8b83407  1a02cbe1  \n",
       "198  5840adea  bf647035        -1  32c7478e  1481ceb4  e8b83407  988b0775  \n",
       "199        -1        -1        -1  32c7478e        -1        -1        -1  \n",
       "\n",
       "[200 rows x 40 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, linear_feature_columns, dnn_feature_columns, l2_reg_linear=1e-5, l2_reg_embedding=1e-5,\n",
    "                 init_std=0.0001, seed=1024, task='binary', device='cpu', gpus=None):\n",
    "\n",
    "        super(BaseModel, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.dnn_feature_columns = dnn_feature_columns\n",
    "\n",
    "        self.reg_loss = torch.zeros((1,), device=device)\n",
    "        self.aux_loss = torch.zeros((1,), device=device)\n",
    "        self.device = device\n",
    "        self.gpus = gpus\n",
    "        if gpus and str(self.gpus[0]) not in self.device:\n",
    "            raise ValueError(\n",
    "                \"`gpus[0]` should be the same gpu with `device`\")\n",
    "\n",
    "        self.feature_index = build_input_features(\n",
    "            linear_feature_columns + dnn_feature_columns)\n",
    "        self.dnn_feature_columns = dnn_feature_columns\n",
    "\n",
    "        self.embedding_dict = create_embedding_matrix(dnn_feature_columns, init_std, sparse=False, device=device)\n",
    "        #         nn.ModuleDict(\n",
    "        #             {feat.embedding_name: nn.Embedding(feat.dimension, embedding_size, sparse=True) for feat in\n",
    "        #              self.dnn_feature_columns}\n",
    "        #         )\n",
    "\n",
    "        self.linear_model = Linear(\n",
    "            linear_feature_columns, self.feature_index, device=device)\n",
    "\n",
    "        self.regularization_weight = []\n",
    "\n",
    "        self.add_regularization_weight(self.embedding_dict.parameters(), l2=l2_reg_embedding)\n",
    "        self.add_regularization_weight(self.linear_model.parameters(), l2=l2_reg_linear)\n",
    "\n",
    "        self.out = PredictionLayer(task, )\n",
    "        self.to(device)\n",
    "\n",
    "        # parameters of callbacks\n",
    "        self._is_graph_network = True  # used for ModelCheckpoint\n",
    "        self.stop_training = False  # used for EarlyStopping\n",
    "        self.history = History()\n",
    "\n",
    "    def fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, initial_epoch=0, validation_split=0.,\n",
    "            validation_data=None, shuffle=True, callbacks=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: Numpy array of training data (if the model has a single input), or list of Numpy arrays (if the model has multiple inputs).If input layers in the model are named, you can also pass a\n",
    "            dictionary mapping input names to Numpy arrays.\n",
    "        :param y: Numpy array of target (label) data (if the model has a single output), or list of Numpy arrays (if the model has multiple outputs).\n",
    "        :param batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 256.\n",
    "        :param epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided. Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached.\n",
    "        :param verbose: Integer. 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "        :param initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run).\n",
    "        :param validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling.\n",
    "        :param validation_data: tuple `(x_val, y_val)` or tuple `(x_val, y_val, val_sample_weights)` on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. `validation_data` will override `validation_split`.\n",
    "        :param shuffle: Boolean. Whether to shuffle the order of the batches at the beginning of each epoch.\n",
    "        :param callbacks: List of `deepctr_torch.callbacks.Callback` instances. List of callbacks to apply during training and validation (if ). See [callbacks](https://tensorflow.google.cn/api_docs/python/tf/keras/callbacks). Now available: `EarlyStopping` , `ModelCheckpoint`\n",
    "\n",
    "        :return: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).\n",
    "        \"\"\"\n",
    "        if isinstance(x, dict):\n",
    "            x = [x[feature] for feature in self.feature_index]\n",
    "\n",
    "        do_validation = False\n",
    "        if validation_data:\n",
    "            do_validation = True\n",
    "            if len(validation_data) == 2:\n",
    "                val_x, val_y = validation_data\n",
    "                val_sample_weight = None\n",
    "            elif len(validation_data) == 3:\n",
    "                val_x, val_y, val_sample_weight = validation_data  # pylint: disable=unpacking-non-sequence\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    'When passing a `validation_data` argument, '\n",
    "                    'it must contain either 2 items (x_val, y_val), '\n",
    "                    'or 3 items (x_val, y_val, val_sample_weights), '\n",
    "                    'or alternatively it could be a dataset or a '\n",
    "                    'dataset or a dataset iterator. '\n",
    "                    'However we received `validation_data=%s`' % validation_data)\n",
    "            if isinstance(val_x, dict):\n",
    "                val_x = [val_x[feature] for feature in self.feature_index]\n",
    "\n",
    "        elif validation_split and 0. < validation_split < 1.:\n",
    "            do_validation = True\n",
    "            if hasattr(x[0], 'shape'):\n",
    "                split_at = int(x[0].shape[0] * (1. - validation_split))\n",
    "            else:\n",
    "                split_at = int(len(x[0]) * (1. - validation_split))\n",
    "            x, val_x = (slice_arrays(x, 0, split_at),\n",
    "                        slice_arrays(x, split_at))\n",
    "            y, val_y = (slice_arrays(y, 0, split_at),\n",
    "                        slice_arrays(y, split_at))\n",
    "\n",
    "        else:\n",
    "            val_x = []\n",
    "            val_y = []\n",
    "        for i in range(len(x)):\n",
    "            if len(x[i].shape) == 1:\n",
    "                x[i] = np.expand_dims(x[i], axis=1)\n",
    "\n",
    "        train_tensor_data = Data.TensorDataset(\n",
    "            torch.from_numpy(\n",
    "                np.concatenate(x, axis=-1)),\n",
    "            torch.from_numpy(y))\n",
    "        if batch_size is None:\n",
    "            batch_size = 256\n",
    "\n",
    "        model = self.train()\n",
    "        loss_func = self.loss_func\n",
    "        optim = self.optim\n",
    "\n",
    "        if self.gpus:\n",
    "            print('parallel running on these gpus:', self.gpus)\n",
    "            model = torch.nn.DataParallel(model, device_ids=self.gpus)\n",
    "            batch_size *= len(self.gpus)  # input `batch_size` is batch_size per gpu\n",
    "        else:\n",
    "            print(self.device)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            dataset=train_tensor_data, shuffle=shuffle, batch_size=batch_size)\n",
    "\n",
    "        sample_num = len(train_tensor_data)\n",
    "        steps_per_epoch = (sample_num - 1) // batch_size + 1\n",
    "\n",
    "        # configure callbacks\n",
    "        callbacks = (callbacks or []) + [self.history]  # add history callback\n",
    "        callbacks = CallbackList(callbacks)\n",
    "        callbacks.on_train_begin()\n",
    "        callbacks.set_model(self)\n",
    "        if not hasattr(callbacks, 'model'):\n",
    "            callbacks.__setattr__('model', self)\n",
    "        callbacks.model.stop_training = False\n",
    "\n",
    "        # Train\n",
    "        print(\"Train on {0} samples, validate on {1} samples, {2} steps per epoch\".format(\n",
    "            len(train_tensor_data), len(val_y), steps_per_epoch))\n",
    "        for epoch in range(initial_epoch, epochs):\n",
    "            callbacks.on_epoch_begin(epoch)\n",
    "            epoch_logs = {}\n",
    "            start_time = time.time()\n",
    "            loss_epoch = 0\n",
    "            total_loss_epoch = 0\n",
    "            train_result = {}\n",
    "            try:\n",
    "                with tqdm(enumerate(train_loader), disable=verbose != 1) as t:\n",
    "                    for _, (x_train, y_train) in t:\n",
    "                        x = x_train.to(self.device).float()\n",
    "                        y = y_train.to(self.device).float()\n",
    "\n",
    "                        y_pred = model(x).squeeze()\n",
    "\n",
    "                        optim.zero_grad()\n",
    "                        loss = loss_func(y_pred, y.squeeze(), reduction='sum')\n",
    "                        reg_loss = self.get_regularization_loss()\n",
    "\n",
    "                        total_loss = loss + reg_loss + self.aux_loss\n",
    "\n",
    "                        loss_epoch += loss.item()\n",
    "                        total_loss_epoch += total_loss.item()\n",
    "                        total_loss.backward()\n",
    "                        optim.step()\n",
    "\n",
    "                        if verbose > 0:\n",
    "                            for name, metric_fun in self.metrics.items():\n",
    "                                if name not in train_result:\n",
    "                                    train_result[name] = []\n",
    "                                train_result[name].append(metric_fun(\n",
    "                                    y.cpu().data.numpy(), y_pred.cpu().data.numpy().astype(\"float64\")))\n",
    "\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                t.close()\n",
    "                raise\n",
    "            t.close()\n",
    "\n",
    "            # Add epoch_logs\n",
    "            epoch_logs[\"loss\"] = total_loss_epoch / sample_num\n",
    "            for name, result in train_result.items():\n",
    "                epoch_logs[name] = np.sum(result) / steps_per_epoch\n",
    "\n",
    "            if do_validation:\n",
    "                eval_result = self.evaluate(val_x, val_y, batch_size)\n",
    "                for name, result in eval_result.items():\n",
    "                    epoch_logs[\"val_\" + name] = result\n",
    "            # verbose\n",
    "            if verbose > 0:\n",
    "                epoch_time = int(time.time() - start_time)\n",
    "                print('Epoch {0}/{1}'.format(epoch + 1, epochs))\n",
    "\n",
    "                eval_str = \"{0}s - loss: {1: .4f}\".format(\n",
    "                    epoch_time, epoch_logs[\"loss\"])\n",
    "\n",
    "                for name in self.metrics:\n",
    "                    eval_str += \" - \" + name + \\\n",
    "                                \": {0: .4f}\".format(epoch_logs[name])\n",
    "\n",
    "                if do_validation:\n",
    "                    for name in self.metrics:\n",
    "                        eval_str += \" - \" + \"val_\" + name + \\\n",
    "                                    \": {0: .4f}\".format(epoch_logs[\"val_\" + name])\n",
    "                print(eval_str)\n",
    "            callbacks.on_epoch_end(epoch, epoch_logs)\n",
    "            if self.stop_training:\n",
    "                break\n",
    "\n",
    "        callbacks.on_train_end()\n",
    "\n",
    "        return self.history\n",
    "\n",
    "    def evaluate(self, x, y, batch_size=256):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: Numpy array of test data (if the model has a single input), or list of Numpy arrays (if the model has multiple inputs).\n",
    "        :param y: Numpy array of target (label) data (if the model has a single output), or list of Numpy arrays (if the model has multiple outputs).\n",
    "        :param batch_size: Integer or `None`. Number of samples per evaluation step. If unspecified, `batch_size` will default to 256.\n",
    "        :return: Dict contains metric names and metric values.\n",
    "        \"\"\"\n",
    "        pred_ans = self.predict(x, batch_size)\n",
    "        eval_result = {}\n",
    "        for name, metric_fun in self.metrics.items():\n",
    "            eval_result[name] = metric_fun(y, pred_ans)\n",
    "        return eval_result\n",
    "\n",
    "    def predict(self, x, batch_size=256):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: The input data, as a Numpy array (or list of Numpy arrays if the model has multiple inputs).\n",
    "        :param batch_size: Integer. If unspecified, it will default to 256.\n",
    "        :return: Numpy array(s) of predictions.\n",
    "        \"\"\"\n",
    "        model = self.eval()\n",
    "        if isinstance(x, dict):\n",
    "            x = [x[feature] for feature in self.feature_index]\n",
    "        for i in range(len(x)):\n",
    "            if len(x[i].shape) == 1:\n",
    "                x[i] = np.expand_dims(x[i], axis=1)\n",
    "\n",
    "        tensor_data = Data.TensorDataset(\n",
    "            torch.from_numpy(np.concatenate(x, axis=-1)))\n",
    "        test_loader = DataLoader(\n",
    "            dataset=tensor_data, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "        pred_ans = []\n",
    "        with torch.no_grad():\n",
    "            for _, x_test in enumerate(test_loader):\n",
    "                x = x_test[0].to(self.device).float()\n",
    "\n",
    "                y_pred = model(x).cpu().data.numpy()  # .squeeze()\n",
    "                pred_ans.append(y_pred)\n",
    "\n",
    "        return np.concatenate(pred_ans).astype(\"float64\")\n",
    "\n",
    "    def input_from_feature_columns(self, X, feature_columns, embedding_dict, support_dense=True):\n",
    "\n",
    "        sparse_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "        dense_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, DenseFeat), feature_columns)) if len(feature_columns) else []\n",
    "\n",
    "        varlen_sparse_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if feature_columns else []\n",
    "\n",
    "        if not support_dense and len(dense_feature_columns) > 0:\n",
    "            raise ValueError(\n",
    "                \"DenseFeat is not supported in dnn_feature_columns\")\n",
    "\n",
    "        sparse_embedding_list = [embedding_dict[feat.embedding_name](\n",
    "            X[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]].long()) for\n",
    "            feat in sparse_feature_columns]\n",
    "\n",
    "        varlen_sparse_embedding_list = get_varlen_pooling_list(self.embedding_dict, X, self.feature_index,\n",
    "                                                               varlen_sparse_feature_columns, self.device)\n",
    "\n",
    "        dense_value_list = [X[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]] for feat in\n",
    "                            dense_feature_columns]\n",
    "\n",
    "        return sparse_embedding_list + varlen_sparse_embedding_list, dense_value_list\n",
    "\n",
    "    def compute_input_dim(self, feature_columns, include_sparse=True, include_dense=True, feature_group=False):\n",
    "        sparse_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, (SparseFeat, VarLenSparseFeat)), feature_columns)) if len(\n",
    "            feature_columns) else []\n",
    "        dense_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, DenseFeat), feature_columns)) if len(feature_columns) else []\n",
    "\n",
    "        dense_input_dim = sum(\n",
    "            map(lambda x: x.dimension, dense_feature_columns))\n",
    "        if feature_group:\n",
    "            sparse_input_dim = len(sparse_feature_columns)\n",
    "        else:\n",
    "            sparse_input_dim = sum(feat.embedding_dim for feat in sparse_feature_columns)\n",
    "        input_dim = 0\n",
    "        if include_sparse:\n",
    "            input_dim += sparse_input_dim\n",
    "        if include_dense:\n",
    "            input_dim += dense_input_dim\n",
    "        return input_dim\n",
    "\n",
    "    def add_regularization_weight(self, weight_list, l1=0.0, l2=0.0):\n",
    "        # For a Parameter, put it in a list to keep Compatible with get_regularization_loss()\n",
    "        if isinstance(weight_list, torch.nn.parameter.Parameter):\n",
    "            weight_list = [weight_list]\n",
    "        # For generators, filters and ParameterLists, convert them to a list of tensors to avoid bugs.\n",
    "        # e.g., we can't pickle generator objects when we save the model.\n",
    "        else:\n",
    "            weight_list = list(weight_list)\n",
    "        print(\"weight_list\", len(weight_list), weight_list)\n",
    "        self.regularization_weight.append((weight_list, l1, l2))\n",
    "\n",
    "    def get_regularization_loss(self, ):\n",
    "        total_reg_loss = torch.zeros((1,), device=self.device)\n",
    "        for weight_list, l1, l2 in self.regularization_weight:\n",
    "            for w in weight_list:\n",
    "                if isinstance(w, tuple):\n",
    "                    parameter = w[1]  # named_parameters\n",
    "                else:\n",
    "                    parameter = w\n",
    "                print(\"parameter\", parameter.shape)\n",
    "                if l1 > 0:\n",
    "                    total_reg_loss += torch.sum(l1 * torch.abs(parameter))\n",
    "                if l2 > 0:\n",
    "                    try:\n",
    "                        total_reg_loss += torch.sum(l2 * torch.square(parameter))\n",
    "                    except AttributeError:\n",
    "                        total_reg_loss += torch.sum(l2 * parameter * parameter)\n",
    "\n",
    "        return total_reg_loss\n",
    "\n",
    "    def add_auxiliary_loss(self, aux_loss, alpha):\n",
    "        self.aux_loss = aux_loss * alpha\n",
    "\n",
    "    def compile(self, optimizer,\n",
    "                loss=None,\n",
    "                metrics=None,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        :param optimizer: String (name of optimizer) or optimizer instance. See [optimizers](https://pytorch.org/docs/stable/optim.html).\n",
    "        :param loss: String (name of objective function) or objective function. See [losses](https://pytorch.org/docs/stable/nn.functional.html#loss-functions).\n",
    "        :param metrics: List of metrics to be evaluated by the model during training and testing. Typically you will use `metrics=['accuracy']`.\n",
    "        \"\"\"\n",
    "        self.metrics_names = [\"loss\"]\n",
    "        self.optim = self._get_optim(optimizer)\n",
    "        self.loss_func = self._get_loss_func(loss)\n",
    "        self.metrics = self._get_metrics(metrics)\n",
    "\n",
    "    def _get_optim(self, optimizer):\n",
    "        if isinstance(optimizer, str):\n",
    "            if optimizer == \"sgd\":\n",
    "                optim = torch.optim.SGD(self.parameters(), lr=0.01)\n",
    "            elif optimizer == \"adam\":\n",
    "                optim = torch.optim.Adam(self.parameters())  # 0.001\n",
    "            elif optimizer == \"adagrad\":\n",
    "                optim = torch.optim.Adagrad(self.parameters())  # 0.01\n",
    "            elif optimizer == \"rmsprop\":\n",
    "                optim = torch.optim.RMSprop(self.parameters())\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        else:\n",
    "            optim = optimizer\n",
    "        return optim\n",
    "\n",
    "    def _get_loss_func(self, loss):\n",
    "        if isinstance(loss, str):\n",
    "            if loss == \"binary_crossentropy\":\n",
    "                loss_func = F.binary_cross_entropy\n",
    "            elif loss == \"mse\":\n",
    "                loss_func = F.mse_loss\n",
    "            elif loss == \"mae\":\n",
    "                loss_func = F.l1_loss\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        else:\n",
    "            loss_func = loss\n",
    "        return loss_func\n",
    "\n",
    "    def _log_loss(self, y_true, y_pred, eps=1e-7, normalize=True, sample_weight=None, labels=None):\n",
    "        # change eps to improve calculation accuracy\n",
    "        return log_loss(y_true,\n",
    "                        y_pred,\n",
    "                        eps,\n",
    "                        normalize,\n",
    "                        sample_weight,\n",
    "                        labels)\n",
    "\n",
    "    def _get_metrics(self, metrics, set_eps=False):\n",
    "        metrics_ = {}\n",
    "        if metrics:\n",
    "            for metric in metrics:\n",
    "                if metric == \"binary_crossentropy\" or metric == \"logloss\":\n",
    "                    if set_eps:\n",
    "                        metrics_[metric] = self._log_loss\n",
    "                    else:\n",
    "                        metrics_[metric] = log_loss\n",
    "                if metric == \"auc\":\n",
    "                    metrics_[metric] = roc_auc_score\n",
    "                if metric == \"mse\":\n",
    "                    metrics_[metric] = mean_squared_error\n",
    "                if metric == \"accuracy\" or metric == \"acc\":\n",
    "                    metrics_[metric] = lambda y_true, y_pred: accuracy_score(\n",
    "                        y_true, np.where(y_pred > 0.5, 1, 0))\n",
    "                self.metrics_names.append(metric)\n",
    "        return metrics_\n",
    "\n",
    "    @property\n",
    "    def embedding_size(self, ):\n",
    "        feature_columns = self.dnn_feature_columns\n",
    "        sparse_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, (SparseFeat, VarLenSparseFeat)), feature_columns)) if len(\n",
    "            feature_columns) else []\n",
    "        embedding_size_set = set([feat.embedding_dim for feat in sparse_feature_columns])\n",
    "        if len(embedding_size_set) > 1:\n",
    "            raise ValueError(\"embedding_dim of SparseFeat and VarlenSparseFeat must be same in this model!\")\n",
    "        return list(embedding_size_set)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.dense_feature_columns []\n",
      "weight_list 6 [Parameter containing:\n",
      "tensor([[-9.2073e-06, -1.0325e-04, -2.7390e-04,  1.3671e-04],\n",
      "        [ 1.3341e-04,  2.5759e-05,  3.0293e-05, -2.0531e-04],\n",
      "        [ 5.4441e-05,  7.2517e-05,  3.0518e-05, -9.8938e-05],\n",
      "        [ 1.5572e-07,  8.2216e-05, -1.8979e-06, -4.6548e-06],\n",
      "        [ 1.4590e-04,  1.0519e-04,  5.1599e-05,  9.8651e-05],\n",
      "        [ 1.7420e-04,  7.9522e-05, -2.2112e-04,  6.5155e-05],\n",
      "        [-1.0004e-04,  2.3526e-05, -2.1083e-05,  1.4310e-04]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 1.0573e-04, -1.4435e-04,  1.1939e-04, -1.3196e-04],\n",
      "        [ 1.0446e-04, -3.1636e-05, -1.1692e-04, -3.8083e-05]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 6.8786e-05,  1.5395e-05, -5.2857e-05,  2.9081e-05],\n",
      "        [-1.3223e-04,  8.1198e-05,  7.7033e-05,  9.1481e-07],\n",
      "        [ 5.0202e-05, -1.2437e-04, -6.8994e-05,  3.6039e-05],\n",
      "        [-3.0208e-05,  1.0812e-05, -1.3066e-04, -1.1113e-04],\n",
      "        [ 6.1911e-05, -9.0453e-05, -1.1028e-05,  7.4364e-05],\n",
      "        [-1.7847e-04,  1.8938e-04, -3.0456e-05, -9.1763e-06],\n",
      "        [-6.7387e-06, -4.0616e-05,  1.7742e-04,  1.1777e-04],\n",
      "        [ 3.0401e-04, -6.1492e-05, -5.0069e-06,  5.2046e-06],\n",
      "        [-1.7256e-04,  9.3251e-06,  1.6399e-06, -1.9791e-05],\n",
      "        [ 8.4472e-05, -1.0754e-05, -6.1170e-06, -5.4582e-05],\n",
      "        [ 1.8916e-05, -1.2162e-07,  2.7024e-05,  9.4908e-05],\n",
      "        [ 1.8183e-04, -8.6070e-05,  5.9252e-05, -1.5735e-05],\n",
      "        [-6.0310e-05,  1.1216e-05, -1.6124e-04, -9.6801e-06],\n",
      "        [-1.7703e-04,  6.2223e-05, -1.9656e-05, -1.7094e-04],\n",
      "        [ 3.4415e-05,  1.9324e-04, -1.7472e-05, -5.9499e-06],\n",
      "        [ 1.1545e-04,  5.4405e-05,  8.8636e-05, -1.3253e-04],\n",
      "        [ 4.2003e-05,  2.9558e-05, -1.0434e-04,  4.2186e-05],\n",
      "        [-1.5244e-04, -2.8215e-06, -5.1811e-05,  1.4686e-04],\n",
      "        [ 1.1318e-04,  2.0032e-04,  1.4238e-04,  7.8319e-05],\n",
      "        [ 9.4319e-05,  7.2762e-05, -9.5901e-05, -1.1053e-04],\n",
      "        [ 1.6764e-04, -1.1199e-04,  1.0779e-05,  1.8093e-04],\n",
      "        [-1.5647e-04,  1.4461e-04,  3.5947e-05, -1.2504e-04],\n",
      "        [ 8.9602e-05,  1.3103e-05, -3.6131e-05,  1.6087e-05],\n",
      "        [ 4.9448e-05, -3.7040e-05,  1.2095e-05,  5.9407e-05],\n",
      "        [ 6.9117e-05, -5.0192e-05,  6.7819e-05, -6.5194e-05],\n",
      "        [-1.0650e-04, -1.5414e-05, -9.7093e-05,  1.0071e-04],\n",
      "        [ 1.5667e-05,  9.1103e-05, -1.5656e-04, -6.4138e-05],\n",
      "        [ 5.2961e-05, -4.2461e-05, -5.6030e-05, -8.3228e-05],\n",
      "        [ 1.1009e-04,  8.7640e-05,  9.2749e-05, -1.5937e-04],\n",
      "        [-1.1262e-04, -2.5699e-04,  3.8733e-05, -9.3140e-05],\n",
      "        [ 1.3974e-04, -1.9174e-05, -6.9233e-05, -7.8758e-05],\n",
      "        [ 1.0012e-04, -4.8198e-05, -1.3772e-04,  1.1182e-04],\n",
      "        [-8.8153e-05, -1.8395e-05,  1.5408e-04, -5.2443e-05],\n",
      "        [ 4.7697e-05, -7.7776e-05, -5.0040e-05, -4.1573e-05],\n",
      "        [ 2.1061e-04,  1.6518e-05,  9.9581e-05,  3.5726e-06],\n",
      "        [-2.0940e-04,  9.3933e-05, -2.1658e-05, -8.4829e-05],\n",
      "        [-1.6931e-05,  2.0380e-05, -6.2955e-05,  7.4003e-05],\n",
      "        [ 1.5985e-04, -9.8153e-05,  9.4519e-05, -3.6711e-05],\n",
      "        [-3.5873e-05, -8.8484e-05,  2.1109e-05,  1.3982e-04],\n",
      "        [-2.8212e-05, -2.5140e-05, -8.0495e-05, -1.1636e-05],\n",
      "        [ 3.4753e-05,  8.2589e-05, -1.7918e-04, -9.2032e-05],\n",
      "        [ 2.3668e-06,  4.4783e-05, -9.0329e-05, -2.3593e-04],\n",
      "        [ 1.0314e-04, -9.3286e-06,  1.5562e-04, -1.9404e-04],\n",
      "        [ 9.3694e-05,  1.6335e-04,  5.4161e-05,  4.1616e-05],\n",
      "        [ 5.2453e-05, -4.6926e-05,  4.5832e-05,  1.1827e-05],\n",
      "        [-9.0229e-05,  4.6877e-05,  1.5904e-04,  5.8496e-05],\n",
      "        [-1.1506e-04, -1.1839e-04,  7.8110e-05, -1.0715e-04],\n",
      "        [ 4.6617e-05, -9.4754e-05, -5.5639e-05,  1.0796e-04],\n",
      "        [ 4.2266e-05,  7.6276e-05, -9.2658e-05,  4.7990e-05],\n",
      "        [ 5.2472e-05,  8.0583e-05,  1.6125e-04, -2.2783e-05],\n",
      "        [ 6.2741e-05, -2.5243e-05,  1.6433e-04,  3.1837e-05],\n",
      "        [-1.1904e-04,  1.4405e-05, -8.5531e-05,  3.4448e-05],\n",
      "        [ 7.6856e-05, -7.1386e-05, -1.0219e-05,  8.9634e-05],\n",
      "        [ 8.2870e-05,  2.5247e-05, -4.9689e-05,  4.5812e-06],\n",
      "        [ 1.4360e-04, -2.1928e-05,  4.1947e-05,  6.8583e-05],\n",
      "        [ 4.8108e-05, -1.0319e-04,  1.7214e-04,  4.6398e-05],\n",
      "        [ 1.2523e-04, -8.0340e-05,  4.5765e-05,  1.4183e-04],\n",
      "        [ 1.0226e-05, -3.3934e-05,  1.8781e-04,  4.7474e-05],\n",
      "        [ 7.9183e-05,  1.8976e-05,  2.3072e-04, -1.4304e-04],\n",
      "        [ 1.4942e-04,  9.2840e-05, -1.9073e-05,  6.3209e-05],\n",
      "        [-3.6316e-05,  1.1509e-04, -2.7022e-05,  1.0233e-04],\n",
      "        [ 2.4746e-06, -1.1436e-04,  1.3738e-04, -2.1052e-05],\n",
      "        [ 2.8923e-04,  5.9915e-05,  6.4050e-05,  1.3090e-04],\n",
      "        [-5.8620e-05, -4.3092e-05,  5.4258e-05, -1.0310e-04],\n",
      "        [ 3.4082e-06,  1.2558e-04,  1.1600e-05,  2.3125e-05],\n",
      "        [-1.5070e-04, -1.4378e-04, -1.7349e-05,  5.9499e-06],\n",
      "        [-1.5971e-05,  1.1486e-04,  4.6217e-06, -2.0646e-05],\n",
      "        [-1.1703e-04,  8.6338e-05, -6.3129e-06, -4.8905e-05],\n",
      "        [ 6.5876e-05,  1.4097e-04, -1.2772e-04, -6.3581e-05],\n",
      "        [-1.7448e-04,  3.2164e-05, -3.0776e-05,  6.4293e-06],\n",
      "        [ 2.4733e-06,  1.9287e-05,  5.1096e-05,  4.2248e-05],\n",
      "        [-1.2187e-04, -4.9454e-05,  5.6107e-06,  6.3694e-05],\n",
      "        [-6.5426e-05, -1.4705e-04, -7.5509e-05,  9.6116e-05],\n",
      "        [ 1.7579e-05, -4.2116e-05,  5.3208e-05, -4.8354e-06],\n",
      "        [ 8.0963e-05, -7.4999e-05, -1.0880e-04,  1.9509e-05],\n",
      "        [-8.6841e-05, -4.0030e-05,  5.3517e-05, -5.9997e-05],\n",
      "        [-3.6464e-05, -2.0658e-05,  8.1438e-05, -1.8749e-05],\n",
      "        [-4.2007e-06,  6.9059e-05, -5.7328e-05,  9.9413e-05],\n",
      "        [ 4.3230e-05,  2.7735e-05,  1.9423e-06,  3.1651e-05],\n",
      "        [-5.1950e-06, -1.0229e-04,  3.5216e-05,  1.6569e-04],\n",
      "        [-2.5493e-04,  7.6930e-05, -8.2212e-05,  6.1571e-05],\n",
      "        [-2.1434e-04, -6.6604e-05,  3.2507e-05,  4.6191e-05],\n",
      "        [ 6.5227e-05,  8.5705e-05, -1.0946e-04,  1.1966e-04],\n",
      "        [ 4.0212e-05,  2.0210e-05, -1.3141e-04, -1.6088e-04],\n",
      "        [ 6.7881e-05, -5.4963e-05,  7.4576e-05,  7.7204e-05],\n",
      "        [-2.9719e-06, -1.5094e-06,  2.5470e-05,  1.7875e-04],\n",
      "        [-1.3546e-04,  1.9399e-04,  2.2813e-05, -5.7571e-05],\n",
      "        [-8.8742e-05,  1.5518e-04, -1.3315e-05,  5.6990e-05],\n",
      "        [ 7.1846e-06,  2.6226e-06, -1.6340e-04,  1.8004e-05],\n",
      "        [ 3.8156e-05, -1.8665e-05, -2.5494e-05, -1.7135e-04],\n",
      "        [-6.1561e-05,  1.8926e-04,  1.0848e-05, -7.4378e-07],\n",
      "        [ 2.2725e-05, -7.4919e-05, -9.7872e-05, -7.6595e-05],\n",
      "        [-4.6467e-05, -1.8158e-05,  3.6124e-05, -1.4849e-04],\n",
      "        [-2.5683e-05, -5.3599e-05, -5.9538e-06, -1.9379e-04],\n",
      "        [ 1.2454e-04,  1.1328e-04, -4.2379e-05, -1.3618e-04],\n",
      "        [ 1.5375e-04,  6.8366e-05, -5.1903e-05,  1.7803e-04],\n",
      "        [ 1.8069e-04, -9.8149e-07,  6.4284e-05, -4.5761e-05],\n",
      "        [ 1.4014e-04, -8.6590e-05,  3.4498e-05, -1.5287e-05],\n",
      "        [ 1.0339e-04, -1.7546e-04,  3.2815e-05, -3.1684e-05],\n",
      "        [-3.5109e-05,  1.0810e-04,  1.1439e-04, -1.1682e-05],\n",
      "        [-2.8822e-05, -1.3633e-05,  7.1481e-05,  1.5215e-05],\n",
      "        [-2.9988e-05, -6.0955e-05,  1.1485e-04, -3.6518e-05],\n",
      "        [-2.7297e-04,  8.1283e-05,  5.7063e-06,  1.0186e-04],\n",
      "        [-3.1285e-05,  1.1953e-04,  9.4761e-06, -1.0698e-04],\n",
      "        [ 1.8576e-04, -3.2780e-05, -6.0810e-05,  9.0929e-05],\n",
      "        [-2.5930e-05,  8.3827e-05,  6.5641e-05, -7.4724e-05],\n",
      "        [ 1.6208e-04,  1.7781e-05,  2.5487e-04, -1.6130e-04],\n",
      "        [-9.1004e-05,  1.0080e-05, -1.0801e-04, -7.7494e-05],\n",
      "        [-5.2315e-05, -1.7445e-04,  8.5935e-06, -6.8038e-05],\n",
      "        [ 7.1547e-05, -1.6115e-04, -1.0919e-04, -2.2697e-05],\n",
      "        [-1.0098e-04, -9.4776e-06, -5.7015e-05,  2.9888e-04],\n",
      "        [ 1.5739e-04,  1.7770e-04,  6.9235e-05,  3.0023e-05],\n",
      "        [ 5.4425e-06,  1.1340e-04, -1.5495e-05,  1.1561e-05],\n",
      "        [-3.1049e-05, -7.0632e-05,  1.9377e-04, -1.0877e-04],\n",
      "        [ 8.3203e-05,  7.4415e-05, -2.2375e-05, -1.6825e-04],\n",
      "        [ 6.5880e-05, -8.3903e-05, -2.4066e-05, -7.5682e-05],\n",
      "        [ 1.0054e-04, -1.5710e-05, -6.3477e-05,  7.2088e-05],\n",
      "        [-3.3024e-05, -6.8637e-05,  1.6578e-04, -3.9489e-05],\n",
      "        [ 9.5594e-05, -4.5137e-05,  6.5324e-05,  6.2820e-05],\n",
      "        [ 7.3713e-05, -2.2047e-04,  7.4474e-06, -1.3627e-05],\n",
      "        [-1.0061e-04, -2.1230e-05, -1.0755e-04, -7.4091e-05],\n",
      "        [-4.0605e-05,  8.6051e-05,  1.7187e-04,  1.7232e-05],\n",
      "        [ 7.2794e-05, -1.3871e-05, -3.2838e-05,  4.3642e-05],\n",
      "        [ 1.9710e-05,  4.6901e-05,  4.5268e-05,  2.5772e-05],\n",
      "        [ 7.0066e-05,  1.7760e-04, -7.9494e-05, -7.8054e-05],\n",
      "        [-5.1373e-05,  2.7486e-05, -9.0012e-05, -1.1673e-04],\n",
      "        [-6.8519e-05,  9.6196e-05, -1.4913e-04,  6.1801e-05],\n",
      "        [-2.0891e-04,  5.1780e-05,  1.2224e-04,  2.9775e-05],\n",
      "        [ 1.3237e-04,  5.6934e-05, -5.7021e-05,  9.8969e-06],\n",
      "        [-7.5352e-05,  2.5127e-05,  3.5628e-05, -6.6898e-05],\n",
      "        [ 1.2676e-04, -2.5339e-05, -1.3023e-05, -3.6036e-05],\n",
      "        [ 4.1270e-05, -1.1438e-04, -1.7780e-04,  1.4655e-05],\n",
      "        [ 1.4869e-05,  3.5739e-05, -1.7734e-04,  1.4292e-04],\n",
      "        [-1.0262e-04,  3.9862e-05,  1.6189e-05, -1.0342e-05],\n",
      "        [ 9.7043e-05, -4.3250e-05,  6.5872e-05, -8.9953e-06],\n",
      "        [ 9.5500e-06, -1.9300e-04, -1.4728e-05,  3.3455e-05],\n",
      "        [ 1.1494e-04,  3.2884e-05,  4.1441e-05,  1.3092e-05],\n",
      "        [ 6.5815e-05, -1.0840e-04, -4.8249e-05, -3.7471e-05],\n",
      "        [-7.2122e-05, -2.3906e-05,  1.4033e-05,  9.1146e-06],\n",
      "        [ 8.8518e-05, -3.5265e-06,  9.4720e-05,  1.4255e-04],\n",
      "        [ 3.0935e-05,  2.2783e-05, -1.8620e-05, -9.0140e-05],\n",
      "        [-8.8471e-05, -7.1063e-05, -2.4372e-04, -2.0084e-05],\n",
      "        [ 8.6188e-05, -2.0059e-04,  1.6752e-05,  2.6767e-04],\n",
      "        [ 5.3607e-05, -6.9594e-06,  6.5360e-05, -7.8446e-05],\n",
      "        [ 7.0923e-05, -2.4861e-05,  1.5960e-05, -6.9939e-05],\n",
      "        [-1.0628e-04,  3.4989e-05,  4.5225e-05, -7.1543e-05],\n",
      "        [ 2.1417e-05, -2.7922e-05,  1.5789e-04,  5.2407e-05],\n",
      "        [ 1.0579e-04,  5.8532e-05, -1.0660e-04,  1.7610e-05],\n",
      "        [-4.6907e-05, -7.1883e-05, -8.8624e-05,  1.6479e-04],\n",
      "        [ 8.7118e-05,  7.1542e-05,  1.7370e-04, -9.7928e-05],\n",
      "        [-8.5281e-05, -1.0867e-04, -1.3641e-04, -2.7427e-04],\n",
      "        [-8.6194e-05,  1.3616e-04, -6.4184e-05,  3.1990e-07],\n",
      "        [-3.0659e-05, -1.1101e-04, -1.2966e-04,  6.8522e-05],\n",
      "        [-1.3724e-04, -4.5187e-05,  1.8461e-04,  2.3212e-05],\n",
      "        [-1.7261e-04, -7.3288e-05,  1.7780e-05,  1.6168e-04],\n",
      "        [-4.8355e-05, -3.2776e-05,  1.5989e-04, -1.8122e-04],\n",
      "        [ 3.9172e-05, -2.7559e-04, -1.6260e-05, -1.8010e-04],\n",
      "        [ 1.7725e-05, -4.2146e-05, -9.9368e-06,  1.2819e-04],\n",
      "        [ 5.1182e-05, -1.0139e-04,  1.5103e-05,  5.0374e-05],\n",
      "        [-4.5801e-05,  4.3634e-05, -2.0606e-04, -1.8518e-04],\n",
      "        [ 1.3589e-04,  1.5008e-04, -2.5430e-05,  1.3615e-05],\n",
      "        [ 7.2643e-05,  7.4402e-05, -8.8689e-05, -3.4221e-05],\n",
      "        [ 4.5593e-05,  1.0452e-04,  6.1486e-05, -1.8380e-05],\n",
      "        [ 1.8208e-05,  1.3725e-04, -3.5127e-05, -2.4223e-04],\n",
      "        [-9.7748e-05,  5.5668e-05,  5.0511e-05, -1.0070e-04],\n",
      "        [-1.5266e-05,  1.7646e-04,  4.8270e-05, -2.0968e-05],\n",
      "        [ 1.7079e-04, -2.0185e-04, -1.5200e-04, -8.5855e-05],\n",
      "        [ 6.9216e-05, -1.3382e-04, -8.0385e-05, -1.0038e-04],\n",
      "        [-2.6833e-05, -2.4872e-05,  7.9505e-05,  1.0065e-04],\n",
      "        [ 3.6095e-05, -1.0453e-04,  2.8034e-05, -1.3757e-04],\n",
      "        [ 2.4728e-04,  8.0270e-05,  8.0810e-05, -1.0030e-04],\n",
      "        [-4.9186e-05,  6.7352e-05, -7.9309e-05,  8.6040e-05],\n",
      "        [-1.1660e-04, -1.3584e-04,  1.5310e-04,  8.8681e-05],\n",
      "        [-1.1028e-04,  1.2366e-04, -1.0488e-04, -2.0406e-05],\n",
      "        [ 1.0219e-04, -1.5252e-04,  1.4906e-05,  3.0980e-05],\n",
      "        [ 3.6209e-05, -1.1226e-04,  1.0009e-04,  2.6403e-04],\n",
      "        [-5.0053e-05, -1.1308e-04, -1.1066e-04,  1.8817e-05],\n",
      "        [ 1.5708e-05, -5.5319e-05,  1.3518e-04, -9.0098e-05],\n",
      "        [ 1.3516e-04, -8.4195e-05,  1.5526e-04, -9.3801e-06],\n",
      "        [ 2.2973e-05,  6.5754e-06,  5.5492e-05,  2.4304e-05],\n",
      "        [-2.4255e-05, -7.6251e-05, -8.9869e-05,  1.1341e-04],\n",
      "        [-1.8729e-05, -1.3130e-04,  1.9968e-04,  7.2807e-05],\n",
      "        [ 1.4079e-04,  8.4607e-05,  6.0661e-05,  1.6942e-04],\n",
      "        [ 1.6985e-04,  1.3561e-04, -3.6905e-05, -1.3769e-05],\n",
      "        [ 1.3506e-04,  8.8115e-05, -3.1076e-05,  8.4313e-05],\n",
      "        [ 2.0539e-04,  9.7699e-06, -1.1525e-04,  9.1926e-05],\n",
      "        [-9.7279e-05, -6.2891e-05,  1.1895e-04,  1.7333e-05]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 8.0145e-06, -5.4022e-05, -1.9403e-04, -2.4376e-04],\n",
      "        [ 1.0973e-04, -5.7315e-05, -1.2010e-05, -6.9735e-05],\n",
      "        [-2.0461e-05, -1.2481e-04,  2.1241e-04, -4.1985e-05],\n",
      "        [-8.5268e-05,  8.2262e-05, -1.0083e-05, -1.1580e-04],\n",
      "        [ 1.7310e-05, -1.0896e-04, -5.8802e-05, -1.1306e-04],\n",
      "        [ 1.3669e-05,  4.4985e-05,  1.3087e-04, -9.4020e-05],\n",
      "        [-2.4519e-05,  5.3408e-05, -9.4919e-05,  1.3643e-04],\n",
      "        [-1.2694e-04, -3.7249e-05, -2.1421e-04, -5.0754e-05],\n",
      "        [ 6.0421e-05,  6.6370e-05,  6.6342e-05,  3.3213e-05],\n",
      "        [ 1.5190e-05,  9.2122e-05,  1.9000e-05, -4.5505e-05],\n",
      "        [-1.1309e-05, -1.2528e-05, -4.4440e-06,  1.8127e-04],\n",
      "        [-1.1664e-04,  7.5411e-05,  2.1818e-04,  7.1894e-05],\n",
      "        [ 1.0187e-04, -3.9644e-05,  6.9383e-05,  6.5968e-05],\n",
      "        [-1.7436e-04,  2.8719e-05,  3.4407e-05, -1.3593e-04],\n",
      "        [-8.8096e-05,  3.0528e-06, -2.4168e-05,  7.6082e-06],\n",
      "        [-1.9430e-04, -6.6582e-06, -1.8283e-05, -1.7515e-04],\n",
      "        [-1.0240e-04,  3.2270e-05,  7.0496e-05, -9.2647e-05],\n",
      "        [ 1.0491e-04,  1.0644e-04, -1.9194e-05, -5.7403e-05],\n",
      "        [-9.7932e-06, -9.3971e-05, -1.6727e-04,  4.8239e-06],\n",
      "        [-1.4187e-04, -5.6752e-05, -1.0572e-04,  5.8013e-05]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-1.8555e-04, -7.3180e-05,  2.6279e-05,  3.7406e-05],\n",
      "        [ 2.2064e-05,  4.1374e-05,  1.5008e-05, -1.8906e-04],\n",
      "        [-4.2906e-06,  5.6603e-05, -2.1228e-04, -1.7074e-04],\n",
      "        [-8.2263e-05, -1.5556e-04,  5.2462e-05, -8.0025e-05],\n",
      "        [-7.3173e-05,  3.3364e-05, -3.8742e-05, -1.2164e-04],\n",
      "        [-1.2445e-04,  8.2930e-05,  7.4732e-06,  6.9774e-05],\n",
      "        [ 1.7160e-04,  9.6895e-05, -4.7601e-05,  1.0206e-05],\n",
      "        [ 1.4474e-05,  1.3343e-04,  1.1397e-04, -1.0879e-04],\n",
      "        [ 8.5667e-05, -7.3958e-05,  5.7940e-05, -7.0877e-05],\n",
      "        [ 5.2243e-05, -3.5177e-05,  1.5913e-04,  2.2119e-05],\n",
      "        [ 1.0433e-04,  1.2714e-05, -8.3786e-05, -5.8389e-05],\n",
      "        [-1.8891e-05, -1.9809e-06,  1.5299e-04, -1.1590e-04],\n",
      "        [-1.2184e-04,  1.1482e-04,  7.7180e-05, -2.8762e-05],\n",
      "        [-7.9591e-05,  8.9529e-05, -1.0862e-04,  1.1047e-04],\n",
      "        [ 8.6146e-05, -9.6049e-05, -1.4932e-04,  4.3571e-06],\n",
      "        [-1.3289e-04,  5.5333e-05, -8.1765e-05, -1.1995e-05],\n",
      "        [-8.0694e-05,  5.0950e-05,  1.6174e-05, -8.2236e-05],\n",
      "        [-1.1753e-04, -1.0588e-04,  1.3374e-04,  1.0623e-04],\n",
      "        [ 7.6600e-05,  5.4391e-05, -1.2326e-04,  7.6384e-05],\n",
      "        [-1.5471e-05,  6.7439e-05,  9.4961e-05,  5.6471e-06],\n",
      "        [-8.2610e-05, -8.1789e-05,  2.8957e-05,  7.5113e-05],\n",
      "        [ 2.7459e-05, -1.9035e-04,  6.4466e-06, -1.4844e-05],\n",
      "        [-1.0966e-04,  1.0028e-04,  6.2943e-05, -3.5295e-05],\n",
      "        [-1.6718e-04, -9.9070e-05, -2.3543e-04,  2.2847e-05],\n",
      "        [-1.5055e-04,  1.0447e-04, -2.5062e-05,  1.5600e-04],\n",
      "        [ 8.1256e-05,  1.1494e-04,  2.9502e-06,  1.9755e-05],\n",
      "        [ 8.2979e-05, -2.1170e-07, -2.1745e-05,  7.6761e-06],\n",
      "        [-1.4503e-04, -2.9748e-06,  1.4657e-05,  3.8629e-05],\n",
      "        [-7.8032e-05,  1.4933e-04, -4.2275e-05, -2.3330e-04],\n",
      "        [-7.4631e-05,  2.2992e-04, -1.0620e-04, -2.2520e-04],\n",
      "        [ 6.8730e-05, -1.7427e-04,  1.7742e-04, -1.6388e-05],\n",
      "        [ 2.4936e-05,  1.1408e-04,  4.0393e-05,  8.7529e-06],\n",
      "        [ 1.1904e-04, -7.4128e-05,  1.7815e-04, -1.4304e-04],\n",
      "        [-7.6919e-05,  4.8650e-05, -1.1310e-04, -3.4193e-05],\n",
      "        [ 2.5787e-05, -5.4127e-06,  1.4754e-05,  7.5978e-05],\n",
      "        [ 1.3126e-04, -1.3569e-04, -1.4953e-04,  7.1394e-06],\n",
      "        [-1.2418e-04,  2.3003e-05, -3.7687e-05, -1.0188e-04],\n",
      "        [ 1.1301e-04,  4.4715e-05,  2.3046e-05, -1.4046e-04],\n",
      "        [ 7.1511e-05,  7.7689e-08,  5.0331e-05,  2.7207e-05],\n",
      "        [-2.4731e-05, -1.3547e-05,  1.3579e-04,  8.6827e-05],\n",
      "        [-2.6303e-05, -2.0117e-04,  7.7249e-05, -3.4469e-05],\n",
      "        [-4.2020e-05,  2.5785e-05,  5.9847e-05, -2.5660e-05],\n",
      "        [ 7.6075e-05, -9.0000e-05, -1.9004e-05, -6.1620e-05],\n",
      "        [-2.3659e-04, -1.8830e-04, -1.4981e-04, -8.1912e-05],\n",
      "        [ 1.5190e-05, -2.6406e-04, -7.7540e-06, -8.6842e-05],\n",
      "        [-1.6980e-04,  1.8384e-04,  6.1981e-05,  1.6320e-04],\n",
      "        [ 9.4741e-05, -7.2071e-05, -1.7364e-05, -1.3429e-04],\n",
      "        [-2.1820e-05, -9.8610e-06,  1.3032e-04,  2.7127e-05],\n",
      "        [-1.0816e-04,  5.8987e-05, -1.9265e-04, -1.7147e-04],\n",
      "        [-5.1686e-05, -6.0175e-06,  3.3049e-05, -2.0865e-05],\n",
      "        [-1.5036e-05, -4.6159e-05,  1.1736e-04,  2.0743e-05],\n",
      "        [-1.0729e-04,  6.7577e-05,  8.9857e-05, -3.7216e-05],\n",
      "        [ 1.9505e-04, -8.2803e-05, -1.1749e-04,  1.0963e-05],\n",
      "        [ 7.1256e-05, -4.2291e-05, -3.8993e-05,  4.1190e-05],\n",
      "        [ 6.4211e-05,  1.7215e-04, -1.5337e-04, -1.2059e-04],\n",
      "        [ 1.2990e-04,  1.7239e-05,  8.2683e-05, -1.0205e-04],\n",
      "        [ 7.7637e-05,  3.6340e-05,  5.5505e-05,  2.5357e-05],\n",
      "        [ 3.1732e-06, -5.5885e-05,  1.9738e-05, -3.0301e-05],\n",
      "        [-1.4403e-04,  2.5780e-06,  7.0639e-05, -5.6256e-05],\n",
      "        [ 5.2017e-05,  1.4679e-04,  2.8644e-05, -1.5316e-04],\n",
      "        [ 1.4802e-04,  7.1595e-05,  6.5320e-05,  2.7239e-05],\n",
      "        [ 7.1582e-05, -1.7099e-04, -7.0420e-05,  4.4065e-05],\n",
      "        [-7.8013e-05,  7.7076e-05,  9.3141e-05, -5.0940e-05],\n",
      "        [-6.3187e-05, -1.4478e-04, -4.3157e-05, -1.4472e-04],\n",
      "        [ 3.5173e-05,  3.6693e-05,  4.8870e-05, -1.0142e-04],\n",
      "        [ 6.7191e-06, -5.4876e-05, -1.2597e-04, -1.4639e-05],\n",
      "        [-4.7592e-05, -1.0616e-04,  1.1114e-04, -2.3062e-05],\n",
      "        [ 9.7830e-05, -5.1983e-05, -2.8390e-05, -1.2144e-04],\n",
      "        [ 4.4473e-05,  1.3596e-07, -2.9185e-05,  1.0691e-04],\n",
      "        [ 6.0163e-05,  2.8181e-05, -5.0990e-05,  4.8183e-05],\n",
      "        [ 1.4640e-05,  1.8982e-05, -9.8569e-05, -1.1390e-05],\n",
      "        [ 2.2556e-04,  2.3775e-05, -5.5164e-05, -6.2332e-05],\n",
      "        [ 2.1905e-05, -1.0711e-04,  1.4500e-05, -7.6975e-06],\n",
      "        [-2.8238e-05, -6.3350e-05, -3.4659e-05, -2.8757e-05],\n",
      "        [ 1.4017e-04,  1.3982e-04, -2.4046e-04, -3.0182e-05],\n",
      "        [-8.0322e-05,  1.4818e-04, -1.3078e-04, -1.0872e-04],\n",
      "        [-1.3886e-04, -1.8634e-04,  1.9647e-04,  8.0885e-05],\n",
      "        [-2.5698e-05, -2.0302e-04,  7.3127e-05,  7.2554e-05],\n",
      "        [ 9.4856e-05,  1.0762e-04,  8.4253e-05, -3.1390e-05],\n",
      "        [ 3.6066e-05,  1.0613e-04,  6.4733e-05, -7.4096e-05],\n",
      "        [-6.4534e-05,  7.5856e-05,  7.0897e-05,  1.0370e-05],\n",
      "        [ 5.9901e-05, -1.3911e-04, -4.9228e-05,  1.3225e-05],\n",
      "        [-5.1917e-05,  1.6967e-04,  6.9686e-05, -7.9763e-06],\n",
      "        [-2.7858e-05, -9.3517e-06,  1.6173e-05, -2.7552e-05],\n",
      "        [ 1.3613e-04,  3.2535e-05,  2.9130e-05,  7.0743e-05],\n",
      "        [ 1.7804e-04, -1.4657e-04,  5.9661e-05,  2.0611e-04],\n",
      "        [ 1.4967e-04,  2.4139e-04,  2.7146e-05, -3.1003e-05],\n",
      "        [ 6.0835e-05, -1.2937e-04, -1.5268e-04,  1.6902e-04],\n",
      "        [ 7.2714e-05,  1.7092e-04, -5.9324e-05,  2.4560e-04],\n",
      "        [ 5.1190e-05,  4.6952e-05, -2.6194e-05, -9.7243e-05],\n",
      "        [ 1.2664e-05, -1.5705e-04,  4.8369e-05, -1.6943e-04],\n",
      "        [-2.1665e-04,  4.4845e-05, -8.2783e-05, -1.7044e-04],\n",
      "        [-5.9569e-05,  1.8538e-04,  9.2650e-08,  1.6730e-05],\n",
      "        [ 2.4176e-05, -2.4338e-05, -1.3687e-04, -7.2678e-05],\n",
      "        [ 2.1511e-05, -1.3839e-04, -1.0041e-04,  2.1481e-05],\n",
      "        [ 3.7281e-05, -5.8202e-06,  1.2516e-04,  3.6003e-05],\n",
      "        [ 5.6991e-05, -5.2487e-05,  2.1022e-05,  1.7824e-04],\n",
      "        [ 1.1639e-04, -6.0388e-05, -3.7168e-06,  3.1317e-05],\n",
      "        [ 8.2770e-05, -1.3246e-05, -2.7789e-05,  2.6136e-05],\n",
      "        [-9.4071e-05,  1.8689e-06,  5.3038e-05, -1.9959e-05],\n",
      "        [ 7.8401e-05, -5.7556e-05, -1.7684e-04,  2.5017e-05],\n",
      "        [-1.0523e-04,  8.8489e-06, -4.2960e-05,  6.2661e-05],\n",
      "        [ 1.0089e-04, -1.1790e-04, -1.6178e-04, -1.0771e-04],\n",
      "        [ 1.8896e-04,  8.2475e-05, -3.5223e-05, -5.3874e-05],\n",
      "        [ 5.9822e-05, -3.2207e-05, -9.5084e-05, -9.2446e-05],\n",
      "        [ 1.1620e-05,  7.5752e-05,  1.1346e-04, -2.7230e-05],\n",
      "        [ 3.7846e-05, -1.0529e-04,  7.1451e-05,  8.2635e-05],\n",
      "        [ 5.4611e-05, -6.8257e-05,  4.1052e-05,  1.9919e-05],\n",
      "        [-2.0190e-05,  1.0547e-04, -8.2228e-05,  9.1838e-05],\n",
      "        [ 1.6312e-04, -3.3641e-05, -9.1103e-05,  2.4900e-05],\n",
      "        [-3.6215e-05,  1.0013e-05, -2.3577e-05,  5.5629e-05],\n",
      "        [ 6.7466e-05,  4.9880e-05,  8.9781e-05,  1.0475e-04],\n",
      "        [-2.1547e-05,  1.5382e-05, -1.2068e-04, -4.7143e-05],\n",
      "        [ 8.5152e-05,  8.1929e-05, -1.2668e-05,  4.0465e-05],\n",
      "        [-1.9735e-04,  1.0088e-04, -1.0733e-04,  1.1466e-04],\n",
      "        [-2.0248e-05, -4.1444e-05, -3.6854e-05, -1.2348e-05],\n",
      "        [-8.3797e-05,  4.6966e-05,  3.2190e-05,  8.2751e-05],\n",
      "        [-1.3304e-04, -4.8712e-05,  6.5017e-05,  2.7804e-05],\n",
      "        [-1.0993e-04, -7.3868e-05, -3.3155e-05, -6.8357e-05],\n",
      "        [ 9.3519e-05,  9.0042e-05, -8.0891e-05, -1.2873e-04],\n",
      "        [-2.8726e-05, -5.7773e-05,  4.3327e-05, -1.5004e-04],\n",
      "        [ 1.5687e-04, -1.4106e-05, -1.1863e-04,  9.1832e-06],\n",
      "        [ 2.4950e-05, -3.5261e-05, -1.2248e-05, -9.7616e-05],\n",
      "        [ 1.0553e-04,  5.4504e-05, -7.1829e-05, -3.1288e-05],\n",
      "        [-8.5098e-05,  9.2970e-05,  5.0344e-05, -3.0125e-05],\n",
      "        [ 3.5716e-05,  4.2751e-05,  1.0641e-04,  4.6112e-05],\n",
      "        [-1.1264e-04,  1.5537e-04,  1.5623e-04, -2.7490e-05],\n",
      "        [ 1.2919e-04,  1.0080e-04, -1.1414e-04, -1.2456e-04],\n",
      "        [ 6.0754e-05,  5.7843e-05,  1.7188e-05,  7.1213e-05],\n",
      "        [ 5.8211e-05,  9.1769e-06,  5.8977e-05, -2.3153e-06],\n",
      "        [-1.3134e-04, -7.1491e-05, -2.0961e-05, -1.6238e-05],\n",
      "        [-3.4504e-05, -1.1806e-04,  3.1562e-05, -5.3306e-05],\n",
      "        [ 1.2011e-04,  9.2610e-05, -6.9803e-05, -5.0775e-05],\n",
      "        [ 1.3986e-04, -1.6853e-04, -5.7482e-05,  6.8153e-05],\n",
      "        [-1.0898e-04,  2.1070e-04, -2.2392e-04, -1.9828e-05],\n",
      "        [-1.4953e-04, -1.6421e-05,  3.4747e-05,  2.2762e-05],\n",
      "        [ 1.1718e-05,  9.8553e-05, -6.6270e-05, -4.9107e-05],\n",
      "        [ 2.0604e-06,  2.3848e-05, -5.0380e-05,  6.3090e-05],\n",
      "        [ 8.0937e-05,  1.9718e-04, -9.9136e-05, -1.9288e-05],\n",
      "        [ 4.3453e-05,  7.6236e-05, -3.4145e-05,  3.3072e-05],\n",
      "        [-2.6825e-05, -7.8407e-05, -1.0210e-04, -6.7698e-05],\n",
      "        [-1.7491e-06,  1.8715e-04, -1.0674e-04, -4.7103e-05],\n",
      "        [-8.5393e-05, -1.2632e-04, -1.0811e-04, -2.3117e-05],\n",
      "        [ 1.1144e-04, -1.0237e-04,  9.1026e-06, -8.7000e-05],\n",
      "        [ 3.1993e-05, -8.0662e-05,  9.2088e-05,  2.3905e-05],\n",
      "        [-1.1985e-04,  7.2140e-05,  8.7465e-05,  1.1912e-04],\n",
      "        [-2.3794e-05,  1.0611e-04,  1.5642e-05, -1.4960e-04],\n",
      "        [ 7.2545e-05, -2.1920e-05, -3.8876e-05,  1.7716e-04],\n",
      "        [-6.2169e-05, -1.6438e-06, -2.4874e-05, -4.8828e-05],\n",
      "        [-1.1373e-05, -2.2584e-05, -5.6025e-07, -1.7302e-04],\n",
      "        [-3.1464e-05,  3.5274e-05, -2.7527e-05,  8.9696e-05],\n",
      "        [ 6.0570e-05,  1.0418e-05, -1.6682e-04, -2.9815e-04],\n",
      "        [-1.8379e-04,  5.1631e-05,  8.6242e-05,  6.3430e-05],\n",
      "        [-5.2063e-05, -4.3199e-05,  5.0583e-05, -1.4620e-04],\n",
      "        [ 2.0021e-04,  1.4935e-05, -9.0394e-05, -4.8806e-05],\n",
      "        [-9.1631e-05,  8.1380e-05,  2.6362e-04,  6.1145e-05],\n",
      "        [ 1.7266e-06, -1.1490e-04,  4.6919e-05,  2.8588e-04],\n",
      "        [-1.8602e-04,  4.4694e-05, -4.1065e-05, -9.0780e-05],\n",
      "        [-6.5430e-05, -2.5265e-04, -8.6476e-05, -1.0710e-04],\n",
      "        [ 7.0418e-05,  3.3018e-05,  1.0296e-04, -1.1872e-04],\n",
      "        [-6.0235e-05, -1.2801e-04, -7.9656e-05, -5.4384e-05],\n",
      "        [-4.5234e-05,  2.2415e-05,  4.7745e-05, -4.8376e-05],\n",
      "        [ 1.5698e-04,  9.8218e-05,  4.1385e-06, -9.8772e-05],\n",
      "        [-1.5672e-04, -3.0226e-04,  4.8620e-05,  1.4721e-04],\n",
      "        [-9.5729e-05,  1.0798e-05, -3.7303e-05,  3.2005e-06],\n",
      "        [ 6.8237e-05, -2.1498e-05, -3.6390e-05, -5.2316e-07],\n",
      "        [-8.8264e-05, -1.4952e-04, -8.9985e-06, -2.8029e-05],\n",
      "        [-1.3602e-04, -1.2218e-05,  1.9818e-04,  9.9154e-06],\n",
      "        [-1.6544e-04, -7.4470e-06,  2.0612e-05,  7.8412e-08],\n",
      "        [-2.0621e-04,  1.6232e-04,  1.5590e-05, -7.9661e-05],\n",
      "        [ 7.5474e-05, -1.5115e-04,  7.3026e-05, -1.6831e-04],\n",
      "        [ 4.2939e-05, -4.2902e-05,  6.9306e-05, -2.0715e-05],\n",
      "        [-5.7703e-05, -1.5168e-04,  1.8490e-06, -3.4803e-05],\n",
      "        [-9.6742e-05, -9.7396e-05, -5.9045e-05, -7.6008e-05],\n",
      "        [ 1.4840e-06, -2.3219e-04,  9.9966e-05,  1.0710e-04],\n",
      "        [ 1.6810e-04, -7.5751e-05, -1.7561e-05,  8.2092e-05],\n",
      "        [ 2.4164e-05, -1.2091e-04, -1.4113e-04,  2.3137e-05],\n",
      "        [ 1.2416e-04, -9.6856e-05,  7.8056e-05, -5.6031e-06],\n",
      "        [-4.3471e-05,  2.0835e-05, -5.1111e-05,  1.3888e-05],\n",
      "        [ 7.1004e-06,  3.1848e-05,  1.6096e-04,  8.6629e-07],\n",
      "        [-1.9958e-05,  8.1014e-05,  1.9374e-05, -3.6392e-05],\n",
      "        [ 6.4765e-05,  7.2197e-06,  1.0230e-05,  1.7254e-04],\n",
      "        [ 1.6219e-04,  3.1442e-05,  2.4785e-04, -1.2843e-04],\n",
      "        [ 1.4337e-05,  1.3466e-05,  1.3665e-04, -5.5599e-05],\n",
      "        [-1.3334e-05,  2.0664e-04, -5.4353e-05, -1.0350e-05],\n",
      "        [ 2.2040e-05, -2.6456e-04, -1.0401e-04,  7.2603e-06],\n",
      "        [ 1.6295e-04, -9.9608e-06,  1.1369e-04, -9.0588e-06],\n",
      "        [ 7.7292e-05,  3.1703e-05,  2.0507e-05,  1.4552e-04],\n",
      "        [-1.1485e-04, -1.1573e-04,  2.5722e-05,  7.8830e-05],\n",
      "        [-1.7365e-04, -1.5591e-04,  2.4249e-04, -8.5858e-05],\n",
      "        [-8.0148e-05, -7.6773e-05,  5.1355e-05, -7.4322e-05],\n",
      "        [ 1.2174e-04, -1.4976e-04,  3.7606e-06, -1.8516e-05],\n",
      "        [ 1.0449e-04, -1.1163e-04, -3.8854e-05, -7.6076e-05]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-1.4989e-04, -3.7073e-05, -8.4666e-05, -2.5786e-05],\n",
      "        [ 1.6171e-04, -5.4541e-05,  5.7321e-05,  1.5910e-04],\n",
      "        [ 1.9101e-05,  1.2152e-04,  7.2635e-06,  1.0792e-05],\n",
      "        [ 9.6835e-05, -7.2800e-05, -6.1092e-06, -3.5503e-05],\n",
      "        [-4.9045e-05,  6.1452e-05, -1.8674e-04, -4.6050e-05],\n",
      "        [-1.1580e-04,  1.0046e-04,  1.5169e-04, -1.9976e-05],\n",
      "        [-1.6408e-04, -2.4203e-05,  6.7335e-05, -4.3860e-05],\n",
      "        [-2.0602e-04,  2.3623e-04,  2.1913e-05,  6.5144e-05],\n",
      "        [ 1.0247e-04,  1.0831e-04, -2.7193e-06, -8.7655e-05],\n",
      "        [ 7.4247e-06,  4.1253e-06,  1.2966e-05, -4.8652e-05],\n",
      "        [-6.0549e-05,  5.3779e-05, -8.6500e-05,  6.3748e-05],\n",
      "        [-5.8216e-06, -1.0166e-04,  3.6720e-05, -1.5543e-04],\n",
      "        [-3.7435e-05,  2.5003e-04,  1.8023e-04,  4.6466e-05],\n",
      "        [-7.2683e-05,  4.2025e-05,  1.2629e-04, -1.1198e-04],\n",
      "        [-6.2591e-05, -5.6437e-05, -6.7626e-05,  8.2794e-05],\n",
      "        [ 1.8901e-04,  9.8749e-05,  4.3476e-05, -1.8437e-05],\n",
      "        [-1.3010e-04, -3.1317e-05, -2.5568e-05,  7.3262e-05],\n",
      "        [ 3.5560e-05, -6.8252e-05,  1.5442e-05,  2.6345e-05],\n",
      "        [ 1.3282e-04,  1.4226e-04, -1.7175e-04,  1.3614e-05],\n",
      "        [-2.3184e-05, -5.9939e-05, -3.8468e-05, -1.4382e-05],\n",
      "        [-1.4393e-04, -3.8548e-05, -5.6724e-06, -2.5102e-05],\n",
      "        [-7.6798e-05,  8.1187e-05, -9.4058e-06,  1.6047e-04],\n",
      "        [ 8.7048e-05,  9.5448e-05,  1.0538e-04, -1.0492e-04],\n",
      "        [ 3.7531e-05,  1.0376e-04, -5.3752e-05,  2.3506e-05],\n",
      "        [-1.3379e-05, -2.8579e-04,  1.8115e-04,  1.4139e-05],\n",
      "        [ 4.5641e-05,  7.8793e-05,  1.3170e-04, -9.9230e-05],\n",
      "        [-6.2179e-06,  1.1668e-05,  2.5812e-05,  5.2752e-05],\n",
      "        [-7.0950e-05,  4.7441e-05,  8.4101e-05, -8.3024e-05],\n",
      "        [ 5.3238e-05, -2.0453e-05,  2.8819e-05,  1.6204e-04],\n",
      "        [ 1.6452e-04, -2.2345e-04,  5.3309e-05,  6.2802e-05],\n",
      "        [-1.2608e-04,  1.2284e-04,  1.3057e-04,  1.3451e-04],\n",
      "        [-3.7903e-05, -2.6109e-04, -7.2106e-05,  8.3441e-05],\n",
      "        [ 9.9727e-06,  6.1030e-05, -1.1263e-05, -1.2404e-05],\n",
      "        [ 2.2287e-05, -1.1141e-05,  4.5724e-05,  1.8404e-05],\n",
      "        [ 1.3443e-04, -1.0901e-05, -3.3656e-05,  6.0974e-05],\n",
      "        [ 5.2070e-05, -4.2253e-05, -5.7485e-05,  8.3472e-06],\n",
      "        [-1.6251e-04, -7.0941e-05, -4.5216e-05,  3.8075e-05],\n",
      "        [-1.6679e-04,  9.5260e-05, -3.1543e-05,  3.1679e-05],\n",
      "        [ 1.6605e-04, -7.0301e-05, -7.6507e-05, -7.6532e-05],\n",
      "        [ 6.4758e-05, -1.3948e-04, -1.1075e-04, -2.7991e-04],\n",
      "        [-4.5323e-05,  1.0349e-04, -6.9899e-05, -2.3520e-04],\n",
      "        [ 8.8768e-05, -4.1877e-05, -7.5391e-07, -9.8482e-06],\n",
      "        [ 5.6555e-05, -7.6254e-05, -5.7220e-05,  3.4112e-05],\n",
      "        [ 7.1830e-05, -2.4224e-05,  3.8008e-05,  1.2767e-05],\n",
      "        [ 2.3862e-06, -1.0680e-04, -6.7504e-05, -3.6740e-05],\n",
      "        [ 6.8488e-05, -1.1892e-04, -8.5653e-05,  1.3892e-04],\n",
      "        [-1.4103e-04,  4.9065e-05,  1.9517e-04,  6.4340e-05],\n",
      "        [-2.5715e-05,  1.1690e-05,  1.1637e-04,  1.2227e-04],\n",
      "        [ 1.7837e-05, -1.7375e-05,  6.3928e-05, -2.3785e-04],\n",
      "        [-2.5775e-05,  4.4555e-05,  4.1930e-05,  2.6973e-05],\n",
      "        [ 7.8153e-07, -1.0242e-04, -8.0720e-05,  6.2929e-05],\n",
      "        [-5.6631e-05,  6.7700e-05, -7.5322e-07,  6.6134e-05],\n",
      "        [-1.6146e-04, -9.7892e-05,  1.1985e-04,  2.7726e-05],\n",
      "        [ 1.4188e-04,  9.6422e-05, -7.0026e-05, -1.4024e-04],\n",
      "        [ 7.5982e-05, -6.0185e-05, -6.0855e-05,  5.2292e-06],\n",
      "        [-6.0530e-06,  1.0824e-04,  2.1010e-05, -5.3922e-05],\n",
      "        [-2.3237e-05,  2.2797e-04, -7.7963e-05, -2.7578e-05],\n",
      "        [-5.8231e-05,  8.5377e-05,  2.6892e-05, -4.1863e-05],\n",
      "        [-1.6318e-04,  9.6798e-05,  1.6001e-04, -1.1675e-04],\n",
      "        [-1.2496e-04, -8.6578e-05,  7.7537e-05, -1.3380e-04],\n",
      "        [ 8.6831e-05,  1.3913e-04,  8.0657e-05,  3.8945e-05],\n",
      "        [ 6.4910e-05, -2.1227e-04,  1.1250e-04,  1.2800e-04],\n",
      "        [ 1.1120e-04,  1.7159e-04, -2.1232e-05, -8.4488e-05],\n",
      "        [-6.4087e-05,  3.3535e-05,  6.9653e-06, -7.3328e-05],\n",
      "        [ 1.6587e-04,  2.5403e-05,  5.2285e-06, -3.2991e-05],\n",
      "        [ 1.2947e-04, -1.5377e-04,  5.5759e-05, -6.4541e-05],\n",
      "        [ 6.1085e-05, -7.6531e-05,  5.0133e-05, -1.7274e-05],\n",
      "        [ 5.6206e-05, -7.3829e-05, -5.1384e-05, -7.9117e-05],\n",
      "        [ 6.3400e-05, -7.5574e-05,  1.8991e-05, -1.1073e-04],\n",
      "        [-2.7166e-05, -1.0554e-04, -2.1933e-04, -1.3593e-04],\n",
      "        [ 1.2148e-05,  1.9419e-04, -4.4522e-05, -7.6857e-05],\n",
      "        [-1.0806e-04, -3.9363e-05, -1.8181e-04, -3.2857e-05],\n",
      "        [ 2.6816e-05, -2.2015e-05,  6.9671e-05,  1.0638e-04],\n",
      "        [-1.0024e-05,  8.3550e-05, -2.7491e-05,  5.8580e-05],\n",
      "        [ 1.1633e-04, -7.9504e-05, -2.0168e-04,  1.7720e-04],\n",
      "        [-4.1294e-05, -2.0823e-05,  6.9328e-05,  1.5286e-04],\n",
      "        [ 1.5247e-04,  6.6383e-05,  1.3597e-04, -2.2385e-05],\n",
      "        [-2.9892e-05,  2.8525e-05,  1.8000e-04,  3.5800e-05],\n",
      "        [-1.2021e-05, -7.3482e-06, -1.1609e-04, -2.5150e-05],\n",
      "        [-1.1893e-05,  7.3201e-06, -1.8125e-04,  8.7575e-05],\n",
      "        [-4.9210e-06, -6.1464e-05, -5.1618e-05,  9.6474e-05],\n",
      "        [-2.8690e-04, -2.0061e-04, -1.9264e-04,  4.7214e-05],\n",
      "        [ 1.0942e-04, -2.9908e-05, -4.2422e-05, -4.8597e-05],\n",
      "        [ 2.5103e-05, -2.2602e-05, -2.3282e-05, -1.3774e-04],\n",
      "        [ 2.1023e-06,  3.5711e-05, -7.1748e-05, -2.3509e-06],\n",
      "        [-1.9990e-04, -9.9116e-05, -8.6920e-06, -8.9563e-05],\n",
      "        [-1.4900e-04,  3.8123e-06, -5.4838e-05,  1.4297e-04],\n",
      "        [ 1.6911e-05, -1.4104e-05, -6.2320e-05, -2.6146e-05],\n",
      "        [ 1.9526e-04, -1.1260e-06,  1.5211e-04, -3.1763e-05],\n",
      "        [-5.7236e-05,  1.7109e-04, -2.0727e-04, -8.5983e-06],\n",
      "        [ 2.3535e-04, -4.0745e-05, -6.7857e-05,  1.2629e-04],\n",
      "        [ 2.6650e-05, -1.1971e-04,  3.5425e-05,  1.3327e-05],\n",
      "        [-4.1452e-06,  7.9580e-05,  5.9515e-05,  3.7964e-05],\n",
      "        [ 1.3607e-04, -1.3402e-04, -7.9276e-05, -1.1218e-04],\n",
      "        [-1.0149e-04,  1.1583e-05,  3.4987e-05, -3.8195e-05],\n",
      "        [ 5.3965e-05, -5.4194e-05, -5.1298e-05, -1.7770e-04],\n",
      "        [-3.1752e-05, -6.5107e-05, -8.3037e-05,  1.7304e-04],\n",
      "        [-2.1232e-04, -1.4120e-04, -5.9600e-05, -1.0199e-04],\n",
      "        [-1.1770e-04,  6.3461e-05, -3.0870e-05, -1.0418e-05],\n",
      "        [-9.0774e-05, -6.6967e-05,  1.5317e-04, -1.3424e-04],\n",
      "        [-5.8651e-05,  1.0530e-04,  3.7686e-06,  6.1392e-06],\n",
      "        [-5.6390e-05,  3.4605e-05, -9.7656e-05, -4.0365e-05],\n",
      "        [-3.9162e-05, -2.2055e-04, -9.5582e-05, -2.2402e-05],\n",
      "        [-8.2830e-05,  1.3292e-04,  3.0350e-05,  1.9072e-04],\n",
      "        [-6.6463e-05, -2.2506e-04,  2.0774e-04,  4.6209e-05],\n",
      "        [-1.1301e-04,  4.5810e-05, -1.0860e-04, -1.1747e-04],\n",
      "        [-4.2653e-05, -6.5683e-06, -3.3112e-05, -1.3739e-04],\n",
      "        [-1.3793e-04,  1.0681e-04, -5.8239e-05,  7.1684e-05],\n",
      "        [-7.0229e-05, -6.6324e-05,  4.3870e-05,  1.4396e-04],\n",
      "        [ 9.8033e-05,  6.9957e-06,  1.2853e-04, -1.8231e-04],\n",
      "        [-1.1873e-04, -1.1736e-04,  2.2201e-04,  4.4028e-05],\n",
      "        [-3.1341e-05, -4.4785e-05, -3.7177e-05, -5.7572e-05],\n",
      "        [-1.4315e-05,  1.1923e-04,  3.2313e-05, -1.0652e-04],\n",
      "        [-7.3639e-05,  1.6219e-05, -1.0089e-05,  8.4604e-05],\n",
      "        [-1.3837e-05,  1.5410e-05, -1.3084e-05, -8.7852e-05],\n",
      "        [-1.2183e-04,  1.4428e-04,  2.2583e-05, -1.0557e-05],\n",
      "        [-1.6456e-04, -2.1321e-05, -1.0211e-04, -8.9983e-05],\n",
      "        [-4.2508e-05, -3.4900e-05,  1.6113e-04,  6.7755e-05],\n",
      "        [-1.0100e-04,  1.4797e-04, -5.8030e-05, -8.5238e-05],\n",
      "        [-9.4430e-05,  2.8639e-05, -1.9182e-04, -1.9707e-06],\n",
      "        [ 4.0124e-06,  1.7527e-04, -2.7858e-05,  7.3733e-05],\n",
      "        [ 6.7703e-05,  1.6774e-04,  1.7610e-04,  8.4804e-05],\n",
      "        [ 8.0503e-05, -1.2236e-04,  1.0935e-04,  1.4039e-04],\n",
      "        [ 1.4629e-04,  4.6978e-05,  1.9180e-04, -9.0231e-05],\n",
      "        [ 6.2205e-06, -1.3977e-04, -5.2921e-05,  2.4246e-04],\n",
      "        [-9.5134e-06, -6.4880e-06, -1.0713e-04,  2.1783e-05],\n",
      "        [-1.0466e-04, -2.5089e-05,  6.7539e-05, -1.6788e-05],\n",
      "        [ 1.2529e-04, -1.2569e-04,  2.1415e-05,  2.9669e-05],\n",
      "        [-7.0342e-06, -9.0478e-05, -5.0117e-06, -1.1574e-04],\n",
      "        [ 1.4515e-04,  6.8048e-05,  1.2325e-05,  5.1256e-05],\n",
      "        [ 7.3578e-05,  9.0734e-05,  9.6130e-05,  1.4009e-04],\n",
      "        [-2.6070e-05,  1.1036e-04, -1.4345e-04,  9.3455e-05],\n",
      "        [ 5.2190e-05, -1.8582e-04,  1.3487e-04,  6.3760e-05],\n",
      "        [ 7.9971e-05,  1.2670e-06,  1.7673e-04,  1.3426e-04],\n",
      "        [ 1.1154e-05, -2.2156e-04,  4.5639e-05, -2.5962e-05],\n",
      "        [ 1.4001e-04, -4.3008e-05,  1.2726e-05, -5.1032e-05],\n",
      "        [-9.8861e-05,  8.9249e-06,  1.6412e-04,  4.8162e-05],\n",
      "        [ 1.0220e-04, -7.8737e-05, -3.8649e-05,  6.4298e-05],\n",
      "        [ 6.5555e-05, -9.0873e-05,  8.1080e-06, -3.8253e-05],\n",
      "        [-2.3771e-05, -2.7301e-05, -4.2275e-05, -1.0474e-04],\n",
      "        [-4.8012e-06, -3.7863e-05, -9.6645e-05,  6.7066e-05],\n",
      "        [ 4.1635e-05,  2.5675e-04, -3.8106e-06,  1.2370e-04],\n",
      "        [ 1.5293e-04,  2.3349e-05,  1.6049e-04,  2.8508e-05],\n",
      "        [ 6.5767e-05, -1.7897e-05, -7.6777e-05, -7.7041e-05],\n",
      "        [ 2.2516e-05,  7.1616e-08, -1.1227e-06, -6.4235e-05],\n",
      "        [ 8.0811e-05,  8.6290e-05,  1.3430e-04, -7.3283e-06],\n",
      "        [-4.8717e-05, -1.3163e-04, -1.2407e-04,  6.1949e-05],\n",
      "        [-1.2291e-04,  3.7609e-05, -8.1966e-06, -9.1476e-05],\n",
      "        [-1.2206e-04, -6.2845e-05, -1.0027e-04, -7.9811e-05],\n",
      "        [ 6.5270e-06,  1.9572e-05,  2.8955e-05,  3.6848e-05],\n",
      "        [ 1.0701e-04, -8.4355e-05,  8.6339e-05,  4.9522e-05],\n",
      "        [ 7.0707e-06, -1.2049e-05, -5.6427e-05, -1.0706e-04],\n",
      "        [ 9.3136e-05, -3.5760e-05, -1.8268e-04,  1.2619e-05],\n",
      "        [-4.5254e-05,  5.8770e-05,  2.3941e-04, -1.2180e-04],\n",
      "        [-2.3081e-04,  3.7089e-05, -1.3980e-04,  5.1586e-06],\n",
      "        [-8.1483e-05, -9.2135e-05, -8.6829e-05,  7.5907e-05],\n",
      "        [ 5.0913e-05,  1.5685e-04,  1.6434e-04, -3.0711e-05],\n",
      "        [ 3.2774e-06,  8.4324e-05,  3.3875e-05, -9.2719e-05],\n",
      "        [-4.9510e-05, -1.7115e-05,  1.1099e-04, -2.8229e-05],\n",
      "        [ 1.6902e-04, -3.5754e-05, -5.5357e-05,  1.6304e-04],\n",
      "        [ 3.2374e-05, -3.7491e-05,  1.2188e-04, -7.7977e-06],\n",
      "        [ 2.0556e-05, -1.0135e-05, -2.3389e-05,  9.9293e-05],\n",
      "        [-1.0533e-04,  9.9496e-05, -6.0565e-06,  5.9848e-05],\n",
      "        [ 1.2960e-05,  8.5644e-05, -9.4174e-05,  5.5344e-07],\n",
      "        [ 1.7832e-04, -3.1057e-05,  1.0284e-04, -1.7362e-04],\n",
      "        [-1.5224e-05, -9.5958e-05,  3.6998e-05,  6.6578e-05],\n",
      "        [-1.6556e-04, -3.8473e-05,  3.6959e-05,  1.4981e-05],\n",
      "        [-7.4211e-05,  5.0930e-05,  1.0613e-04, -1.5213e-05],\n",
      "        [-8.7351e-06, -9.0531e-05, -2.4649e-04,  1.4943e-05],\n",
      "        [ 5.2217e-05, -5.3494e-05,  9.6439e-05,  1.8764e-06],\n",
      "        [-6.2446e-05, -2.5651e-04,  1.7968e-05, -5.7668e-05],\n",
      "        [ 4.3242e-05, -3.6364e-05, -9.4163e-05, -1.9706e-04],\n",
      "        [-7.7960e-05,  3.1438e-05,  7.5687e-05, -1.9661e-04],\n",
      "        [-1.8626e-04, -2.1997e-04, -1.3955e-05,  2.4592e-05],\n",
      "        [ 3.1118e-05,  1.0355e-04, -1.0519e-04, -7.6806e-05],\n",
      "        [ 2.9112e-05,  1.1041e-04,  4.4961e-06,  2.8642e-05],\n",
      "        [ 4.6721e-06, -9.5303e-05,  3.1266e-10,  2.6990e-05],\n",
      "        [ 2.9661e-05,  7.9551e-05,  8.2904e-05,  1.0748e-04],\n",
      "        [-2.5360e-04, -4.2841e-05,  9.4914e-05,  5.8427e-05],\n",
      "        [ 1.0453e-04, -9.6755e-05,  1.5100e-05,  6.3961e-05],\n",
      "        [ 4.8714e-05,  3.7344e-05,  3.9560e-05,  4.4943e-05],\n",
      "        [-1.1381e-04,  2.2370e-04, -1.4272e-04, -2.8710e-05],\n",
      "        [ 8.5795e-05,  4.9169e-05, -1.9326e-04, -3.1573e-05],\n",
      "        [ 3.6484e-05, -2.4352e-04,  1.4179e-05, -8.7232e-05],\n",
      "        [-1.9462e-04,  9.0917e-06, -3.5090e-05, -4.6852e-05],\n",
      "        [ 2.3780e-05,  1.5607e-06,  1.3098e-04,  1.0028e-04],\n",
      "        [-2.2401e-04,  8.3498e-05,  6.0817e-06, -6.0089e-05],\n",
      "        [ 7.9528e-05,  8.1188e-05,  5.3385e-05, -1.7118e-05]],\n",
      "       requires_grad=True)]\n",
      "weight_list 6 [Parameter containing:\n",
      "tensor([[-1.9748e-05],\n",
      "        [ 1.6969e-05],\n",
      "        [ 8.9782e-05],\n",
      "        [ 1.2498e-04],\n",
      "        [-8.5211e-05],\n",
      "        [ 1.2590e-04],\n",
      "        [-4.3096e-05]], requires_grad=True), Parameter containing:\n",
      "tensor([[ 6.1917e-05],\n",
      "        [-1.5377e-05]], requires_grad=True), Parameter containing:\n",
      "tensor([[ 5.5449e-05],\n",
      "        [-1.0929e-04],\n",
      "        [ 8.8991e-05],\n",
      "        [ 5.4031e-05],\n",
      "        [-9.9852e-05],\n",
      "        [ 1.4914e-05],\n",
      "        [ 1.0499e-04],\n",
      "        [ 8.9088e-06],\n",
      "        [ 8.5460e-05],\n",
      "        [-2.2737e-04],\n",
      "        [ 2.5896e-04],\n",
      "        [-8.7201e-05],\n",
      "        [ 3.1016e-05],\n",
      "        [ 7.0454e-05],\n",
      "        [ 3.2006e-05],\n",
      "        [-6.8040e-05],\n",
      "        [-1.4825e-04],\n",
      "        [ 6.4449e-05],\n",
      "        [ 2.6108e-06],\n",
      "        [ 1.1841e-04],\n",
      "        [ 1.0493e-04],\n",
      "        [ 1.5057e-04],\n",
      "        [-5.5054e-05],\n",
      "        [ 7.7492e-06],\n",
      "        [ 1.4270e-04],\n",
      "        [ 7.9536e-06],\n",
      "        [-5.1661e-05],\n",
      "        [-7.7438e-05],\n",
      "        [ 2.0193e-04],\n",
      "        [ 1.3115e-04],\n",
      "        [ 3.2078e-06],\n",
      "        [-1.0944e-05],\n",
      "        [-8.4001e-05],\n",
      "        [ 4.3703e-05],\n",
      "        [-4.8440e-05],\n",
      "        [ 4.6146e-05],\n",
      "        [-1.0242e-04],\n",
      "        [-1.5429e-05],\n",
      "        [ 2.1824e-04],\n",
      "        [ 1.0885e-04],\n",
      "        [ 1.8868e-04],\n",
      "        [-1.4574e-05],\n",
      "        [ 2.8601e-05],\n",
      "        [ 8.2750e-05],\n",
      "        [ 3.1751e-05],\n",
      "        [-4.8220e-05],\n",
      "        [ 3.5565e-05],\n",
      "        [-6.2080e-05],\n",
      "        [ 1.6655e-04],\n",
      "        [-1.8185e-05],\n",
      "        [-1.6187e-04],\n",
      "        [ 4.3361e-05],\n",
      "        [ 5.8642e-05],\n",
      "        [-1.1763e-04],\n",
      "        [ 1.0109e-04],\n",
      "        [ 3.6791e-05],\n",
      "        [-1.2517e-04],\n",
      "        [-1.5766e-05],\n",
      "        [ 1.0747e-05],\n",
      "        [-1.3751e-04],\n",
      "        [ 1.1237e-05],\n",
      "        [ 9.4264e-05],\n",
      "        [ 4.9677e-05],\n",
      "        [-1.6872e-04],\n",
      "        [-2.9116e-05],\n",
      "        [ 1.2981e-05],\n",
      "        [ 2.3035e-04],\n",
      "        [ 3.1802e-05],\n",
      "        [-5.9356e-05],\n",
      "        [-9.8939e-06],\n",
      "        [ 6.9856e-05],\n",
      "        [ 8.3458e-05],\n",
      "        [ 1.8802e-04],\n",
      "        [ 1.0516e-04],\n",
      "        [-2.0208e-04],\n",
      "        [ 1.7807e-04],\n",
      "        [-2.2593e-04],\n",
      "        [ 3.6493e-05],\n",
      "        [-2.0749e-04],\n",
      "        [ 6.2014e-05],\n",
      "        [-1.8672e-04],\n",
      "        [ 3.7391e-05],\n",
      "        [ 9.9832e-06],\n",
      "        [ 8.2602e-05],\n",
      "        [-1.8919e-04],\n",
      "        [ 5.5702e-05],\n",
      "        [ 2.5040e-06],\n",
      "        [ 6.6310e-05],\n",
      "        [-1.3146e-04],\n",
      "        [ 7.3970e-05],\n",
      "        [-5.6175e-05],\n",
      "        [ 6.3961e-05],\n",
      "        [-6.3581e-05],\n",
      "        [-8.0652e-05],\n",
      "        [-7.9886e-05],\n",
      "        [ 1.5772e-05],\n",
      "        [ 5.0919e-05],\n",
      "        [ 1.0226e-04],\n",
      "        [-1.6100e-04],\n",
      "        [-7.0405e-05],\n",
      "        [ 9.5630e-05],\n",
      "        [ 1.3359e-04],\n",
      "        [ 1.5741e-04],\n",
      "        [-3.1061e-05],\n",
      "        [ 1.5253e-05],\n",
      "        [-3.4042e-05],\n",
      "        [ 1.1845e-05],\n",
      "        [ 5.9476e-06],\n",
      "        [ 1.9797e-04],\n",
      "        [ 1.6653e-04],\n",
      "        [-1.9012e-05],\n",
      "        [ 5.6462e-05],\n",
      "        [-2.9975e-05],\n",
      "        [ 1.5486e-05],\n",
      "        [-6.8088e-06],\n",
      "        [-9.8851e-05],\n",
      "        [-1.7007e-04],\n",
      "        [ 2.6181e-05],\n",
      "        [ 8.0262e-05],\n",
      "        [-4.3089e-06],\n",
      "        [-6.6664e-05],\n",
      "        [-1.4978e-04],\n",
      "        [ 4.3629e-05],\n",
      "        [ 4.3815e-05],\n",
      "        [-1.6875e-04],\n",
      "        [ 1.0991e-04],\n",
      "        [-1.0958e-04],\n",
      "        [ 3.1254e-05],\n",
      "        [-1.3472e-05],\n",
      "        [-1.2256e-04],\n",
      "        [ 1.0174e-05],\n",
      "        [ 1.0679e-06],\n",
      "        [ 1.1675e-04],\n",
      "        [-1.4270e-04],\n",
      "        [-5.3372e-05],\n",
      "        [ 1.0006e-04],\n",
      "        [ 1.3971e-04],\n",
      "        [ 3.0096e-05],\n",
      "        [ 5.3366e-05],\n",
      "        [ 9.9993e-05],\n",
      "        [ 6.1111e-06],\n",
      "        [-6.8079e-05],\n",
      "        [ 1.0083e-05],\n",
      "        [ 8.8758e-06],\n",
      "        [ 4.4854e-05],\n",
      "        [ 6.9444e-05],\n",
      "        [ 1.7390e-04],\n",
      "        [ 6.6849e-05],\n",
      "        [ 1.2653e-04],\n",
      "        [-3.3767e-05],\n",
      "        [ 1.2790e-05],\n",
      "        [ 2.0159e-04],\n",
      "        [-7.1129e-05],\n",
      "        [-2.3559e-05],\n",
      "        [-6.8161e-05],\n",
      "        [ 4.5422e-05],\n",
      "        [ 6.5392e-05],\n",
      "        [-3.3447e-05],\n",
      "        [ 1.9030e-04],\n",
      "        [ 1.3294e-04],\n",
      "        [ 5.8534e-05],\n",
      "        [ 1.4878e-05],\n",
      "        [-7.9375e-06],\n",
      "        [ 8.7959e-05],\n",
      "        [-7.0727e-05],\n",
      "        [ 2.0194e-05],\n",
      "        [-1.1706e-04],\n",
      "        [-1.0698e-04],\n",
      "        [ 3.0485e-05],\n",
      "        [-3.5439e-05],\n",
      "        [-1.1505e-05],\n",
      "        [-4.3202e-05],\n",
      "        [-1.9588e-04],\n",
      "        [-1.1819e-04],\n",
      "        [ 1.1189e-04],\n",
      "        [ 9.5614e-05],\n",
      "        [-5.7892e-05],\n",
      "        [ 7.9358e-05],\n",
      "        [ 1.5726e-05],\n",
      "        [-6.4983e-05],\n",
      "        [-1.0951e-04],\n",
      "        [-1.4853e-04],\n",
      "        [-3.1006e-04],\n",
      "        [ 3.1667e-05],\n",
      "        [-1.6562e-05],\n",
      "        [-1.1137e-04],\n",
      "        [ 1.0685e-04]], requires_grad=True), Parameter containing:\n",
      "tensor([[-6.2068e-05],\n",
      "        [-9.1326e-05],\n",
      "        [ 8.7254e-05],\n",
      "        [ 2.1509e-05],\n",
      "        [-1.2472e-04],\n",
      "        [-7.7827e-05],\n",
      "        [ 1.3559e-04],\n",
      "        [ 1.4405e-04],\n",
      "        [ 1.3586e-04],\n",
      "        [-1.8235e-04],\n",
      "        [ 6.3436e-05],\n",
      "        [ 1.6053e-04],\n",
      "        [-8.6590e-05],\n",
      "        [ 1.5111e-04],\n",
      "        [ 9.2472e-05],\n",
      "        [ 1.2627e-04],\n",
      "        [-6.8594e-06],\n",
      "        [ 3.6081e-05],\n",
      "        [-5.0930e-05],\n",
      "        [-1.0670e-04]], requires_grad=True), Parameter containing:\n",
      "tensor([[-8.0417e-05],\n",
      "        [ 1.1092e-04],\n",
      "        [-8.0279e-05],\n",
      "        [-5.0183e-05],\n",
      "        [-9.2710e-05],\n",
      "        [ 8.1777e-05],\n",
      "        [-2.2696e-04],\n",
      "        [ 4.6429e-05],\n",
      "        [ 4.5752e-05],\n",
      "        [ 6.6819e-05],\n",
      "        [-1.6064e-04],\n",
      "        [-1.7419e-05],\n",
      "        [ 9.6481e-05],\n",
      "        [ 9.8658e-05],\n",
      "        [-1.3935e-04],\n",
      "        [-6.0330e-05],\n",
      "        [-4.5165e-06],\n",
      "        [-1.1150e-04],\n",
      "        [ 8.5759e-05],\n",
      "        [-5.6413e-05],\n",
      "        [ 9.7091e-05],\n",
      "        [-1.2180e-04],\n",
      "        [-1.2041e-04],\n",
      "        [-1.1923e-04],\n",
      "        [-1.6112e-05],\n",
      "        [ 5.8889e-05],\n",
      "        [-1.0100e-04],\n",
      "        [-3.1830e-05],\n",
      "        [-1.3249e-04],\n",
      "        [-2.5634e-05],\n",
      "        [ 1.5859e-04],\n",
      "        [-2.5388e-04],\n",
      "        [-1.0069e-04],\n",
      "        [-4.2301e-06],\n",
      "        [ 9.0747e-05],\n",
      "        [ 3.8031e-06],\n",
      "        [-2.4984e-05],\n",
      "        [ 2.3616e-05],\n",
      "        [-1.0333e-04],\n",
      "        [ 1.0585e-04],\n",
      "        [ 1.1071e-04],\n",
      "        [-7.7449e-05],\n",
      "        [-1.6134e-04],\n",
      "        [ 1.0477e-05],\n",
      "        [-5.4250e-05],\n",
      "        [ 7.8780e-05],\n",
      "        [ 1.3337e-04],\n",
      "        [-6.8863e-05],\n",
      "        [-1.2142e-04],\n",
      "        [ 1.0472e-04],\n",
      "        [-1.6682e-04],\n",
      "        [ 4.4657e-06],\n",
      "        [-6.1420e-05],\n",
      "        [ 1.4366e-04],\n",
      "        [-4.0128e-05],\n",
      "        [ 1.8599e-05],\n",
      "        [-5.1881e-05],\n",
      "        [-1.6999e-04],\n",
      "        [ 6.9836e-05],\n",
      "        [ 2.1282e-04],\n",
      "        [ 2.2468e-05],\n",
      "        [ 5.4502e-05],\n",
      "        [-1.5914e-04],\n",
      "        [-9.3710e-05],\n",
      "        [-1.9581e-04],\n",
      "        [ 1.8437e-04],\n",
      "        [-9.9014e-05],\n",
      "        [-1.3419e-04],\n",
      "        [ 3.9425e-05],\n",
      "        [-8.6719e-06],\n",
      "        [-1.1649e-04],\n",
      "        [-2.1488e-04],\n",
      "        [-2.8758e-05],\n",
      "        [-1.9354e-04],\n",
      "        [ 1.1178e-04],\n",
      "        [-7.1381e-05],\n",
      "        [-1.0889e-05],\n",
      "        [ 6.2749e-05],\n",
      "        [ 2.6259e-05],\n",
      "        [-1.9926e-05],\n",
      "        [ 7.4968e-05],\n",
      "        [ 8.1641e-05],\n",
      "        [ 1.6991e-05],\n",
      "        [-4.0175e-05],\n",
      "        [-1.6066e-04],\n",
      "        [-2.0253e-05],\n",
      "        [ 1.8435e-05],\n",
      "        [-9.3878e-05],\n",
      "        [ 1.8223e-05],\n",
      "        [-8.9271e-05],\n",
      "        [ 1.1592e-04],\n",
      "        [-3.3982e-05],\n",
      "        [-3.0081e-05],\n",
      "        [ 1.0303e-04],\n",
      "        [-4.6936e-05],\n",
      "        [-1.6086e-04],\n",
      "        [-8.0302e-05],\n",
      "        [-5.7626e-06],\n",
      "        [ 1.0331e-04],\n",
      "        [ 7.5692e-05],\n",
      "        [-7.3778e-05],\n",
      "        [ 1.1915e-04],\n",
      "        [ 4.6850e-05],\n",
      "        [-2.6629e-05],\n",
      "        [-4.7562e-05],\n",
      "        [ 1.2657e-04],\n",
      "        [-1.7508e-04],\n",
      "        [-6.1687e-05],\n",
      "        [-1.3324e-04],\n",
      "        [ 6.0459e-05],\n",
      "        [ 3.0346e-05],\n",
      "        [-1.1073e-04],\n",
      "        [-9.4741e-05],\n",
      "        [ 6.3220e-05],\n",
      "        [-1.5024e-04],\n",
      "        [-5.7440e-05],\n",
      "        [ 2.1438e-04],\n",
      "        [ 8.4001e-05],\n",
      "        [-7.5759e-06],\n",
      "        [-6.0933e-05],\n",
      "        [-1.3919e-04],\n",
      "        [-1.3893e-05],\n",
      "        [-1.1093e-04],\n",
      "        [ 7.3408e-06],\n",
      "        [ 1.0847e-04],\n",
      "        [-3.1249e-05],\n",
      "        [ 2.5264e-06],\n",
      "        [-1.0008e-04],\n",
      "        [ 8.6991e-05],\n",
      "        [ 1.1171e-04],\n",
      "        [ 1.7419e-04],\n",
      "        [-1.2848e-04],\n",
      "        [ 1.6792e-05],\n",
      "        [-5.8617e-05],\n",
      "        [ 1.5426e-04],\n",
      "        [ 4.3364e-05],\n",
      "        [ 1.1107e-04],\n",
      "        [ 2.2079e-04],\n",
      "        [-7.0584e-05],\n",
      "        [-8.7846e-05],\n",
      "        [-5.1326e-05],\n",
      "        [ 8.4152e-05],\n",
      "        [-2.0122e-04],\n",
      "        [-1.0296e-05],\n",
      "        [ 9.7132e-05],\n",
      "        [-6.1969e-05],\n",
      "        [ 7.1907e-05],\n",
      "        [-1.4672e-04],\n",
      "        [-8.4841e-05],\n",
      "        [ 1.5574e-05],\n",
      "        [-2.2082e-04],\n",
      "        [ 1.4002e-04],\n",
      "        [ 1.7869e-04],\n",
      "        [-1.0610e-04],\n",
      "        [-2.0838e-04],\n",
      "        [ 8.2462e-05],\n",
      "        [ 2.5876e-05],\n",
      "        [ 4.9921e-05],\n",
      "        [ 7.4650e-05],\n",
      "        [ 1.2564e-04],\n",
      "        [ 9.3573e-05],\n",
      "        [-1.7720e-05],\n",
      "        [-1.0074e-04],\n",
      "        [-9.8189e-05],\n",
      "        [ 1.3949e-04],\n",
      "        [ 3.7877e-05],\n",
      "        [-1.1136e-04],\n",
      "        [-1.3248e-05],\n",
      "        [ 2.4869e-05],\n",
      "        [-6.0647e-05],\n",
      "        [-6.9039e-05],\n",
      "        [-8.3130e-05],\n",
      "        [-5.4011e-05],\n",
      "        [ 1.3146e-04],\n",
      "        [ 5.0020e-05],\n",
      "        [-1.9981e-05],\n",
      "        [-5.1599e-05],\n",
      "        [-4.2155e-06],\n",
      "        [-1.6467e-05],\n",
      "        [ 7.2414e-05],\n",
      "        [-8.1314e-06],\n",
      "        [-5.8980e-06],\n",
      "        [ 7.0760e-05],\n",
      "        [ 7.7703e-05],\n",
      "        [-9.1797e-05],\n",
      "        [ 1.6166e-04],\n",
      "        [-1.3754e-04],\n",
      "        [ 8.3272e-05],\n",
      "        [-5.2327e-05],\n",
      "        [-2.7024e-05],\n",
      "        [ 7.4432e-06],\n",
      "        [ 1.0056e-04],\n",
      "        [-2.6088e-04]], requires_grad=True), Parameter containing:\n",
      "tensor([[ 4.7593e-05],\n",
      "        [-1.4110e-04],\n",
      "        [ 1.8301e-04],\n",
      "        [-8.0964e-05],\n",
      "        [ 2.9920e-06],\n",
      "        [ 1.0483e-04],\n",
      "        [-3.3862e-05],\n",
      "        [ 6.4992e-05],\n",
      "        [ 1.0162e-04],\n",
      "        [-8.5931e-06],\n",
      "        [ 5.4508e-05],\n",
      "        [-8.0581e-05],\n",
      "        [ 1.4252e-05],\n",
      "        [-4.6053e-05],\n",
      "        [ 1.3319e-04],\n",
      "        [ 2.2547e-05],\n",
      "        [-1.9970e-05],\n",
      "        [ 8.6275e-05],\n",
      "        [ 2.0789e-04],\n",
      "        [ 1.6718e-04],\n",
      "        [-1.4581e-04],\n",
      "        [ 6.9639e-05],\n",
      "        [ 1.5180e-05],\n",
      "        [-6.3620e-05],\n",
      "        [ 1.9303e-04],\n",
      "        [-7.9664e-05],\n",
      "        [-4.4943e-05],\n",
      "        [ 1.2873e-04],\n",
      "        [ 1.7065e-04],\n",
      "        [-8.4571e-05],\n",
      "        [-4.5864e-05],\n",
      "        [-4.1645e-05],\n",
      "        [ 5.7401e-06],\n",
      "        [-1.0083e-04],\n",
      "        [ 2.8556e-05],\n",
      "        [ 2.1057e-04],\n",
      "        [-5.7282e-05],\n",
      "        [ 9.5141e-05],\n",
      "        [-2.1104e-04],\n",
      "        [-1.3247e-04],\n",
      "        [ 4.5145e-05],\n",
      "        [ 8.5794e-05],\n",
      "        [-1.6803e-04],\n",
      "        [ 4.7188e-05],\n",
      "        [-7.7351e-05],\n",
      "        [ 5.9743e-05],\n",
      "        [-3.2636e-05],\n",
      "        [-1.5221e-04],\n",
      "        [-1.8276e-04],\n",
      "        [-1.6727e-04],\n",
      "        [-1.4644e-04],\n",
      "        [ 1.8577e-04],\n",
      "        [-6.9495e-05],\n",
      "        [-2.4105e-04],\n",
      "        [ 1.7126e-04],\n",
      "        [-1.5679e-04],\n",
      "        [-6.8438e-05],\n",
      "        [-8.3892e-05],\n",
      "        [-1.1212e-05],\n",
      "        [ 1.4852e-04],\n",
      "        [ 1.1356e-04],\n",
      "        [-1.0833e-04],\n",
      "        [-2.7536e-05],\n",
      "        [-1.2478e-05],\n",
      "        [-1.4891e-04],\n",
      "        [ 3.5891e-05],\n",
      "        [ 1.2128e-04],\n",
      "        [-4.3893e-05],\n",
      "        [ 1.0332e-04],\n",
      "        [ 7.2870e-05],\n",
      "        [ 1.7626e-05],\n",
      "        [-2.3358e-05],\n",
      "        [ 1.4554e-05],\n",
      "        [ 1.2983e-04],\n",
      "        [-1.1183e-04],\n",
      "        [-1.2359e-05],\n",
      "        [ 2.9258e-05],\n",
      "        [-6.8230e-05],\n",
      "        [ 1.0307e-06],\n",
      "        [-6.3872e-05],\n",
      "        [ 7.8592e-05],\n",
      "        [ 3.0079e-05],\n",
      "        [-1.5635e-04],\n",
      "        [-2.5791e-05],\n",
      "        [ 2.3788e-05],\n",
      "        [-4.3542e-05],\n",
      "        [ 1.3454e-04],\n",
      "        [ 1.9469e-05],\n",
      "        [ 2.0826e-04],\n",
      "        [ 1.3241e-04],\n",
      "        [ 1.4757e-04],\n",
      "        [ 9.2699e-05],\n",
      "        [ 8.9364e-05],\n",
      "        [ 3.6158e-05],\n",
      "        [ 7.3651e-05],\n",
      "        [ 7.2373e-05],\n",
      "        [-7.6355e-05],\n",
      "        [-7.4918e-05],\n",
      "        [ 1.3502e-04],\n",
      "        [-8.7931e-05],\n",
      "        [ 1.7077e-04],\n",
      "        [-1.2594e-04],\n",
      "        [-3.1518e-05],\n",
      "        [-2.3560e-04],\n",
      "        [ 1.0592e-04],\n",
      "        [-1.8906e-05],\n",
      "        [ 1.0690e-04],\n",
      "        [-4.0143e-07],\n",
      "        [-1.0800e-04],\n",
      "        [-1.2396e-04],\n",
      "        [ 7.1497e-05],\n",
      "        [ 1.4114e-05],\n",
      "        [ 1.3777e-04],\n",
      "        [ 1.8613e-05],\n",
      "        [ 1.0765e-04],\n",
      "        [ 7.6884e-05],\n",
      "        [ 4.0231e-05],\n",
      "        [ 1.0630e-04],\n",
      "        [-5.2891e-05],\n",
      "        [ 1.5550e-04],\n",
      "        [ 7.6874e-05],\n",
      "        [ 1.3271e-04],\n",
      "        [-5.1790e-06],\n",
      "        [ 4.8048e-05],\n",
      "        [-4.7253e-06],\n",
      "        [-2.9791e-05],\n",
      "        [ 7.6451e-05],\n",
      "        [ 6.5166e-05],\n",
      "        [-2.0249e-04],\n",
      "        [-1.1846e-04],\n",
      "        [-9.8574e-05],\n",
      "        [-1.2104e-04],\n",
      "        [-9.3624e-05],\n",
      "        [-5.3492e-05],\n",
      "        [ 7.2887e-07],\n",
      "        [ 1.1039e-04],\n",
      "        [ 2.2656e-05],\n",
      "        [ 6.7553e-06],\n",
      "        [ 3.0701e-05],\n",
      "        [-3.6602e-05],\n",
      "        [ 1.4449e-05],\n",
      "        [-7.8465e-05],\n",
      "        [ 2.1156e-05],\n",
      "        [ 5.1551e-05],\n",
      "        [ 1.5151e-05],\n",
      "        [-9.1397e-05],\n",
      "        [ 9.6664e-06],\n",
      "        [-3.9466e-05],\n",
      "        [ 9.8516e-05],\n",
      "        [ 1.6712e-05],\n",
      "        [-4.1670e-05],\n",
      "        [-6.7731e-05],\n",
      "        [ 7.7530e-05],\n",
      "        [-8.8538e-05],\n",
      "        [-4.3376e-05],\n",
      "        [ 9.2497e-05],\n",
      "        [-3.2516e-06],\n",
      "        [-7.0777e-06],\n",
      "        [-1.2162e-04],\n",
      "        [-6.9107e-05],\n",
      "        [-1.8880e-05],\n",
      "        [-4.2912e-05],\n",
      "        [ 1.2967e-04],\n",
      "        [ 6.4414e-05],\n",
      "        [ 8.6607e-05],\n",
      "        [-6.1833e-05],\n",
      "        [-1.6165e-04],\n",
      "        [ 1.6147e-04],\n",
      "        [-4.6526e-05],\n",
      "        [-7.3864e-05],\n",
      "        [ 2.1009e-04],\n",
      "        [-1.1852e-04],\n",
      "        [ 1.5757e-04],\n",
      "        [ 1.4080e-05],\n",
      "        [ 1.0384e-04],\n",
      "        [-4.7584e-05],\n",
      "        [ 2.1571e-05],\n",
      "        [-1.0367e-04],\n",
      "        [-2.5404e-05],\n",
      "        [ 1.1159e-04],\n",
      "        [ 3.4399e-06],\n",
      "        [ 1.0011e-04],\n",
      "        [-2.6022e-05],\n",
      "        [-4.2543e-05],\n",
      "        [ 7.4244e-05],\n",
      "        [ 5.7126e-05],\n",
      "        [ 5.6589e-05],\n",
      "        [-3.1220e-05]], requires_grad=True)]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PredictionLayer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-9b02c571063c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbasemodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaseModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_feature_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdnn_feature_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-184-64665d849e23>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, linear_feature_columns, dnn_feature_columns, l2_reg_linear, l2_reg_embedding, init_std, seed, task, device, gpus)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_regularization_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml2_reg_linear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredictionLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PredictionLayer' is not defined"
     ]
    }
   ],
   "source": [
    "basemodel = BaseModel(linear_feature_columns, dnn_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
