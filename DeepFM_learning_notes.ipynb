{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Motivation of why features interactions are important?\n",
    "2nd order\n",
    "3rd order\n",
    "\"\"\"\n",
    "import torch.nn as nn\n",
    "\n",
    "def create_embedding_matrix(feature_columns, init_std=0.0001, linear=False, sparse=False, device='cpu'):\n",
    "    # Return nn.ModuleDict: for sparse features, {embedding_name: nn.Embedding}\n",
    "    # for varlen sparse features, {embedding_name: nn.EmbeddingBag}\n",
    "    sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "\n",
    "    varlen_sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "\n",
    "    embedding_dict = nn.ModuleDict(\n",
    "        {feat.embedding_name: nn.Embedding(feat.vocabulary_size, feat.embedding_dim if not linear else 1, sparse=sparse)\n",
    "         for feat in\n",
    "         sparse_feature_columns + varlen_sparse_feature_columns}\n",
    "    )\n",
    "\n",
    "    # embedding weight initalization\n",
    "    for tensor in embedding_dict.values():\n",
    "        nn.init.normal_(tensor.weight, mean=0, std=init_std)\n",
    "\n",
    "    return embedding_dict.to(device)\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, feature_columns, feature_index, init_std=0.0001, device='cpu'):\n",
    "        super(Linear, self).__init__()\n",
    "        self.feature_index = feature_index\n",
    "        self.device = device\n",
    "        self.sparse_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "        self.dense_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, DenseFeat), feature_columns)) if len(feature_columns) else []\n",
    "        print(\"self.dense_feature_columns\", self.dense_feature_columns)\n",
    "        self.varlen_sparse_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "\n",
    "        self.embedding_dict = create_embedding_matrix(feature_columns, init_std, linear=True, sparse=False,\n",
    "                                                      device=device)\n",
    "\n",
    "        #         nn.ModuleDict(\n",
    "        #             {feat.embedding_name: nn.Embedding(feat.dimension, 1, sparse=True) for feat in\n",
    "        #              self.sparse_feature_columns}\n",
    "        #         )\n",
    "        # .to(\"cuda:1\")\n",
    "        for tensor in self.embedding_dict.values():\n",
    "            nn.init.normal_(tensor.weight, mean=0, std=init_std)\n",
    "\n",
    "        if len(self.dense_feature_columns) > 0:\n",
    "            self.weight = nn.Parameter(torch.Tensor(sum(fc.dimension for fc in self.dense_feature_columns), 1).to(\n",
    "                device))\n",
    "            torch.nn.init.normal_(self.weight, mean=0, std=init_std)\n",
    "\n",
    "    def forward(self, X, sparse_feat_refine_weight=None):\n",
    "\n",
    "        sparse_embedding_list = [self.embedding_dict[feat.embedding_name](\n",
    "            X[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]].long()) for\n",
    "            feat in self.sparse_feature_columns]\n",
    "        print(\"sparse_embedding_list\", len(sparse_embedding_list), sparse_embedding_list[0].shape)\n",
    "        dense_value_list = [X[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]] for feat in\n",
    "                            self.dense_feature_columns]\n",
    "\n",
    "        varlen_embedding_list = get_varlen_pooling_list(self.embedding_dict, X, self.feature_index,\n",
    "                                                        self.varlen_sparse_feature_columns, self.device)\n",
    "\n",
    "        sparse_embedding_list += varlen_embedding_list\n",
    "\n",
    "        linear_logit = torch.zeros([X.shape[0], 1]).to(sparse_embedding_list[0].device)\n",
    "        if len(sparse_embedding_list) > 0:\n",
    "            sparse_embedding_cat = torch.cat(sparse_embedding_list, dim=-1)\n",
    "            print(\"sparse_embedding_cat\", sparse_embedding_cat.shape)\n",
    "            if sparse_feat_refine_weight is not None:\n",
    "                # w_{x,i}=m_{x,i} * w_i (in IFM and DIFM)\n",
    "                sparse_embedding_cat = sparse_embedding_cat * sparse_feat_refine_weight.unsqueeze(1)\n",
    "            sparse_feat_logit = torch.sum(sparse_embedding_cat, dim=-1, keepdim=False)\n",
    "            print(\"sparse_feat_logit\", sparse_feat_logit.shape)\n",
    "            linear_logit += sparse_feat_logit\n",
    "        if len(dense_value_list) > 0:\n",
    "            dense_value_logit = torch.cat(\n",
    "                dense_value_list, dim=-1).matmul(self.weight)\n",
    "            linear_logit += dense_value_logit\n",
    "\n",
    "        return linear_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict, namedtuple, defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "DEFAULT_GROUP_NAME = \"default_group\"\n",
    "\n",
    "\n",
    "class SparseFeat(namedtuple('SparseFeat',\n",
    "                            ['name', 'vocabulary_size', 'embedding_dim', 'use_hash', 'dtype', 'embedding_name',\n",
    "                             'group_name'])):\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __new__(cls, name, vocabulary_size, embedding_dim=4, use_hash=False, dtype=\"int32\", embedding_name=None,\n",
    "                group_name=DEFAULT_GROUP_NAME):\n",
    "        if embedding_name is None:\n",
    "            embedding_name = name\n",
    "        if embedding_dim == \"auto\":\n",
    "            embedding_dim = 6 * int(pow(vocabulary_size, 0.25))\n",
    "        if use_hash:\n",
    "            print(\n",
    "                \"Notice! Feature Hashing on the fly currently is not supported in torch version,you can use tensorflow version!\")\n",
    "        return super(SparseFeat, cls).__new__(cls, name, vocabulary_size, embedding_dim, use_hash, dtype,\n",
    "                                              embedding_name, group_name)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.name.__hash__()\n",
    "    \n",
    "class VarLenSparseFeat(namedtuple('VarLenSparseFeat',\n",
    "                                  ['sparsefeat', 'maxlen', 'combiner', 'length_name'])):\n",
    "    __slots__ = ()\n",
    "    \"\"\"\n",
    "    This feature would like to support embedding bag such as average of embedding \n",
    "    \"\"\"\n",
    "    def __new__(cls, sparsefeat, maxlen, combiner=\"mean\", length_name=None):\n",
    "        return super(VarLenSparseFeat, cls).__new__(cls, sparsefeat, maxlen, combiner, length_name)\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.sparsefeat.name\n",
    "\n",
    "    @property\n",
    "    def vocabulary_size(self):\n",
    "        return self.sparsefeat.vocabulary_size\n",
    "\n",
    "    @property\n",
    "    def embedding_dim(self):\n",
    "        return self.sparsefeat.embedding_dim\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.sparsefeat.dtype\n",
    "\n",
    "    @property\n",
    "    def embedding_name(self):\n",
    "        return self.sparsefeat.embedding_name\n",
    "\n",
    "    @property\n",
    "    def group_name(self):\n",
    "        return self.sparsefeat.group_name\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.name.__hash__()\n",
    "\n",
    "\n",
    "class DenseFeat(namedtuple('DenseFeat', ['name', 'dimension', 'dtype'])):\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __new__(cls, name, dimension=1, dtype=\"float32\"):\n",
    "        return super(DenseFeat, cls).__new__(cls, name, dimension, dtype)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.name.__hash__()\n",
    "    \n",
    "def build_input_features(feature_columns):\n",
    "    # Return OrderedDict: {feature_name:(start, start+dimension)}\n",
    "\n",
    "    features = OrderedDict()\n",
    "\n",
    "    start = 0\n",
    "    for feat in feature_columns:\n",
    "        feat_name = feat.name\n",
    "        if feat_name in features:\n",
    "            continue\n",
    "        if isinstance(feat, SparseFeat):\n",
    "            features[feat_name] = (start, start + 1)\n",
    "            start += 1\n",
    "        elif isinstance(feat, DenseFeat):\n",
    "            features[feat_name] = (start, start + feat.dimension)\n",
    "            start += feat.dimension\n",
    "        elif isinstance(feat, VarLenSparseFeat):\n",
    "            features[feat_name] = (start, start + feat.maxlen)\n",
    "            start += feat.maxlen\n",
    "            if feat.length_name is not None and feat.length_name not in features:\n",
    "                features[feat.length_name] = (start, start + 1)\n",
    "                start += 1\n",
    "        else:\n",
    "            raise TypeError(\"Invalid feature column type,got\", type(feat))\n",
    "    return features\n",
    "\n",
    "def get_feature_names(feature_columns):\n",
    "    features = build_input_features(feature_columns)\n",
    "    return list(features.keys())\n",
    "\n",
    "def get_varlen_pooling_list(embedding_dict, features, feature_index, varlen_sparse_feature_columns, device):\n",
    "    varlen_sparse_embedding_list = []\n",
    "\n",
    "    for feat in varlen_sparse_feature_columns:\n",
    "        seq_emb = embedding_dict[feat.embedding_name](\n",
    "            features[:, feature_index[feat.name][0]:feature_index[feat.name][1]].long())\n",
    "        if feat.length_name is None:\n",
    "            seq_mask = features[:, feature_index[feat.name][0]:feature_index[feat.name][1]].long() != 0\n",
    "\n",
    "            emb = SequencePoolingLayer(mode=feat.combiner, supports_masking=True, device=device)(\n",
    "                [seq_emb, seq_mask])\n",
    "        else:\n",
    "            seq_length = features[:,\n",
    "                         feature_index[feat.length_name][0]:feature_index[feat.length_name][1]].long()\n",
    "            emb = SequencePoolingLayer(mode=feat.combiner, supports_masking=False, device=device)(\n",
    "                [seq_emb, seq_length])\n",
    "        varlen_sparse_embedding_list.append(emb)\n",
    "    return varlen_sparse_embedding_list\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"/Users/yunruili/DeepCTR-Torch/examples/movielens_sample.txt\")\n",
    "sparse_features = [\"movie_id\", \"user_id\",\n",
    "                   \"gender\", \"age\", \"occupation\", \"zip\"]\n",
    "target = ['rating']\n",
    "\n",
    "# 1.Label Encoding for sparse features,and do simple Transformation for dense features\n",
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feat] = lbe.fit_transform(data[feat])\n",
    "# 2.count #unique features for each sparse field\n",
    "fixlen_feature_columns = [SparseFeat(feat, data[feat].nunique())\n",
    "                          for feat in sparse_features]\n",
    "\n",
    "\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "\n",
    "train_model_input = {name: train[name] for name in feature_names}\n",
    "test_model_input = {name: test[name] for name in feature_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>169</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>961476022</td>\n",
       "      <td>Star Wars: Episode V - The Empire Strikes Back...</td>\n",
       "      <td>Action|Adventure|Drama|Sci-Fi|War</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>138</td>\n",
       "      <td>96</td>\n",
       "      <td>4</td>\n",
       "      <td>965343416</td>\n",
       "      <td>Fear and Loathing in Las Vegas (1998)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>134</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>966223349</td>\n",
       "      <td>Searching for Bobby Fischer (1993)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>78</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>980013556</td>\n",
       "      <td>Nutty Professor, The (1996)</td>\n",
       "      <td>Comedy|Fantasy|Romance|Sci-Fi</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>176</td>\n",
       "      <td>117</td>\n",
       "      <td>4</td>\n",
       "      <td>959787754</td>\n",
       "      <td>Beetlejuice (1988)</td>\n",
       "      <td>Comedy|Fantasy</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>89</td>\n",
       "      <td>161</td>\n",
       "      <td>5</td>\n",
       "      <td>973804787</td>\n",
       "      <td>Green Mile, The (1999)</td>\n",
       "      <td>Drama|Thriller</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>108</td>\n",
       "      <td>3</td>\n",
       "      <td>977788576</td>\n",
       "      <td>Parent Trap, The (1998)</td>\n",
       "      <td>Children's|Drama</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>76</td>\n",
       "      <td>49</td>\n",
       "      <td>3</td>\n",
       "      <td>974658834</td>\n",
       "      <td>20,000 Leagues Under the Sea (1954)</td>\n",
       "      <td>Adventure|Children's|Fantasy|Sci-Fi</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>80</td>\n",
       "      <td>106</td>\n",
       "      <td>3</td>\n",
       "      <td>979353437</td>\n",
       "      <td>Dangerous Liaisons (1988)</td>\n",
       "      <td>Drama|Romance</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>67</td>\n",
       "      <td>186</td>\n",
       "      <td>5</td>\n",
       "      <td>974753321</td>\n",
       "      <td>Meet the Parents (2000)</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id  movie_id  rating  timestamp  \\\n",
       "175      169        59       2  961476022   \n",
       "119      138        96       4  965343416   \n",
       "8        134        30       5  966223349   \n",
       "15        78        40       3  980013556   \n",
       "35       176       117       4  959787754   \n",
       "..       ...       ...     ...        ...   \n",
       "143       89       161       5  973804787   \n",
       "13         1       108       3  977788576   \n",
       "92        76        49       3  974658834   \n",
       "93        80       106       3  979353437   \n",
       "187       67       186       5  974753321   \n",
       "\n",
       "                                                 title  \\\n",
       "175  Star Wars: Episode V - The Empire Strikes Back...   \n",
       "119              Fear and Loathing in Las Vegas (1998)   \n",
       "8                   Searching for Bobby Fischer (1993)   \n",
       "15                         Nutty Professor, The (1996)   \n",
       "35                                  Beetlejuice (1988)   \n",
       "..                                                 ...   \n",
       "143                             Green Mile, The (1999)   \n",
       "13                             Parent Trap, The (1998)   \n",
       "92                 20,000 Leagues Under the Sea (1954)   \n",
       "93                           Dangerous Liaisons (1988)   \n",
       "187                            Meet the Parents (2000)   \n",
       "\n",
       "                                  genres  gender  age  occupation  zip  \n",
       "175    Action|Adventure|Drama|Sci-Fi|War       1    1           9  110  \n",
       "119                         Comedy|Drama       1    3          15   75  \n",
       "8                                  Drama       1    2          16   84  \n",
       "15         Comedy|Fantasy|Romance|Sci-Fi       1    1           0   94  \n",
       "35                        Comedy|Fantasy       1    4           1  145  \n",
       "..                                   ...     ...  ...         ...  ...  \n",
       "143                       Drama|Thriller       1    2          13   38  \n",
       "13                      Children's|Drama       1    6           1   86  \n",
       "92   Adventure|Children's|Fantasy|Sci-Fi       1    5          15   15  \n",
       "93                         Drama|Romance       1    2           0   77  \n",
       "187                               Comedy       1    2           7  151  \n",
       "\n",
       "[160 rows x 10 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('movie_id', (0, 1)),\n",
       "             ('user_id', (1, 2)),\n",
       "             ('gender', (2, 3)),\n",
       "             ('age', (3, 4)),\n",
       "             ('occupation', (4, 5)),\n",
       "             ('zip', (5, 6))])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_index = build_input_features(\n",
    "            linear_feature_columns + dnn_feature_columns)\n",
    "feature_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_id\n",
      "user_id\n",
      "gender\n",
      "age\n",
      "occupation\n",
      "zip\n"
     ]
    }
   ],
   "source": [
    "for feat in feature_index:\n",
    "    print(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175    1\n",
       "119    3\n",
       "8      2\n",
       "15     1\n",
       "35     4\n",
       "      ..\n",
       "143    2\n",
       "13     6\n",
       "92     5\n",
       "93     2\n",
       "187    2\n",
       "Name: age, Length: 160, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model_input[\"age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.dense_feature_columns []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n所以對linear model\\n每一個sparese feature根據你的id會找到對應的weight\\n\\n可以想成是一個y = w1*x1 +....wn*xn\\nn = 7+2+187+20+193+188, 非常大的一個數子\\n只有在對應的id xj, 是1其他都是0\\n'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "linear_model = Linear(\n",
    "            linear_feature_columns, feature_index, device=device)\n",
    "linear_model\n",
    "\"\"\"\n",
    "所以對linear model, 我們把每一個sparese feature根據你的id會找到對應的weight\n",
    "\n",
    "數學式, 可以想成是一個y = w1*x1 +....wn*xn, 而總有有多少個weight of n = 7+2+187+20+193+188,是非常大的一個數子\n",
    "其中只有在對應的id 的 xj, 是1其他都是0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(7, 1)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat = \"age\"\n",
    "embedding_dict = create_embedding_matrix(linear_feature_columns, linear = True)[feat]\n",
    "embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = feature_index[feat][0]\n",
    "end = feature_index[feat][1]\n",
    "feature_index[feat][0], feature_index[feat][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5],\n",
       "        [2],\n",
       "        [5],\n",
       "        [2],\n",
       "        [3]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = x[:, start:end].long()\n",
    "input_[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 16.,  51.,   1.,   5.,   7.,  53.],\n",
       "        [102.,  48.,   1.,   2.,  11., 147.],\n",
       "        [ 37.,  95.,   1.,   5.,  11., 177.],\n",
       "        [168., 192.,   1.,   2.,   6.,  26.],\n",
       "        [ 31., 124.,   1.,   3.,   7.,  16.]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each training example,\n",
    "#   for each spare feature, we get his corresponding embedding and concate them together (linear --> dim of embedding is 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 1])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dict(input_).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_embedding_list = [self.embedding_dict[feat.embedding_name](\n",
    "            X[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]].long()) for\n",
    "            feat in self.sparse_feature_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit method\n",
    "# -*- coding:utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "Author:\n",
    "    Weichen Shen,weichenswc@163.com\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def concat_fun(inputs, axis=-1):\n",
    "    if len(inputs) == 1:\n",
    "        return inputs[0]\n",
    "    else:\n",
    "        return torch.cat(inputs, dim=axis)\n",
    "\n",
    "\n",
    "def slice_arrays(arrays, start=None, stop=None):\n",
    "    \"\"\"Slice an array or list of arrays.\n",
    "\n",
    "    This takes an array-like, or a list of\n",
    "    array-likes, and outputs:\n",
    "        - arrays[start:stop] if `arrays` is an array-like\n",
    "        - [x[start:stop] for x in arrays] if `arrays` is a list\n",
    "\n",
    "    Can also work on list/array of indices: `slice_arrays(x, indices)`\n",
    "\n",
    "    Arguments:\n",
    "        arrays: Single array or list of arrays.\n",
    "        start: can be an integer index (start index)\n",
    "            or a list/array of indices\n",
    "        stop: integer (stop index); should be None if\n",
    "            `start` was a list.\n",
    "\n",
    "    Returns:\n",
    "        A slice of the array(s).\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the value of start is a list and stop is not None.\n",
    "    \"\"\"\n",
    "\n",
    "    if arrays is None:\n",
    "        return [None]\n",
    "\n",
    "    if isinstance(arrays, np.ndarray):\n",
    "        arrays = [arrays]\n",
    "\n",
    "    if isinstance(start, list) and stop is not None:\n",
    "        raise ValueError('The stop argument has to be None if the value of start '\n",
    "                         'is a list.')\n",
    "    elif isinstance(arrays, list):\n",
    "        if hasattr(start, '__len__'):\n",
    "            # hdf5 datasets only support list objects as indices\n",
    "            if hasattr(start, 'shape'):\n",
    "                start = start.tolist()\n",
    "            return [None if x is None else x[start] for x in arrays]\n",
    "        else:\n",
    "            if len(arrays) == 1:\n",
    "                return arrays[0][start:stop]\n",
    "            return [None if x is None else x[start:stop] for x in arrays]\n",
    "    else:\n",
    "        if hasattr(start, '__len__'):\n",
    "            if hasattr(start, 'shape'):\n",
    "                start = start.tolist()\n",
    "            return arrays[start]\n",
    "        elif hasattr(start, '__getitem__'):\n",
    "            return arrays[start:stop]\n",
    "        else:\n",
    "            return [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "split_at 128\n",
      "x 6 <class 'pandas.core.series.Series'> (128,)\n",
      "val_x 6 <class 'pandas.core.series.Series'> (32,)\n",
      "y <class 'numpy.ndarray'> (128, 1)\n",
      "val_y <class 'numpy.ndarray'> (32, 1)\n",
      "x <class 'list'> 6 <class 'numpy.ndarray'> (128, 1)\n",
      "Train on 128 samples, validate on 32 samples, 1 steps per epoch\n",
      "torch.Size([128, 6])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 6])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 6])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 6])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 6])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 6])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 6])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 6])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 6])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 6])\n",
      "torch.Size([128, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as Data\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "x = train_model_input\n",
    "y = train[target].values\n",
    "batch_size = None\n",
    "shuffle = True\n",
    "if isinstance(x, dict):\n",
    "    x = [x[feature] for feature in feature_index]\n",
    "    print(type(x[0]))\n",
    "\n",
    "validation_split = 0.2\n",
    "do_validation = True\n",
    "initial_epoch = 0\n",
    "epochs = 10\n",
    "verbose = 0\n",
    "if hasattr(x[0], 'shape'):\n",
    "    split_at = int(x[0].shape[0] * (1. - validation_split))\n",
    "else:\n",
    "    split_at = int(len(x[0]) * (1. - validation_split))\n",
    "print(\"split_at\", split_at)\n",
    "x, val_x = (slice_arrays(x, 0, split_at),\n",
    "            slice_arrays(x, split_at))\n",
    "print(\"x\", len(x), type(x[0]), x[0].shape)\n",
    "print(\"val_x\", len(val_x), type(val_x[0]), val_x[0].shape)\n",
    "\n",
    "y, val_y = (slice_arrays(y, 0, split_at),\n",
    "            slice_arrays(y, split_at))\n",
    "print(\"y\", type(y), y.shape)\n",
    "print(\"val_y\", type(val_y), val_y.shape)\n",
    "for i in range(len(x)):\n",
    "    if len(x[i].shape) == 1:\n",
    "        x[i] = np.expand_dims(x[i], axis=1)\n",
    "print(\"x\", type(x), len(x), type(x[0]), x[0].shape)\n",
    "train_tensor_data = Data.TensorDataset(\n",
    "    torch.from_numpy(\n",
    "        np.concatenate(x, axis=-1)),\n",
    "    torch.from_numpy(y))\n",
    "if batch_size is None:\n",
    "    batch_size = 256\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_tensor_data, shuffle=shuffle, batch_size=batch_size)\n",
    "sample_num = len(train_tensor_data)\n",
    "steps_per_epoch = (sample_num - 1) // batch_size + 1\n",
    "# Train\n",
    "print(\"Train on {0} samples, validate on {1} samples, {2} steps per epoch\".format(\n",
    "    len(train_tensor_data), len(val_y), steps_per_epoch))\n",
    "for epoch in range(initial_epoch, epochs):\n",
    "    #callbacks.on_epoch_begin(epoch)\n",
    "    epoch_logs = {}\n",
    "    start_time = time.time()\n",
    "    loss_epoch = 0\n",
    "    total_loss_epoch = 0\n",
    "    train_result = {}\n",
    "    try:\n",
    "        with tqdm(enumerate(train_loader), disable=verbose != 1) as t:\n",
    "            for _, (x_train, y_train) in t:\n",
    "                print(x_train.shape)\n",
    "                print(y_train.shape)\n",
    "                x = x_train.to(device).float()\n",
    "                y = y_train.to(device).float()\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        t.close()\n",
    "        raise\n",
    "    t.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 1)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_num, steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse_embedding_list 6 torch.Size([128, 1, 1])\n",
      "sparse_embedding_cat torch.Size([128, 1, 6])\n",
      "sparse_feat_logit torch.Size([128, 1])\n"
     ]
    }
   ],
   "source": [
    "linear_logit = linear_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_logit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 9])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "x_cat = torch.cat([x, x, x], -1)\n",
    "x_cat.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6068,  2.2713, -0.4924,  0.6068,  2.2713, -0.4924,  0.6068,  2.2713,\n",
       "         -0.4924],\n",
       "        [ 0.8889, -0.7034,  0.0802,  0.8889, -0.7034,  0.0802,  0.8889, -0.7034,\n",
       "          0.0802]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.1572, 0.7969])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x_cat, dim=-1, keepdim=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dense features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 500)\n",
    "data = pd.read_csv('/Users/yunruili/DeepCTR-Torch/examples/criteo_sample.txt')\n",
    "\n",
    "sparse_features = ['C' + str(i) for i in range(1, 27)]\n",
    "dense_features = ['I' + str(i) for i in range(1, 14)]\n",
    "\n",
    "data[sparse_features] = data[sparse_features].fillna('-1', )\n",
    "data[dense_features] = data[dense_features].fillna(0, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>I10</th>\n",
       "      <th>I11</th>\n",
       "      <th>I12</th>\n",
       "      <th>I13</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>C10</th>\n",
       "      <th>C11</th>\n",
       "      <th>C12</th>\n",
       "      <th>C13</th>\n",
       "      <th>C14</th>\n",
       "      <th>C15</th>\n",
       "      <th>C16</th>\n",
       "      <th>C17</th>\n",
       "      <th>C18</th>\n",
       "      <th>C19</th>\n",
       "      <th>C20</th>\n",
       "      <th>C21</th>\n",
       "      <th>C22</th>\n",
       "      <th>C23</th>\n",
       "      <th>C24</th>\n",
       "      <th>C25</th>\n",
       "      <th>C26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>260.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17668.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>05db9164</td>\n",
       "      <td>08d6d899</td>\n",
       "      <td>9143c832</td>\n",
       "      <td>f56b7dd5</td>\n",
       "      <td>25c83c98</td>\n",
       "      <td>7e0ccccf</td>\n",
       "      <td>df5c2d18</td>\n",
       "      <td>0b153874</td>\n",
       "      <td>a73ee510</td>\n",
       "      <td>8f48ce11</td>\n",
       "      <td>a7b606c4</td>\n",
       "      <td>ae1bb660</td>\n",
       "      <td>eae197fd</td>\n",
       "      <td>b28479f6</td>\n",
       "      <td>bfef54b3</td>\n",
       "      <td>bad5ee18</td>\n",
       "      <td>e5ba7672</td>\n",
       "      <td>87c6f83c</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0429f84b</td>\n",
       "      <td>-1</td>\n",
       "      <td>3a171ecb</td>\n",
       "      <td>c0d61a5c</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>30251.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>68fd1e64</td>\n",
       "      <td>04e09220</td>\n",
       "      <td>95e13fd4</td>\n",
       "      <td>a1e6a194</td>\n",
       "      <td>25c83c98</td>\n",
       "      <td>fe6b92e5</td>\n",
       "      <td>f819e175</td>\n",
       "      <td>062b5529</td>\n",
       "      <td>a73ee510</td>\n",
       "      <td>ab9456b4</td>\n",
       "      <td>6153cf57</td>\n",
       "      <td>8882c6cd</td>\n",
       "      <td>769a1844</td>\n",
       "      <td>b28479f6</td>\n",
       "      <td>69f825dd</td>\n",
       "      <td>23056e4f</td>\n",
       "      <td>d4bb7bd8</td>\n",
       "      <td>6fc84bfb</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>5155d8a3</td>\n",
       "      <td>-1</td>\n",
       "      <td>be7c41b4</td>\n",
       "      <td>ded4aac9</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>05db9164</td>\n",
       "      <td>38a947a1</td>\n",
       "      <td>3f55fb72</td>\n",
       "      <td>5de245c7</td>\n",
       "      <td>30903e74</td>\n",
       "      <td>7e0ccccf</td>\n",
       "      <td>b72ec13d</td>\n",
       "      <td>1f89b562</td>\n",
       "      <td>a73ee510</td>\n",
       "      <td>acce978c</td>\n",
       "      <td>3547565f</td>\n",
       "      <td>a5b0521a</td>\n",
       "      <td>12880350</td>\n",
       "      <td>b28479f6</td>\n",
       "      <td>c12fc269</td>\n",
       "      <td>95a8919c</td>\n",
       "      <td>e5ba7672</td>\n",
       "      <td>675c9258</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2e01979f</td>\n",
       "      <td>-1</td>\n",
       "      <td>bcdee96c</td>\n",
       "      <td>6d5d1302</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16836.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>05db9164</td>\n",
       "      <td>8084ee93</td>\n",
       "      <td>02cf9876</td>\n",
       "      <td>c18be181</td>\n",
       "      <td>25c83c98</td>\n",
       "      <td>-1</td>\n",
       "      <td>e14874c9</td>\n",
       "      <td>0b153874</td>\n",
       "      <td>7cc72ec2</td>\n",
       "      <td>2462946f</td>\n",
       "      <td>636405ac</td>\n",
       "      <td>8fe001f4</td>\n",
       "      <td>31b42deb</td>\n",
       "      <td>07d13a8f</td>\n",
       "      <td>422c8577</td>\n",
       "      <td>36103458</td>\n",
       "      <td>e5ba7672</td>\n",
       "      <td>52e44668</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>e587c466</td>\n",
       "      <td>-1</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>3b183c5c</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>05db9164</td>\n",
       "      <td>207b2d81</td>\n",
       "      <td>5d076085</td>\n",
       "      <td>862b5ba0</td>\n",
       "      <td>25c83c98</td>\n",
       "      <td>fbad5c96</td>\n",
       "      <td>17c22666</td>\n",
       "      <td>0b153874</td>\n",
       "      <td>a73ee510</td>\n",
       "      <td>534fc986</td>\n",
       "      <td>feb49a68</td>\n",
       "      <td>f24b551c</td>\n",
       "      <td>8978af5c</td>\n",
       "      <td>64c94865</td>\n",
       "      <td>32ec6582</td>\n",
       "      <td>b6d021e8</td>\n",
       "      <td>e5ba7672</td>\n",
       "      <td>25c88e42</td>\n",
       "      <td>21ddcdc9</td>\n",
       "      <td>b1252a9d</td>\n",
       "      <td>0e8585d2</td>\n",
       "      <td>-1</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>0d4a6d1a</td>\n",
       "      <td>001f3601</td>\n",
       "      <td>92c878de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3036.0</td>\n",
       "      <td>575.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>05db9164</td>\n",
       "      <td>0468d672</td>\n",
       "      <td>628b07b0</td>\n",
       "      <td>b63c0277</td>\n",
       "      <td>25c83c98</td>\n",
       "      <td>7e0ccccf</td>\n",
       "      <td>0d339a25</td>\n",
       "      <td>c8ddd494</td>\n",
       "      <td>a73ee510</td>\n",
       "      <td>1722d4c8</td>\n",
       "      <td>7d756b25</td>\n",
       "      <td>0c87b3e9</td>\n",
       "      <td>6f833c7a</td>\n",
       "      <td>1adce6ef</td>\n",
       "      <td>4f3b3616</td>\n",
       "      <td>48af915a</td>\n",
       "      <td>07c540c4</td>\n",
       "      <td>9880032b</td>\n",
       "      <td>21ddcdc9</td>\n",
       "      <td>5840adea</td>\n",
       "      <td>34cc61bb</td>\n",
       "      <td>c9d4222a</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>e5ed7da2</td>\n",
       "      <td>ea9a246c</td>\n",
       "      <td>984e0db0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1607.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>be589b51</td>\n",
       "      <td>aa8fcc21</td>\n",
       "      <td>4255f8fd</td>\n",
       "      <td>7501d94a</td>\n",
       "      <td>25c83c98</td>\n",
       "      <td>fe6b92e5</td>\n",
       "      <td>0492c809</td>\n",
       "      <td>1f89b562</td>\n",
       "      <td>a73ee510</td>\n",
       "      <td>13ba96b0</td>\n",
       "      <td>ba0f9e8a</td>\n",
       "      <td>887a0c20</td>\n",
       "      <td>4e4dd817</td>\n",
       "      <td>07d13a8f</td>\n",
       "      <td>a4f91020</td>\n",
       "      <td>022714ba</td>\n",
       "      <td>1e88c74f</td>\n",
       "      <td>3972b4ed</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>d1aa4512</td>\n",
       "      <td>-1</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>9257f75f</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>05db9164</td>\n",
       "      <td>09e68b86</td>\n",
       "      <td>db151f8b</td>\n",
       "      <td>f1b645fc</td>\n",
       "      <td>25c83c98</td>\n",
       "      <td>-1</td>\n",
       "      <td>b87f4a4a</td>\n",
       "      <td>0b153874</td>\n",
       "      <td>a73ee510</td>\n",
       "      <td>e70742b0</td>\n",
       "      <td>319687c9</td>\n",
       "      <td>af6ad6b6</td>\n",
       "      <td>62036f49</td>\n",
       "      <td>f862f261</td>\n",
       "      <td>1dca7862</td>\n",
       "      <td>05a97a3c</td>\n",
       "      <td>3486227d</td>\n",
       "      <td>5aed7436</td>\n",
       "      <td>54591762</td>\n",
       "      <td>a458ea53</td>\n",
       "      <td>4a2c3526</td>\n",
       "      <td>-1</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>1793a828</td>\n",
       "      <td>e8b83407</td>\n",
       "      <td>1a02cbe1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "      <td>6.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>508.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>05db9164</td>\n",
       "      <td>e5fb1af3</td>\n",
       "      <td>7e1ad1fe</td>\n",
       "      <td>46ec0a38</td>\n",
       "      <td>43b19349</td>\n",
       "      <td>7e0ccccf</td>\n",
       "      <td>24c48926</td>\n",
       "      <td>0b153874</td>\n",
       "      <td>a73ee510</td>\n",
       "      <td>afa26c81</td>\n",
       "      <td>9f0003f4</td>\n",
       "      <td>651d80c6</td>\n",
       "      <td>5afd9e51</td>\n",
       "      <td>07d13a8f</td>\n",
       "      <td>b5de5956</td>\n",
       "      <td>72401022</td>\n",
       "      <td>3486227d</td>\n",
       "      <td>13145934</td>\n",
       "      <td>55dd3565</td>\n",
       "      <td>5840adea</td>\n",
       "      <td>bf647035</td>\n",
       "      <td>-1</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>1481ceb4</td>\n",
       "      <td>e8b83407</td>\n",
       "      <td>988b0775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>be589b51</td>\n",
       "      <td>b46aceb6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>43b19349</td>\n",
       "      <td>-1</td>\n",
       "      <td>17cdc396</td>\n",
       "      <td>0b153874</td>\n",
       "      <td>a73ee510</td>\n",
       "      <td>75d852fc</td>\n",
       "      <td>d79cc967</td>\n",
       "      <td>-1</td>\n",
       "      <td>115d29f4</td>\n",
       "      <td>07d13a8f</td>\n",
       "      <td>217d99f2</td>\n",
       "      <td>-1</td>\n",
       "      <td>d4bb7bd8</td>\n",
       "      <td>908eaeb8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label   I1  I2     I3    I4       I5     I6    I7    I8     I9  I10  \\\n",
       "0        0  0.0   3  260.0   0.0  17668.0    0.0   0.0  33.0    0.0  0.0   \n",
       "1        0  0.0  -1   19.0  35.0  30251.0  247.0   1.0  35.0  160.0  0.0   \n",
       "2        0  0.0   0    2.0  12.0   2013.0  164.0   6.0  35.0  523.0  0.0   \n",
       "3        0  0.0  13    1.0   4.0  16836.0  200.0   5.0   4.0   29.0  0.0   \n",
       "4        0  0.0   0  104.0  27.0   1990.0  142.0   4.0  32.0   37.0  0.0   \n",
       "..     ...  ...  ..    ...   ...      ...    ...   ...   ...    ...  ...   \n",
       "195      0  0.0   0  113.0   3.0   3036.0  575.0   2.0   3.0  214.0  0.0   \n",
       "196      1  0.0   1    1.0   1.0   1607.0   12.0   1.0  12.0   15.0  0.0   \n",
       "197      1  1.0   0    6.0   3.0      0.0    0.0  19.0   3.0    3.0  1.0   \n",
       "198      0  0.0  22    6.0  22.0    203.0  153.0  80.0  18.0  508.0  0.0   \n",
       "199      0  1.0  -1    0.0   0.0    138.0    0.0   1.0   0.0    0.0  1.0   \n",
       "\n",
       "      I11  I12   I13        C1        C2        C3        C4        C5  \\\n",
       "0     0.0  0.0   0.0  05db9164  08d6d899  9143c832  f56b7dd5  25c83c98   \n",
       "1     1.0  0.0  35.0  68fd1e64  04e09220  95e13fd4  a1e6a194  25c83c98   \n",
       "2     3.0  0.0  18.0  05db9164  38a947a1  3f55fb72  5de245c7  30903e74   \n",
       "3     2.0  0.0   4.0  05db9164  8084ee93  02cf9876  c18be181  25c83c98   \n",
       "4     1.0  0.0  27.0  05db9164  207b2d81  5d076085  862b5ba0  25c83c98   \n",
       "..    ...  ...   ...       ...       ...       ...       ...       ...   \n",
       "195   1.0  0.0   3.0  05db9164  0468d672  628b07b0  b63c0277  25c83c98   \n",
       "196   1.0  0.0  12.0  be589b51  aa8fcc21  4255f8fd  7501d94a  25c83c98   \n",
       "197   9.0  0.0   0.0  05db9164  09e68b86  db151f8b  f1b645fc  25c83c98   \n",
       "198  11.0  0.0  22.0  05db9164  e5fb1af3  7e1ad1fe  46ec0a38  43b19349   \n",
       "199   1.0  0.0   0.0  be589b51  b46aceb6        -1        -1  43b19349   \n",
       "\n",
       "           C6        C7        C8        C9       C10       C11       C12  \\\n",
       "0    7e0ccccf  df5c2d18  0b153874  a73ee510  8f48ce11  a7b606c4  ae1bb660   \n",
       "1    fe6b92e5  f819e175  062b5529  a73ee510  ab9456b4  6153cf57  8882c6cd   \n",
       "2    7e0ccccf  b72ec13d  1f89b562  a73ee510  acce978c  3547565f  a5b0521a   \n",
       "3          -1  e14874c9  0b153874  7cc72ec2  2462946f  636405ac  8fe001f4   \n",
       "4    fbad5c96  17c22666  0b153874  a73ee510  534fc986  feb49a68  f24b551c   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "195  7e0ccccf  0d339a25  c8ddd494  a73ee510  1722d4c8  7d756b25  0c87b3e9   \n",
       "196  fe6b92e5  0492c809  1f89b562  a73ee510  13ba96b0  ba0f9e8a  887a0c20   \n",
       "197        -1  b87f4a4a  0b153874  a73ee510  e70742b0  319687c9  af6ad6b6   \n",
       "198  7e0ccccf  24c48926  0b153874  a73ee510  afa26c81  9f0003f4  651d80c6   \n",
       "199        -1  17cdc396  0b153874  a73ee510  75d852fc  d79cc967        -1   \n",
       "\n",
       "          C13       C14       C15       C16       C17       C18       C19  \\\n",
       "0    eae197fd  b28479f6  bfef54b3  bad5ee18  e5ba7672  87c6f83c        -1   \n",
       "1    769a1844  b28479f6  69f825dd  23056e4f  d4bb7bd8  6fc84bfb        -1   \n",
       "2    12880350  b28479f6  c12fc269  95a8919c  e5ba7672  675c9258        -1   \n",
       "3    31b42deb  07d13a8f  422c8577  36103458  e5ba7672  52e44668        -1   \n",
       "4    8978af5c  64c94865  32ec6582  b6d021e8  e5ba7672  25c88e42  21ddcdc9   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "195  6f833c7a  1adce6ef  4f3b3616  48af915a  07c540c4  9880032b  21ddcdc9   \n",
       "196  4e4dd817  07d13a8f  a4f91020  022714ba  1e88c74f  3972b4ed        -1   \n",
       "197  62036f49  f862f261  1dca7862  05a97a3c  3486227d  5aed7436  54591762   \n",
       "198  5afd9e51  07d13a8f  b5de5956  72401022  3486227d  13145934  55dd3565   \n",
       "199  115d29f4  07d13a8f  217d99f2        -1  d4bb7bd8  908eaeb8        -1   \n",
       "\n",
       "          C20       C21       C22       C23       C24       C25       C26  \n",
       "0          -1  0429f84b        -1  3a171ecb  c0d61a5c        -1        -1  \n",
       "1          -1  5155d8a3        -1  be7c41b4  ded4aac9        -1        -1  \n",
       "2          -1  2e01979f        -1  bcdee96c  6d5d1302        -1        -1  \n",
       "3          -1  e587c466        -1  32c7478e  3b183c5c        -1        -1  \n",
       "4    b1252a9d  0e8585d2        -1  32c7478e  0d4a6d1a  001f3601  92c878de  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "195  5840adea  34cc61bb  c9d4222a  32c7478e  e5ed7da2  ea9a246c  984e0db0  \n",
       "196        -1  d1aa4512        -1  32c7478e  9257f75f        -1        -1  \n",
       "197  a458ea53  4a2c3526        -1  32c7478e  1793a828  e8b83407  1a02cbe1  \n",
       "198  5840adea  bf647035        -1  32c7478e  1481ceb4  e8b83407  988b0775  \n",
       "199        -1        -1        -1  32c7478e        -1        -1        -1  \n",
       "\n",
       "[200 rows x 40 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import mean_squared_error\n",
    "try:\n",
    "    from tensorflow.python.keras.callbacks import CallbackList\n",
    "except ImportError:\n",
    "    from tensorflow.python.keras._impl.keras.callbacks import CallbackList\n",
    "\n",
    "class PredictionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "      Arguments\n",
    "         - **task**: str, ``\"binary\"`` for  binary logloss or  ``\"regression\"`` for regression loss\n",
    "         - **use_bias**: bool.Whether add bias term or not.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, task='binary', use_bias=True, **kwargs):\n",
    "        if task not in [\"binary\", \"multiclass\", \"regression\"]:\n",
    "            raise ValueError(\"task must be binary,multiclass or regression\")\n",
    "\n",
    "        super(PredictionLayer, self).__init__()\n",
    "        self.use_bias = use_bias\n",
    "        self.task = task\n",
    "        if self.use_bias:\n",
    "            self.bias = nn.Parameter(torch.zeros((1,)))\n",
    "\n",
    "    def forward(self, X):\n",
    "        output = X\n",
    "        if self.use_bias:\n",
    "            output += self.bias\n",
    "        if self.task == \"binary\":\n",
    "            output = torch.sigmoid(output)\n",
    "        return output\n",
    "    \n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, linear_feature_columns, dnn_feature_columns, l2_reg_linear=1e-5, l2_reg_embedding=1e-5,\n",
    "                 init_std=0.0001, seed=1024, task='binary', device='cpu', gpus=None):\n",
    "\n",
    "        super(BaseModel, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.dnn_feature_columns = dnn_feature_columns\n",
    "\n",
    "        self.reg_loss = torch.zeros((1,), device=device)\n",
    "        self.aux_loss = torch.zeros((1,), device=device)\n",
    "        self.device = device\n",
    "        self.gpus = gpus\n",
    "        if gpus and str(self.gpus[0]) not in self.device:\n",
    "            raise ValueError(\n",
    "                \"`gpus[0]` should be the same gpu with `device`\")\n",
    "\n",
    "        self.feature_index = build_input_features(\n",
    "            linear_feature_columns + dnn_feature_columns)\n",
    "        self.dnn_feature_columns = dnn_feature_columns\n",
    "\n",
    "        self.embedding_dict = create_embedding_matrix(dnn_feature_columns, init_std, sparse=False, device=device)\n",
    "        #         nn.ModuleDict(\n",
    "        #             {feat.embedding_name: nn.Embedding(feat.dimension, embedding_size, sparse=True) for feat in\n",
    "        #              self.dnn_feature_columns}\n",
    "        #         )\n",
    "\n",
    "        self.linear_model = Linear(\n",
    "            linear_feature_columns, self.feature_index, device=device)\n",
    "\n",
    "        self.regularization_weight = []\n",
    "\n",
    "        self.add_regularization_weight(self.embedding_dict.parameters(), l2=l2_reg_embedding)\n",
    "        self.add_regularization_weight(self.linear_model.parameters(), l2=l2_reg_linear)\n",
    "\n",
    "        self.out = PredictionLayer(task, )\n",
    "        self.to(device)\n",
    "\n",
    "        # parameters of callbacks\n",
    "        self._is_graph_network = True  # used for ModelCheckpoint\n",
    "        self.stop_training = False  # used for EarlyStopping\n",
    "        self.history = History()\n",
    "\n",
    "    def fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, initial_epoch=0, validation_split=0.,\n",
    "            validation_data=None, shuffle=True, callbacks=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: Numpy array of training data (if the model has a single input), or list of Numpy arrays (if the model has multiple inputs).If input layers in the model are named, you can also pass a\n",
    "            dictionary mapping input names to Numpy arrays.\n",
    "        :param y: Numpy array of target (label) data (if the model has a single output), or list of Numpy arrays (if the model has multiple outputs).\n",
    "        :param batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 256.\n",
    "        :param epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided. Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached.\n",
    "        :param verbose: Integer. 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "        :param initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run).\n",
    "        :param validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling.\n",
    "        :param validation_data: tuple `(x_val, y_val)` or tuple `(x_val, y_val, val_sample_weights)` on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. `validation_data` will override `validation_split`.\n",
    "        :param shuffle: Boolean. Whether to shuffle the order of the batches at the beginning of each epoch.\n",
    "        :param callbacks: List of `deepctr_torch.callbacks.Callback` instances. List of callbacks to apply during training and validation (if ). See [callbacks](https://tensorflow.google.cn/api_docs/python/tf/keras/callbacks). Now available: `EarlyStopping` , `ModelCheckpoint`\n",
    "\n",
    "        :return: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).\n",
    "        \"\"\"\n",
    "        if isinstance(x, dict):\n",
    "            x = [x[feature] for feature in self.feature_index]\n",
    "\n",
    "        do_validation = False\n",
    "        if validation_data:\n",
    "            do_validation = True\n",
    "            if len(validation_data) == 2:\n",
    "                val_x, val_y = validation_data\n",
    "                val_sample_weight = None\n",
    "            elif len(validation_data) == 3:\n",
    "                val_x, val_y, val_sample_weight = validation_data  # pylint: disable=unpacking-non-sequence\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    'When passing a `validation_data` argument, '\n",
    "                    'it must contain either 2 items (x_val, y_val), '\n",
    "                    'or 3 items (x_val, y_val, val_sample_weights), '\n",
    "                    'or alternatively it could be a dataset or a '\n",
    "                    'dataset or a dataset iterator. '\n",
    "                    'However we received `validation_data=%s`' % validation_data)\n",
    "            if isinstance(val_x, dict):\n",
    "                val_x = [val_x[feature] for feature in self.feature_index]\n",
    "\n",
    "        elif validation_split and 0. < validation_split < 1.:\n",
    "            do_validation = True\n",
    "            if hasattr(x[0], 'shape'):\n",
    "                split_at = int(x[0].shape[0] * (1. - validation_split))\n",
    "            else:\n",
    "                split_at = int(len(x[0]) * (1. - validation_split))\n",
    "            x, val_x = (slice_arrays(x, 0, split_at),\n",
    "                        slice_arrays(x, split_at))\n",
    "            y, val_y = (slice_arrays(y, 0, split_at),\n",
    "                        slice_arrays(y, split_at))\n",
    "\n",
    "        else:\n",
    "            val_x = []\n",
    "            val_y = []\n",
    "        for i in range(len(x)):\n",
    "            if len(x[i].shape) == 1:\n",
    "                x[i] = np.expand_dims(x[i], axis=1)\n",
    "\n",
    "        train_tensor_data = Data.TensorDataset(\n",
    "            torch.from_numpy(\n",
    "                np.concatenate(x, axis=-1)),\n",
    "            torch.from_numpy(y))\n",
    "        if batch_size is None:\n",
    "            batch_size = 256\n",
    "\n",
    "        model = self.train()\n",
    "        loss_func = self.loss_func\n",
    "        optim = self.optim\n",
    "\n",
    "        if self.gpus:\n",
    "            print('parallel running on these gpus:', self.gpus)\n",
    "            model = torch.nn.DataParallel(model, device_ids=self.gpus)\n",
    "            batch_size *= len(self.gpus)  # input `batch_size` is batch_size per gpu\n",
    "        else:\n",
    "            print(self.device)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            dataset=train_tensor_data, shuffle=shuffle, batch_size=batch_size)\n",
    "\n",
    "        sample_num = len(train_tensor_data)\n",
    "        steps_per_epoch = (sample_num - 1) // batch_size + 1\n",
    "\n",
    "        # configure callbacks\n",
    "        callbacks = (callbacks or []) + [self.history]  # add history callback\n",
    "        callbacks = CallbackList(callbacks)\n",
    "        callbacks.on_train_begin()\n",
    "        callbacks.set_model(self)\n",
    "        if not hasattr(callbacks, 'model'):\n",
    "            callbacks.__setattr__('model', self)\n",
    "        callbacks.model.stop_training = False\n",
    "\n",
    "        # Train\n",
    "        print(\"Train on {0} samples, validate on {1} samples, {2} steps per epoch\".format(\n",
    "            len(train_tensor_data), len(val_y), steps_per_epoch))\n",
    "        for epoch in range(initial_epoch, epochs):\n",
    "            callbacks.on_epoch_begin(epoch)\n",
    "            epoch_logs = {}\n",
    "            start_time = time.time()\n",
    "            loss_epoch = 0\n",
    "            total_loss_epoch = 0\n",
    "            train_result = {}\n",
    "            try:\n",
    "                with tqdm(enumerate(train_loader), disable=verbose != 1) as t:\n",
    "                    for _, (x_train, y_train) in t:\n",
    "                        x = x_train.to(self.device).float()\n",
    "                        y = y_train.to(self.device).float()\n",
    "\n",
    "                        y_pred = model(x).squeeze()\n",
    "\n",
    "                        optim.zero_grad()\n",
    "                        loss = loss_func(y_pred, y.squeeze(), reduction='sum')\n",
    "                        reg_loss = self.get_regularization_loss()\n",
    "\n",
    "                        total_loss = loss + reg_loss + self.aux_loss\n",
    "\n",
    "                        loss_epoch += loss.item()\n",
    "                        total_loss_epoch += total_loss.item()\n",
    "                        total_loss.backward()\n",
    "                        optim.step()\n",
    "\n",
    "                        if verbose > 0:\n",
    "                            for name, metric_fun in self.metrics.items():\n",
    "                                if name not in train_result:\n",
    "                                    train_result[name] = []\n",
    "                                train_result[name].append(metric_fun(\n",
    "                                    y.cpu().data.numpy(), y_pred.cpu().data.numpy().astype(\"float64\")))\n",
    "\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                t.close()\n",
    "                raise\n",
    "            t.close()\n",
    "\n",
    "            # Add epoch_logs\n",
    "            epoch_logs[\"loss\"] = total_loss_epoch / sample_num\n",
    "            for name, result in train_result.items():\n",
    "                epoch_logs[name] = np.sum(result) / steps_per_epoch\n",
    "\n",
    "            if do_validation:\n",
    "                eval_result = self.evaluate(val_x, val_y, batch_size)\n",
    "                for name, result in eval_result.items():\n",
    "                    epoch_logs[\"val_\" + name] = result\n",
    "            # verbose\n",
    "            if verbose > 0:\n",
    "                epoch_time = int(time.time() - start_time)\n",
    "                print('Epoch {0}/{1}'.format(epoch + 1, epochs))\n",
    "\n",
    "                eval_str = \"{0}s - loss: {1: .4f}\".format(\n",
    "                    epoch_time, epoch_logs[\"loss\"])\n",
    "\n",
    "                for name in self.metrics:\n",
    "                    eval_str += \" - \" + name + \\\n",
    "                                \": {0: .4f}\".format(epoch_logs[name])\n",
    "\n",
    "                if do_validation:\n",
    "                    for name in self.metrics:\n",
    "                        eval_str += \" - \" + \"val_\" + name + \\\n",
    "                                    \": {0: .4f}\".format(epoch_logs[\"val_\" + name])\n",
    "                print(eval_str)\n",
    "            callbacks.on_epoch_end(epoch, epoch_logs)\n",
    "            if self.stop_training:\n",
    "                break\n",
    "\n",
    "        callbacks.on_train_end()\n",
    "\n",
    "        return self.history\n",
    "\n",
    "    def evaluate(self, x, y, batch_size=256):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: Numpy array of test data (if the model has a single input), or list of Numpy arrays (if the model has multiple inputs).\n",
    "        :param y: Numpy array of target (label) data (if the model has a single output), or list of Numpy arrays (if the model has multiple outputs).\n",
    "        :param batch_size: Integer or `None`. Number of samples per evaluation step. If unspecified, `batch_size` will default to 256.\n",
    "        :return: Dict contains metric names and metric values.\n",
    "        \"\"\"\n",
    "        pred_ans = self.predict(x, batch_size)\n",
    "        eval_result = {}\n",
    "        for name, metric_fun in self.metrics.items():\n",
    "            eval_result[name] = metric_fun(y, pred_ans)\n",
    "        return eval_result\n",
    "\n",
    "    def predict(self, x, batch_size=256):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: The input data, as a Numpy array (or list of Numpy arrays if the model has multiple inputs).\n",
    "        :param batch_size: Integer. If unspecified, it will default to 256.\n",
    "        :return: Numpy array(s) of predictions.\n",
    "        \"\"\"\n",
    "        model = self.eval()\n",
    "        if isinstance(x, dict):\n",
    "            x = [x[feature] for feature in self.feature_index]\n",
    "        for i in range(len(x)):\n",
    "            if len(x[i].shape) == 1:\n",
    "                x[i] = np.expand_dims(x[i], axis=1)\n",
    "\n",
    "        tensor_data = Data.TensorDataset(\n",
    "            torch.from_numpy(np.concatenate(x, axis=-1)))\n",
    "        test_loader = DataLoader(\n",
    "            dataset=tensor_data, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "        pred_ans = []\n",
    "        with torch.no_grad():\n",
    "            for _, x_test in enumerate(test_loader):\n",
    "                x = x_test[0].to(self.device).float()\n",
    "\n",
    "                y_pred = model(x).cpu().data.numpy()  # .squeeze()\n",
    "                pred_ans.append(y_pred)\n",
    "\n",
    "        return np.concatenate(pred_ans).astype(\"float64\")\n",
    "\n",
    "    def input_from_feature_columns(self, X, feature_columns, embedding_dict, support_dense=True):\n",
    "\n",
    "        sparse_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "        dense_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, DenseFeat), feature_columns)) if len(feature_columns) else []\n",
    "\n",
    "        varlen_sparse_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if feature_columns else []\n",
    "\n",
    "        if not support_dense and len(dense_feature_columns) > 0:\n",
    "            raise ValueError(\n",
    "                \"DenseFeat is not supported in dnn_feature_columns\")\n",
    "\n",
    "        sparse_embedding_list = [embedding_dict[feat.embedding_name](\n",
    "            X[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]].long()) for\n",
    "            feat in sparse_feature_columns]\n",
    "\n",
    "        varlen_sparse_embedding_list = get_varlen_pooling_list(self.embedding_dict, X, self.feature_index,\n",
    "                                                               varlen_sparse_feature_columns, self.device)\n",
    "\n",
    "        dense_value_list = [X[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]] for feat in\n",
    "                            dense_feature_columns]\n",
    "\n",
    "        return sparse_embedding_list + varlen_sparse_embedding_list, dense_value_list\n",
    "\n",
    "    def compute_input_dim(self, feature_columns, include_sparse=True, include_dense=True, feature_group=False):\n",
    "        sparse_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, (SparseFeat, VarLenSparseFeat)), feature_columns)) if len(\n",
    "            feature_columns) else []\n",
    "        dense_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, DenseFeat), feature_columns)) if len(feature_columns) else []\n",
    "\n",
    "        dense_input_dim = sum(\n",
    "            map(lambda x: x.dimension, dense_feature_columns))\n",
    "        if feature_group:\n",
    "            sparse_input_dim = len(sparse_feature_columns)\n",
    "        else:\n",
    "            sparse_input_dim = sum(feat.embedding_dim for feat in sparse_feature_columns)\n",
    "        input_dim = 0\n",
    "        if include_sparse:\n",
    "            input_dim += sparse_input_dim\n",
    "        if include_dense:\n",
    "            input_dim += dense_input_dim\n",
    "        return input_dim\n",
    "\n",
    "    def add_regularization_weight(self, weight_list, l1=0.0, l2=0.0):\n",
    "        # For a Parameter, put it in a list to keep Compatible with get_regularization_loss()\n",
    "        if isinstance(weight_list, torch.nn.parameter.Parameter):\n",
    "            weight_list = [weight_list]\n",
    "        # For generators, filters and ParameterLists, convert them to a list of tensors to avoid bugs.\n",
    "        # e.g., we can't pickle generator objects when we save the model.\n",
    "        else:\n",
    "            weight_list = list(weight_list)\n",
    "        print(\"weight_list\", len(weight_list))\n",
    "        for w in weight_list:\n",
    "            print(\"weight\", w.shape)\n",
    "        self.regularization_weight.append((weight_list, l1, l2))\n",
    "\n",
    "    def get_regularization_loss(self, ):\n",
    "        total_reg_loss = torch.zeros((1,), device=self.device)\n",
    "        for weight_list, l1, l2 in self.regularization_weight:\n",
    "            for w in weight_list:\n",
    "                if isinstance(w, tuple):\n",
    "                    parameter = w[1]  # named_parameters\n",
    "                else:\n",
    "                    parameter = w\n",
    "                print(\"parameter\", parameter.shape)\n",
    "                if l1 > 0:\n",
    "                    total_reg_loss += torch.sum(l1 * torch.abs(parameter))\n",
    "                if l2 > 0:\n",
    "                    try:\n",
    "                        total_reg_loss += torch.sum(l2 * torch.square(parameter))\n",
    "                    except AttributeError:\n",
    "                        total_reg_loss += torch.sum(l2 * parameter * parameter)\n",
    "\n",
    "        return total_reg_loss\n",
    "\n",
    "    def add_auxiliary_loss(self, aux_loss, alpha):\n",
    "        self.aux_loss = aux_loss * alpha\n",
    "\n",
    "    def compile(self, optimizer,\n",
    "                loss=None,\n",
    "                metrics=None,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        :param optimizer: String (name of optimizer) or optimizer instance. See [optimizers](https://pytorch.org/docs/stable/optim.html).\n",
    "        :param loss: String (name of objective function) or objective function. See [losses](https://pytorch.org/docs/stable/nn.functional.html#loss-functions).\n",
    "        :param metrics: List of metrics to be evaluated by the model during training and testing. Typically you will use `metrics=['accuracy']`.\n",
    "        \"\"\"\n",
    "        self.metrics_names = [\"loss\"]\n",
    "        self.optim = self._get_optim(optimizer)\n",
    "        self.loss_func = self._get_loss_func(loss)\n",
    "        self.metrics = self._get_metrics(metrics)\n",
    "\n",
    "    def _get_optim(self, optimizer):\n",
    "        if isinstance(optimizer, str):\n",
    "            if optimizer == \"sgd\":\n",
    "                optim = torch.optim.SGD(self.parameters(), lr=0.01)\n",
    "            elif optimizer == \"adam\":\n",
    "                optim = torch.optim.Adam(self.parameters())  # 0.001\n",
    "            elif optimizer == \"adagrad\":\n",
    "                optim = torch.optim.Adagrad(self.parameters())  # 0.01\n",
    "            elif optimizer == \"rmsprop\":\n",
    "                optim = torch.optim.RMSprop(self.parameters())\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        else:\n",
    "            optim = optimizer\n",
    "        return optim\n",
    "\n",
    "    def _get_loss_func(self, loss):\n",
    "        if isinstance(loss, str):\n",
    "            if loss == \"binary_crossentropy\":\n",
    "                loss_func = F.binary_cross_entropy\n",
    "            elif loss == \"mse\":\n",
    "                loss_func = F.mse_loss\n",
    "            elif loss == \"mae\":\n",
    "                loss_func = F.l1_loss\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        else:\n",
    "            loss_func = loss\n",
    "        return loss_func\n",
    "\n",
    "    def _log_loss(self, y_true, y_pred, eps=1e-7, normalize=True, sample_weight=None, labels=None):\n",
    "        # change eps to improve calculation accuracy\n",
    "        return log_loss(y_true,\n",
    "                        y_pred,\n",
    "                        eps,\n",
    "                        normalize,\n",
    "                        sample_weight,\n",
    "                        labels)\n",
    "\n",
    "    def _get_metrics(self, metrics, set_eps=False):\n",
    "        metrics_ = {}\n",
    "        if metrics:\n",
    "            for metric in metrics:\n",
    "                if metric == \"binary_crossentropy\" or metric == \"logloss\":\n",
    "                    if set_eps:\n",
    "                        metrics_[metric] = self._log_loss\n",
    "                    else:\n",
    "                        metrics_[metric] = log_loss\n",
    "                if metric == \"auc\":\n",
    "                    metrics_[metric] = roc_auc_score\n",
    "                if metric == \"mse\":\n",
    "                    metrics_[metric] = mean_squared_error\n",
    "                if metric == \"accuracy\" or metric == \"acc\":\n",
    "                    metrics_[metric] = lambda y_true, y_pred: accuracy_score(\n",
    "                        y_true, np.where(y_pred > 0.5, 1, 0))\n",
    "                self.metrics_names.append(metric)\n",
    "        return metrics_\n",
    "\n",
    "    @property\n",
    "    def embedding_size(self, ):\n",
    "        feature_columns = self.dnn_feature_columns\n",
    "        sparse_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, (SparseFeat, VarLenSparseFeat)), feature_columns)) if len(\n",
    "            feature_columns) else []\n",
    "        embedding_size_set = set([feat.embedding_dim for feat in sparse_feature_columns])\n",
    "        if len(embedding_size_set) > 1:\n",
    "            raise ValueError(\"embedding_dim of SparseFeat and VarlenSparseFeat must be same in this model!\")\n",
    "        return list(embedding_size_set)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.keras.callbacks import History\n",
    "\n",
    "EarlyStopping = EarlyStopping\n",
    "History = History\n",
    "\n",
    "class ModelCheckpoint(ModelCheckpoint):\n",
    "    \"\"\"Save the model after every epoch.\n",
    "\n",
    "    `filepath` can contain named formatting options,\n",
    "    which will be filled the value of `epoch` and\n",
    "    keys in `logs` (passed in `on_epoch_end`).\n",
    "\n",
    "    For example: if `filepath` is `weights.{epoch:02d}-{val_loss:.2f}.hdf5`,\n",
    "    then the model checkpoints will be saved with the epoch number and\n",
    "    the validation loss in the filename.\n",
    "\n",
    "    Arguments:\n",
    "        filepath: string, path to save the model file.\n",
    "        monitor: quantity to monitor.\n",
    "        verbose: verbosity mode, 0 or 1.\n",
    "        save_best_only: if `save_best_only=True`,\n",
    "            the latest best model according to\n",
    "            the quantity monitored will not be overwritten.\n",
    "        mode: one of {auto, min, max}.\n",
    "            If `save_best_only=True`, the decision\n",
    "            to overwrite the current save file is made\n",
    "            based on either the maximization or the\n",
    "            minimization of the monitored quantity. For `val_acc`,\n",
    "            this should be `max`, for `val_loss` this should\n",
    "            be `min`, etc. In `auto` mode, the direction is\n",
    "            automatically inferred from the name of the monitored quantity.\n",
    "        save_weights_only: if True, then only the model's weights will be\n",
    "            saved (`model.save_weights(filepath)`), else the full model\n",
    "            is saved (`model.save(filepath)`).\n",
    "        period: Interval (number of epochs) between checkpoints.\n",
    "    \"\"\"\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epochs_since_last_save += 1\n",
    "        if self.epochs_since_last_save >= self.period:\n",
    "            self.epochs_since_last_save = 0\n",
    "            filepath = self.filepath.format(epoch=epoch + 1, **logs)\n",
    "            if self.save_best_only:\n",
    "                current = logs.get(self.monitor)\n",
    "                if current is None:\n",
    "                    print('Can save best model only with %s available, skipping.' % self.monitor)\n",
    "                else:\n",
    "                    if self.monitor_op(current, self.best):\n",
    "                        if self.verbose > 0:\n",
    "                            print('Epoch %05d: %s improved from %0.5f to %0.5f,'\n",
    "                                  ' saving model to %s' % (epoch + 1, self.monitor, self.best,\n",
    "                                                           current, filepath))\n",
    "                        self.best = current\n",
    "                        if self.save_weights_only:\n",
    "                            torch.save(self.model.state_dict(), filepath)\n",
    "                        else:\n",
    "                            torch.save(self.model, filepath)\n",
    "                    else:\n",
    "                        if self.verbose > 0:\n",
    "                            print('Epoch %05d: %s did not improve from %0.5f' %\n",
    "                                  (epoch + 1, self.monitor, self.best))\n",
    "            else:\n",
    "                if self.verbose > 0:\n",
    "                    print('Epoch %05d: saving model to %s' %\n",
    "                          (epoch + 1, filepath))\n",
    "                if self.save_weights_only:\n",
    "                    torch.save(self.model.state_dict(), filepath)\n",
    "                else:\n",
    "                    torch.save(self.model, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.dense_feature_columns []\n",
      "weight_list 6\n",
      "weight torch.Size([7, 4])\n",
      "weight torch.Size([2, 4])\n",
      "weight torch.Size([187, 4])\n",
      "weight torch.Size([20, 4])\n",
      "weight torch.Size([193, 4])\n",
      "weight torch.Size([188, 4])\n",
      "weight_list 6\n",
      "weight torch.Size([7, 1])\n",
      "weight torch.Size([2, 1])\n",
      "weight torch.Size([187, 1])\n",
      "weight torch.Size([20, 1])\n",
      "weight torch.Size([193, 1])\n",
      "weight torch.Size([188, 1])\n"
     ]
    }
   ],
   "source": [
    "basemodel = BaseModel(linear_feature_columns, dnn_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModel(\n",
       "  (embedding_dict): ModuleDict(\n",
       "    (age): Embedding(7, 4)\n",
       "    (gender): Embedding(2, 4)\n",
       "    (movie_id): Embedding(187, 4)\n",
       "    (occupation): Embedding(20, 4)\n",
       "    (user_id): Embedding(193, 4)\n",
       "    (zip): Embedding(188, 4)\n",
       "  )\n",
       "  (linear_model): Linear(\n",
       "    (embedding_dict): ModuleDict(\n",
       "      (age): Embedding(7, 1)\n",
       "      (gender): Embedding(2, 1)\n",
       "      (movie_id): Embedding(187, 1)\n",
       "      (occupation): Embedding(20, 1)\n",
       "      (user_id): Embedding(193, 1)\n",
       "      (zip): Embedding(188, 1)\n",
       "    )\n",
       "  )\n",
       "  (out): PredictionLayer()\n",
       ")"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basemodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "basemodel.compile(\"adam\", \"mse\", metrics=['mse'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mse': <function sklearn.metrics._regression.mean_squared_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average', squared=True)>}"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basemodel.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.nn.functional.mse_loss(input, target, size_average=None, reduce=None, reduction='mean')>"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basemodel.loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Train on 128 samples, validate on 32 samples, 1 steps per epoch\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-230-cc09ae3ce278>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m history = basemodel.fit(train_model_input, train[target].values, batch_size=256, epochs=10, verbose=2,\n\u001b[0;32m----> 2\u001b[0;31m                         validation_split=0.2)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-219-c56faa0d868d>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, initial_epoch, validation_split, validation_data, shuffle, callbacks)\u001b[0m\n\u001b[1;32m    180\u001b[0m                         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = basemodel.fit(train_model_input, train[target].values, batch_size=256, epochs=10, verbose=2,\n",
    "                        validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
